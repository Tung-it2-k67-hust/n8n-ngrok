# Ph√¢n t√≠ch chi ti·∫øt: 8_spark_streaming.pdf

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu ph√¢n t√≠ch v√† tr√¨nh b√†y chi ti·∫øt v·ªÅ n·ªôi dung slide "Spark Streaming" d·ª±a tr√™n nh·ªØng th√¥ng tin b·∫°n cung c·∫•p, ƒë∆∞·ª£c tr√¨nh b√†y chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát v·ªõi ƒë·ªãnh d·∫°ng Markdown.

---

# üöÄ Apache Spark Streaming: X·ª≠ L√Ω D·ªØ Li·ªáu Lu·ªìng Th·ªùi Gian Th·ª±c

Spark Streaming l√† m·ªôt th√†nh ph·∫ßn quan tr·ªçng trong h·ªá sinh th√°i Apache Spark, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c (real-time data) v·ªõi kh·∫£ nƒÉng m·ªü r·ªông cao v√† ƒë·ªô tin c·∫≠y tuy·ªát v·ªùi.

## 1. Kh√°i Ni·ªám C∆° B·∫£n V·ªÅ Data Streaming

### Gi·∫£i th√≠ch Kh√°i ni·ªám
**Data Streaming** (D·ªØ li·ªáu lu·ªìng) l√† m·ªôt k·ªπ thu·∫≠t cho ph√©p truy·ªÅn chuy·ªÉn d·ªØ li·ªáu d∆∞·ªõi d·∫°ng lu·ªìng li√™n t·ª•c v√† b·ªÅn v·ªØng. Thay v√¨ x·ª≠ l√Ω d·ªØ li·ªáu theo batch (l√¥) c·ªë ƒë·ªãnh, streaming x·ª≠ l√Ω d·ªØ li·ªáu khi ch√∫ng ƒë·∫øn g·∫ßn nh∆∞ t·ª©c th·ªùi.

### Vai Tr√≤ Trong Th·ªùi ƒê·∫°i S·ªë
K·ªπ thu·∫≠t streaming ng√†y c√†ng tr·ªü n√™n quan tr·ªçng v√† ph·ªï bi·∫øn c√πng v·ªõi s·ª± tƒÉng tr∆∞·ªüng c·ªßa d·ªØ li·ªáu s·ªë. C√°c h·ªá th·ªëng hi·ªán ƒë·∫°i c·∫ßn x·ª≠ l√Ω l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì ƒë∆∞·ª£c sinh ra li√™n t·ª•c t·ª´ c√°c ngu·ªìn kh√°c nhau.

### Use Cases
- **IoT (Internet of Things)**: D·ªØ li·ªáu c·∫£m bi·∫øn t·ª´ c√°c thi·∫øt b·ªã th√¥ng minh
- **Social Media**: D√≤ng tweet, post Facebook li√™n t·ª•c
- **Financial Trading**: Giao d·ªãch ch·ª©ng kho√°n th·ªùi gian th·ª±c
- **Log Monitoring**: Theo d√µi log h·ªá th·ªëng
- **Fraud Detection**: Ph√°t hi·ªán gian l·∫≠n trong thanh to√°n

## 2. Vai Tr√≤ C·ªßa Spark Streaming Trong H·ªá Sinh Th√°i Apache Spark

H·ªá sinh th√°i Apache Spark bao g·ªìm c√°c th√†nh ph·∫ßn ch√≠nh:

```mermaid
graph TD
    A[Apache Spark Ecosystem] --> B[Spark Core]
    A --> C[Spark SQL]
    A --> D[Spark Streaming]
    A --> E[Spark MLlib]
    A --> F[Spark GraphX]
    
    D --> G[DStream API]
    D --> H[Structured Streaming]
```

## 3. T·∫°i Sao Ch·ªçn Spark Streaming? (Why Spark Streaming)

Spark Streaming ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ **thuy√™n chuy·ªÉn d·ªØ li·ªáu th·ªùi gian th·ª±c (real-time data)** t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau nh∆∞ Twitter, Facebook, IoT v√† cho ph√©p th·ª±c thi c√°c ph√¢n t√≠ch d·ªØ li·ªáu m·∫°nh m·∫Ω t·ª´ c√°c lu·ªìng d·ªØ li·ªáu n√†y.

### ∆Øu ƒêi·ªÉm Ch√≠nh
- **T√≠ch h·ª£p v·ªõi Spark Ecosystem**: S·ª≠ d·ª•ng chung engine t√≠nh to√°n v·ªõi batch processing
- **ƒêa ng√¥n ng·ªØ**: H·ªó tr·ª£ Python (PySpark), Scala, Java, R
- **Fault Tolerance**: Kh·∫£ nƒÉng ch·ªãu l·ªói cao
- **High Throughput**: X·ª≠ l√Ω l∆∞·ª£ng d·ªØ li·ªáu l·ªõn
- **Micro-batch Architecture**: Linh ho·∫°t gi·ªØa latency v√† throughput

### Nh∆∞·ª£c ƒêi·ªÉm
- **Latency**: V√¨ l√† micro-batch, ƒë·ªô tr·ªÖ th∆∞·ªùng cao h∆°n so v·ªõi true streaming (nh∆∞ Flink)
- **Complexity**: C·∫•u h√¨nh ph·ª©c t·∫°p h∆°n so v·ªõi c√°c framework ƒë∆°n gi·∫£n
- **Memory Management**: C·∫ßn tinh ch·ªânh GC v√† memory cho workload l·ªõn

## 4. T·ªïng Quan V·ªÅ Spark Streaming

Spark Streaming l√† th√†nh ph·∫ßn quan tr·ªçng trong h·ªá sinh th√°i Spark, b√™n c·∫°nh Spark core API. N√≥ cho ph√©p x·ª≠ l√Ω lu·ªìng d·ªØ li·ªáu v·ªõi **th√¥ng l∆∞·ª£ng l·ªõn (hight-throughput)** v√† c√≥ **kh·∫£ nƒÉng ch·ªãu l·ªói (fault-tolerant)**.

### Kh√°i Ni·ªám DStream

Spark Streaming g·ªçi lu·ªìng l√† **DStream (Discretized Stream)**, m·ªói lu·ªìng l√† m·ªôt chu·ªói c√°c RDD c·∫ßn ph·∫£i x·ª≠ l√Ω tr·ª±c tuy·∫øn.

#### C·∫•u Tr√∫c DStream
```
Time Interval 1: [RDD_1] ‚Üí [RDD_2] ‚Üí [RDD_3] ‚Üí ...
Time Interval 2: [RDD_1] ‚Üí [RDD_2] ‚Üí [RDD_3] ‚Üí ...
Time Interval 3: [RDD_1] ‚Üí [RDD_2] ‚Üí [RDD_3] ‚Üí ...
```

## 5. Sample Code: C√†i ƒë·∫∑t Spark Streaming c∆° b·∫£n

### V√≠ d·ª• 1: PySpark Streaming v·ªõi Socket Source

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# 1. Kh·ªüi t·∫°o SparkContext v√† StreamingContext
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)  # Batch interval = 1 second

# 2. T·∫°o DStream t·ª´ socket
lines = ssc.socketTextStream("localhost", 9999)

# 3. Ph√¢n t√≠ch lu·ªìng d·ªØ li·ªáu
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda x, y: x + y)

# 4. In k·∫øt qu·∫£
word_counts.pprint()

# 5. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω
ssc.start()
ssc.awaitTermination()
```

### V√≠ d·ª• 2: PySpark Streaming v·ªõi File Source

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Kh·ªüi t·∫°o StreamingContext
ssc = StreamingContext(sc, 1)

# T·∫°o DStream t·ª´ th∆∞ m·ª•c
lines = ssc.textFileStream("hdfs://localhost:9000/user/logs/")

# X·ª≠ l√Ω d·ªØ li·ªáu
error_logs = lines.filter(lambda line: "ERROR" in line)
error_counts = error_logs.count()

# In k·∫øt qu·∫£
error_counts.pprint()

# B·∫Øt ƒë·∫ßu x·ª≠ l√Ω
ssc.start()
ssc.awaitTermination()
```

### V√≠ d·ª• 3: Java Spark Streaming

```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.Durations;

public class JavaNetworkWordCount {
    public static void main(String[] args) throws Exception {
        // Kh·ªüi t·∫°o SparkConf
        SparkConf conf = new SparkConf().setAppName("JavaNetworkWordCount");
        
        // Kh·ªüi t·∫°o JavaSparkContext
        JavaSparkContext jsc = new JavaSparkContext(conf);
        
        // Kh·ªüi t·∫°o JavaStreamingContext v·ªõi batch interval 1 gi√¢y
        JavaStreamingContext jssc = new JavaStreamingContext(jsc, Durations.seconds(1));
        
        // T·∫°o DStream t·ª´ socket
        JavaDStream<String> lines = jssc.socketTextStream("localhost", 9999);
        
        // Ph√¢n t√≠ch d·ªØ li·ªáu
        JavaDStream<String> words = lines.flatMap(line -> Arrays.asList(line.split(" ")).iterator());
        JavaPairDStream<String, Integer> pairs = words.mapToPair(word -> new Tuple2<>(word, 1));
        JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey((x, y) -> x + y);
        
        // In k·∫øt qu·∫£
        wordCounts.print();
        
        // B·∫Øt ƒë·∫ßu x·ª≠ l√Ω
        jssc.start();
        jssc.awaitTermination();
    }
}
```

### V√≠ d·ª• 4: Window Operations trong Spark Streaming

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

ssc = StreamingContext(sc, 1)  # Batch interval = 1s

# T·∫°o DStream
lines = ssc.socketTextStream("localhost", 9999)

# Window operations: 30 gi√¢y window, slide m·ªói 10 gi√¢y
windowed_word_counts = lines.flatMap(lambda line: line.split(" ")) \
    .map(lambda word: (word, 1)) \
    .reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)

windowed_word_counts.pprint()

ssc.start()
ssc.awaitTermination()
```

### V√≠ d·ª• 5: SQL Query tr√™n DStream

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import SQLContext

sc = SparkContext("local[2]", "SQLStream")
ssc = StreamingContext(sc, 1)
sqlContext = SQLContext(sc)

# T·∫°o DStream
lines = ssc.socketTextStream("localhost", 9999)

# Chuy·ªÉn ƒë·ªïi DStream sang DataFrame
def process_rdd(rdd):
    if not rdd.isEmpty():
        df = sqlContext.createDataFrame(rdd, ["text"])
        df.registerTempTable("logs")
        
        # SQL Query
        result = sqlContext.sql("SELECT text, COUNT(*) as count FROM logs GROUP BY text")
        result.show()

lines.foreachRDD(process_rdd)

ssc.start()
ssc.awaitTermination()
```

## 6. C√°c Kh√°i Ni·ªám Quan Tr·ªçng Trong Spark Streaming

### 6.1. Receiver-based vs Direct Approach

| Feature | Receiver-based | Direct Approach |
|---------|----------------|-----------------|
| **Data Loss** | C√≥ th·ªÉ m·∫•t d·ªØ li·ªáu (Write Ahead Log c·∫ßn thi·∫øt) | Kh√¥ng m·∫•t d·ªØ li·ªáu (At-least-once) |
| **Throughput** | Cao h∆°n | T·ªët |
| **Complexity** | Ph·ª©c t·∫°p h∆°n | ƒê∆°n gi·∫£n h∆°n |
| **Use Case** | Legacy systems | Kafka integration |

### 6.2. Checkpointing

```python
# Thi·∫øt l·∫≠p checkpoint
ssc.checkpoint("hdfs://localhost:9000/checkpoint")

# S·ª≠ d·ª•ng updateStateByKey (b·∫Øt bu·ªôc checkpoint)
def updateFunction(newValues, runningCount):
    if runningCount is None:
        runningCount = 0
    return sum(newValues, runningCount)

running_counts = pairs.updateStateByKey(updateFunction)
```

### 6.3. Fault Tolerance trong Spark Streaming

Spark Streaming ƒë·∫£m b·∫£o fault tolerance th√¥ng qua:
- **RDD Lineage**: T√°i t·∫°o d·ªØ li·ªáu b·ªã m·∫•t
- **Checkpointing**: L∆∞u state c·ªßa application
- **Write Ahead Logs**: Ghi log tr∆∞·ªõc khi x·ª≠ l√Ω

## 7. V√≠ d·ª• Th·ª±c T·∫ø Trong Ng√†nh C√¥ng Nghi·ªáp

### Case Study 1: Real-time Fraud Detection (Ng√¢n h√†ng)

```python
# Fraud Detection System
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

ssc = StreamingContext(sc, 1)

# Nh·∫≠n giao d·ªãch t·ª´ Kafka
transactions = ssc.socketTextStream("kafka-broker", 9092)

# Ph√¢n t√≠ch giao d·ªãch nghi ng·ªù
suspicious_transactions = transactions.filter(
    lambda tx: float(tx.split(",")[3]) > 10000 or  # Giao d·ªãch > 10k
    tx.split(",")[2] in ["high_risk_country"]      # Qu·ªëc gia r·ªßi ro cao
)

# G·ª≠i c·∫£nh b√°o
def send_alert(rdd):
    for tx in rdd.collect():
        send_to_fraud_team(tx)

suspicious_transactions.foreachRDD(send_alert)

ssc.start()
ssc.awaitTermination()
```

### Case Study 2: IoT Monitoring System

```python
# IoT Sensor Monitoring
ssc = StreamingContext(sc, 1)

# Nh·∫≠n d·ªØ li·ªáu c·∫£m bi·∫øn
sensor_data = ssc.socketTextStream("iot-gateway", 8888)

# T√≠nh to√°n nhi·ªát ƒë·ªô trung b√¨nh theo window
avg_temp = sensor_data.map(lambda x: float(x.split(",")[1])) \
    .reduceByWindow(lambda x, y: (x + y) / 2, 60, 30)  # 60s window, 30s slide

# Alert n·∫øu nhi·ªát ƒë·ªô > 50¬∞C
avg_temp.filter(lambda temp: temp > 50).pprint()

ssc.start()
ssc.awaitTermination()
```

### Case Study 3: Social Media Analytics

```python
# Twitter Hashtag Counter
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

ssc = StreamingContext(sc, 5)

# ƒê·ªçc t·ª´ Kafka
kafkaStream = KafkaUtils.createStream(
    ssc, 
    'localhost:2181', 
    'twitter-consumer-group', 
    {'twitter-topic': 1}
)

# ƒê·∫øm hashtag
hashtags = kafkaStream.flatMap(lambda line: line[1].split(" ")) \
    .filter(lambda word: word.startswith("#")) \
    .map(lambda tag: (tag, 1)) \
    .reduceByKeyAndWindow(lambda x, y: x + y, 300, 60)  # Top trending 5 ph√∫t

hashtags.pprint()

ssc.start()
ssc.awaitTermination()
```

## 8. So S√°nh Spark Streaming vs C√°c C√¥ng C·ª• Kh√°c

| Ti√™u ch√≠ | Spark Streaming | Flink | Kafka Streams | Storm |
|----------|-----------------|-------|---------------|-------|
| **Latency** | Medium (seconds) | Very Low (ms) | Low (ms) | Very Low (ms) |
| **Throughput** | Very High | Very High | High | Medium |
| **Fault Tolerance** | Excellent | Excellent | Good | Good |
| **Complexity** | Medium | High | Low | Medium |
| **Use Case** | Batch + Stream | Real-time ML | Kafka ecosystem | Legacy streaming |

## 9. Best Practices

### Khi N√†o S·ª≠ D·ª•ng Spark Streaming?
‚úÖ **N√™n d√πng khi:**
- B·∫°n ƒë√£ c√≥ Spark ecosystem
- C·∫ßn x·ª≠ l√Ω l∆∞·ª£ng d·ªØ li·ªáu l·ªõn (TB/ng√†y)
- C·∫ßn t√≠ch h·ª£p v·ªõi batch processing
- ƒê√≤i h·ªèi fault tolerance cao
- C√≥ teamÁÜüÊÇâ v·ªõi Spark

‚ùå **Kh√¥ng n√™n d√πng khi:**
- C·∫ßn latency c·ª±c th·∫•p (< 100ms)
- H·ªá th·ªëng nh·ªè, ƒë∆°n gi·∫£n
- Kh√¥ng c√≥ resource ƒë·ªÉ v·∫≠n h√†nh Spark cluster

### C·∫•u h√¨nh t·ªëi ∆∞u

```python
# C·∫•u h√¨nh Spark Streaming t·ªëi ∆∞u
conf = SparkConf() \
    .set("spark.streaming.backpressure.enabled", "true") \
    .set("spark.streaming.kafka.maxRatePerPartition", "1000") \
    .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .set("spark.sql.shuffle.partitions", "200")

sc = SparkContext(conf=conf)
ssc = StreamingContext(sc, 1)
```

## 10. K·∫øt Lu·∫≠n

Spark Streaming l√† m·ªôt **powerful framework** ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c, ƒë·∫∑c bi·ªát ph√π h·ª£p cho c√°c doanh nghi·ªáp ƒë√£ ƒë·∫ßu t∆∞ v√†o h·ªá sinh th√°i Spark. V·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω high-throughput, fault tolerance v√† t√≠ch h·ª£p s·∫µn v·ªõi c√°c th√†nh ph·∫ßn kh√°c, Spark Streaming l√† l·ª±a ch·ªçn h√†ng ƒë·∫ßu cho c√°c h·ªá th·ªëng big data ph√¢n t√°n.

Tuy nhi√™n, c·∫ßn l∆∞u √Ω v·ªÅ **latency** v√† **complexity** khi l·ª±a ch·ªçn c√¥ng ngh·ªá n√†y cho c√°c use case y√™u c·∫ßu th·ªùi gian th·ª±c c·ª±c k·ª≥ th·∫•p.

---

**T√†i li·ªáu tham kh·∫£o:**
- Apache Spark Documentation: https://spark.apache.org/docs/latest/streaming-programming-guide.html
- Databricks Spark Streaming Guide
- Spark Streaming: The Definitive Guide by Bill Chambers & Matei Zaharia

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide Spark Streaming b·∫°n cung c·∫•p, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp v√† chi ti·∫øt theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch & H∆∞·ªõng d·∫´n Chi ti·∫øt v·ªÅ Spark Streaming

T√†i li·ªáu n√†y t√≥m t·∫Øt c√°c ki·∫øn th·ª©c n·ªÅn t·∫£ng v·ªÅ **Apache Spark Streaming**, m·ªôt th√†nh ph·∫ßn quan tr·ªçng trong h·ªá sinh th√°i SparkÁî®‰∫é x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c (real-time data processing).

## 1. ∆Øu ƒëi·ªÉm c·ªßa Spark Streaming (Advantages)

Theo slide s·ªë 6, n·ªôi dung t·∫≠p trung v√†o c√°c l·ª£i √≠ch ch√≠nh c·ªßa Spark Streaming so v·ªõi c√°c c√¥ng c·ª• x·ª≠ l√Ω lu·ªìng kh√°c.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
**Spark Streaming** l√† m·ªôt component c·ªßa Apache Spark cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn t·ª´ c√°c ngu·ªìn d·ªØ li·ªáu tr·ª±c ti·∫øp (live data sources) v·ªõi c√°c thao t√°c t∆∞∆°ng t·ª± nh∆∞ x·ª≠ l√Ω batch (x·ª≠ l√Ω kh·ªëi l∆∞·ª£ng l·ªõn d·ªØ li·ªáu m·ªôt l·∫ßn). D∆∞·ªõi ƒë√¢y l√† c√°c ∆∞u ƒëi·ªÉm ch√≠nh:

*   **T√≠ch h·ª£p v·ªõi Spark Ecosystem:** B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√πng m·ªôt codebase ƒë·ªÉ x·ª≠ l√Ω c·∫£ d·ªØ li·ªáu batch (RDD) v√† d·ªØ li·ªáu streaming.
*   **M√¥ h√¨nh x·ª≠ l√Ω m·∫°nh m·∫Ω:** H·ªó tr·ª£ c√°c thao t√°c ph·ª©c t·∫°p nh∆∞ `map`, `reduce`, `join`, `window` operations.
*   **T√≠nh Fault Tolerance (Ch·ªãu l·ªói):** D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u tr·ªØ trong b·ªô nh·ªõ (in-memory) v√† c√≥ c∆° ch·∫ø ph·ª•c h·ªìi t·ª± ƒë·ªông n·∫øu l·ªói x·∫£y ra.

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **Khi n√†o s·ª≠ d·ª•ng?** | Khi b·∫°n c·∫ßn x·ª≠ l√Ω d·ªØ li·ªáu li√™n t·ª•c t·ª´ c√°c ngu·ªìn nh∆∞ log server, sensor data, IoT, ho·∫∑c social media feeds (Twitter) v√† c·∫ßn ph√¢n t√≠ch ch√∫ng theo th·ªùi gian th·ª±c. |
| **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** | S·ª≠ d·ª•ng `SparkSession` v√† `StreamingContext` ƒë·ªÉ kh·ªüi t·∫°o ·ª©ng d·ª•ng. D·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh c√°c batch nh·ªè (v√≠ d·ª•: 1 gi√¢y) v√† x·ª≠ l√Ω tu·∫ßn t·ª±. |
| **∆Øu ƒëi·ªÉm** | - **Unified Engine:** M·ªôt engine cho c·∫£ batch v√† streaming.<br>- **High-level API:** D·ªÖ d√†ng ph√°t tri·ªÉn.<br>- **Exactly-once semantics:** ƒê·∫£m b·∫£o d·ªØ li·ªáu kh√¥ng b·ªã m·∫•t ho·∫∑c x·ª≠ l√Ω tr√πng l·∫∑p. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **Latency:** T·ªëc ƒë·ªô delay cao h∆°n so v·ªõi Apache Flink ho·∫∑c Kafka Streams (do c∆° ch·∫ø micro-batch).<br>- **Memory Management:** C·∫ßn c·∫•u h√¨nh k·ªπ l∆∞·ª°ng ƒë·ªÉ tr√°nh OutOfMemory. |

### V√≠ d·ª• Th·ª±c t·∫ø
**Ng√†nh T√†i ch√≠nh:** Ng√¢n h√†ng s·ª≠ d·ª•ng Spark Streaming ƒë·ªÉ gi√°m s√°t c√°c giao d·ªãch th·∫ª t√≠n d·ª•ng trong th·ªùi gian th·ª±c ƒë·ªÉ ph√°t hi·ªán gian l·∫≠n (fraud detection). D·ªØ li·ªáu giao d·ªãch ƒë∆∞·ª£c ƒë∆∞a v√†o, x·ª≠ l√Ω ƒë·ªÉ so s√°nh v·ªõi h√†nh vi ng∆∞·ªùi d√πng v√† c·∫£nh b√°o ngay l·∫≠p t·ª©c n·∫øu c√≥ b·∫•t th∆∞·ªùng.

---

## 2. Lu·ªìng ho·∫°t ƒë·ªông & Chi ti·∫øt Lu·ªìng ho·∫°t ƒë·ªông (Workflow & Details)

Slide s·ªë 7 v√† 8 m√¥ t·∫£ quy tr√¨nh t·ª´ khi kh·ªüi t·∫°o ƒë·∫øn khi x·ª≠ l√Ω d·ªØ li·ªáu.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Quy tr√¨nh ho·∫°t ƒë·ªông c·ªßa Spark Streaming bao g·ªìm c√°c giai ƒëo·∫°n ch√≠nh:
1.  **Kh·ªüi t·∫°o:** T·∫°o `StreamingContext`.
2.  **Nh·∫≠n d·ªØ li·ªáu (Ingestion):** D·ªØ li·ªáu ƒë∆∞·ª£c pull t·ª´ ngu·ªìn (Input Source) v·ªÅ th√¥ng qua **Receivers**.
3.  **T·∫°o Batch (DStream):** D·ªØ li·ªáu ƒë∆∞·ª£c t√≠ch l≈©y trong m·ªôt kho·∫£ng th·ªùi gian (v√≠ d·ª•: 1 gi√¢y) v√† t·∫°o th√†nh m·ªôt **RDD** (Resilient Distributed Dataset).
4.  **X·ª≠ l√Ω:** C√°c thao t√°c (transformation) ƒë∆∞·ª£c √°p d·ª•ng l√™n RDD n√†y.
5.  **Xu·∫•t k·∫øt qu·∫£ (Output):** K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u tr·ªØ ho·∫∑c ƒë·∫©y ra ngo√†i.

### V√≠ d·ª• minh h·ªça Lu·ªìng ho·∫°t ƒë·ªông

D∆∞·ªõi ƒë√¢y l√† m√¥ ph·ªèng lu·ªìng x·ª≠ l√Ω d·ªØ li·ªáu log server:

```mermaid
flowchart LR
    A[Log Server] -->|Stream Data| B[Receiver]
    B -->|T√≠ch l≈©y 1s| C[Micro-Batch RDD]
    C -->|Transformation| D[Map/Reduce/Filter]
    D -->|Output| E[Database/File/HDFS]
```

### Code M·∫´u: C·∫•u h√¨nh Lu·ªìng ho·∫°t ƒë·ªông c∆° b·∫£n (Python/PySpark)

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# 1. Kh·ªüi t·∫°o SparkContext v√† StreamingContext
# Batch Interval = 1 gi√¢y
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)

# 2. T·∫°o InputDStream (K·∫øt n·ªëi t·ªõi m·ªôt m√°y ch·ªß TCP)
# D·ªØ li·ªáu t·ª´ netcat s·∫Ω ƒë∆∞·ª£c nh·∫≠n qua Receiver
lines = ssc.socketTextStream("localhost", 9999)

# 3. X·ª≠ l√Ω d·ªØ li·ªáu (Transformation)
# T√°ch d√≤ng th√†nh c√°c t·ª´ v√† ƒë·∫øm s·ªë l∆∞·ª£ng
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda a, b: a + b)

# 4. Xu·∫•t k·∫øt qu·∫£ (Output)
word_counts.pprint()

# 5. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω
ssc.start()
ssc.awaitTermination()
```

---

## 3. Streaming Fundamentals & Streaming Context

Slide s·ªë 9 v√† 10 ƒëi s√¢u v√†o c√°c th√†nh ph·∫ßn c·ªët l√µi: **Streaming Context**, **InputDStream**, v√† **Receiver**.

### Gi·∫£i th√≠ch Kh√°i ni·ªám

*   **Streaming Context (`ssc`):**
    *   L√† **entry point** (c·ª≠a v√†o) ch√≠nh cho m·ªçi ch∆∞∆°ng tr√¨nh Spark Streaming, t∆∞∆°ng t·ª± nh∆∞ `SparkContext` trong Spark Core ho·∫∑c `SparkSession` trong Spark SQL.
    *   N√≥ qu·∫£n l√Ω vi·ªác ch·∫°y c√°c job, ph·ª•c h·ªìi sau l·ªói, v√† thi·∫øt l·∫≠p c√°c th√¥ng s·ªë c∆° b·∫£n (nh∆∞ batch interval).

*   **InputDStream:**
    *   L√† m·ªôt lu·ªìng d·ªØ li·ªáu ƒë·∫ßu v√†o. N√≥ ƒë·∫°i di·ªán cho m·ªôt chu·ªói c√°c RDD (m·ªói RDD ch·ª©a d·ªØ li·ªáu c·ªßa m·ªôt kho·∫£ng th·ªùi gian).
    *   D·ªØ li·ªáu ƒë·∫ßu v√†o c√≥ th·ªÉ ƒë·∫øn t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau.

*   **Receiver:**
    *   L√† m·ªôt th√†nh ph·∫ßn ch·∫°y tr√™n Executor ƒë·ªÉ "nghe" (listen) v√† nh·∫≠n d·ªØ li·ªáu t·ª´ ngu·ªìn.
    *   D·ªØ li·ªáu nh·∫≠n ƒë∆∞·ª£c s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o b·ªô nh·ªõ (memory) ho·∫∑c disk t√πy c·∫•u h√¨nh, sau ƒë√≥ t·∫°o th√†nh m·ªôt RDD khi h·∫øt th·ªùi gian batch.

*   **C√°c ngu·ªìn d·ªØ li·ªáu c√≥ s·∫µn (Built-in Sources):**
    *   **Twitter:** L·∫•y d·ªØ li·ªáu t·ª´ Twitter Streaming API.
    *   **Akka Actor:** D√πng cho giao ti·∫øp gi·ªØa c√°c Actor.
    *   **ZeroMQ:** M·ªôt th∆∞ vi·ªán messaging library.
    *   Ngo√†i ra c√≤n c√≥ **Kafka**, **Kinesis**, **File System**, **Socket**.

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **Khi n√†o s·ª≠ d·ª•ng?** | Khi b·∫°n c·∫ßn k·∫øt n·ªëi v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu ph·ª©c t·∫°p ho·∫∑c t√πy ch·ªânh lu·ªìng d·ªØ li·ªáu v√†o. |
| **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** | - Kh·ªüi t·∫°o `StreamingContext`.<br>- G·ªçi h√†m t·∫°o InputDStream t∆∞∆°ng ·ª©ng v·ªõi ngu·ªìn (v√≠ d·ª•: `ssc.socketTextStream`, `KafkaUtils.createDirectStream`).<br>- ƒê·ªãnh nghƒ©a c√°c ph√©p to√°n tr√™n DStream. |
| **∆Øu ƒëi·ªÉm** | - **Linh ho·∫°t:** H·ªó tr·ª£ nhi·ªÅu lo·∫°i ngu·ªìn (push-based v√† pull-based).<br>- **Receiver:** ƒê∆°n gi·∫£n h√≥a vi·ªác k·∫øt n·ªëi v·ªõi c√°c API ph·ª©c t·∫°p. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **Receiver:** C√≥ th·ªÉÊàê‰∏∫ bottleneck (ƒëi·ªÉm ngh·∫Ωn) n·∫øu t·ªëc ƒë·ªô nh·∫≠n d·ªØ li·ªáu ch·∫≠m h∆°n t·ªëc ƒë·ªô x·ª≠ l√Ω.<br>- **Complexity:** C·∫•u h√¨nh Receiver cho c√°c ngu·ªìn nh∆∞ Kafka ƒë√≤i h·ªèi hi·ªÉu bi·∫øt s√¢u v·ªÅ c·∫•u tr√∫c d·ªØ li·ªáu. |

### Code M·∫´u: S·ª≠ d·ª•ng Streaming Context v√† c√°c ngu·ªìn kh√°c nhau

```python
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

# Kh·ªüi t·∫°o Streaming Context v·ªõi Batch Interval = 5 gi√¢y
ssc = StreamingContext(sc, 5)

# --- V√≠ d·ª• 1: Ngu·ªìn Socket (ƒê∆°n gi·∫£n, d√πng cho test) ---
# L·∫Øng nghe d·ªØ li·ªáu t·ª´ localhost:9999
socket_stream = ssc.socketTextStream("localhost", 9999)

# --- V√≠ d·ª• 2: Ngu·ªìn Kafka (Ph·ªï bi·∫øn trong th·ª±c t·∫ø) ---
# C·∫•u h√¨nh k·∫øt n·ªëi t·ªõi Kafka
kafka_params = {
    "bootstrap.servers": "localhost:9092",
    "auto.offset.reset": "latest",
    "group.id": "spark-streaming-group"
}
topics = ["sensor_data"]

# T·∫°o DStream t·ª´ Kafka (Direct approach)
kafka_stream = KafkaUtils.createDirectStream(
    ssc,
    topics,
    kafka_params
)

# X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ Kafka (d·ªØ li·ªáu l√† key-value binary)
# V√≠ d·ª•: Parse JSON message
def process_rdd(rdd):
    if not rdd.isEmpty():
        # Logic x·ª≠ l√Ω ·ªü ƒë√¢y
        df = spark.read.json(rdd.map(lambda x: x[1])) # x[1] l√† value
        df.show()

# √Åp d·ª•ng foreachRDD ƒë·ªÉ x·ª≠ l√Ω t·ª´ng batch
kafka_stream.foreachRDD(process_rdd)

# B·∫Øt ƒë·∫ßu lu·ªìng
ssc.start()
ssc.awaitTermination()
```

### V√≠ d·ª• Th·ª±c t·∫ø: H·ªá th·ªëng gi√°m s√°t Server (Server Monitoring)

*   **Scenario:** M·ªôt c√¥ng ty c√≥ h√†ng trƒÉm server, m·ªói server g·ª≠i log (CPU, RAM, Error) v·ªÅ m·ªôt trung t√¢m t·∫≠p trung qua socket ho·∫∑c Kafka.
*   **Application:**
    1.  **Streaming Context** ƒë∆∞·ª£c kh·ªüi t·∫°o v·ªõi batch interval l√† 10 gi√¢y.
    2.  **InputDStream** k·∫øt n·ªëi t·ªõi Kafka topic `server_logs`.
    3.  **Receiver** nh·∫≠n d·ªØ li·ªáu JSON.
    4.  **Logic:** D√πng Spark SQL ƒë·ªÉ parse JSON, l·ªçc c√°c log c√≥ level l√† "ERROR", ƒë·∫øm s·ªë l∆∞·ª£ng l·ªói theo t·ª´ng server.
    5.  **Output:** N·∫øu s·ªë l·ªói > 5 trong 10 gi√¢y, g·ª≠i c·∫£nh b√°o (alert) qua email ho·∫∑c Slack.

---

## T√≥m t·∫Øt ki·∫øn th·ª©c (Cheat Sheet)

| Thu·∫≠t ng·ªØ | ƒê·ªãnh nghƒ©a | V√≠ d·ª• |
| :--- | :--- | :--- |
| **DStream** | Represent a continuous stream of data (chu·ªói c√°c RDD). | `lines = ssc.socketTextStream(...)` |
| **Receiver** | Component nh·∫≠n d·ªØ li·ªáu t·ª´ ngu·ªìn v√† l∆∞u v√†o memory. | `TwitterReceiver`, `KafkaReceiver` |
| **Batch Interval** | Th·ªùi gian ƒë·ªÉ t·∫°o m·ªôt RDD m·ªõi (th∆∞·ªùng l√† 0.5s - v√†i gi√¢y). | `ssc = StreamingContext(sc, 1)` (1 gi√¢y) |
| **Transformations** | C√°c ph√©p to√°n l√™n DStream (map, reduce, window). | `words.map(lambda x: (x, 1))` |
| **Output Operations** | Ghi k·∫øt qu·∫£ ra ngo√†i (foreachRDD, saveAsTextFiles). | `dstream.pprint()` |

---

Ch√†o b·∫°n, t√¥i l√† chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide Spark Streaming c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát v·ªõi c√°c y√™u c·∫ßu c·ª• th·ªÉ b·∫°n ƒë√£ ƒë∆∞a ra.

---

# Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Chi ti·∫øt v·ªÅ Apache Spark Streaming

T√†i li·ªáu n√†y cung c·∫•p c√°i nh√¨n t·ªïng quan v·ªÅ c√°c kh√°i ni·ªám n·ªÅn t·∫£ng c·ªßa **Apache Spark Streaming**, m·ªôt th√†nh ph·∫ßn quan tr·ªçng trong h·ªá sinh th√°i SparkÁî®‰∫é x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c (real-time data processing).

## 1. Kh·ªüi t·∫°o (Initialization)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng Spark Streaming, b∆∞·ªõc ƒë·∫ßu ti√™n l√† kh·ªüi t·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng `StreamingContext`. ƒê√¢y l√† ƒëi·ªÉm –≤—Ö–æ–¥ ch√≠nh cho m·ªçi ch·ª©c nƒÉng streaming.

-   **StreamingContext**: L√† l·ªõp ch√≠nh ƒë·ªÉ t·∫°o c√°c lu·ªìng d·ªØ li·ªáu (DStreams) v√† ƒë·ªãnh nghƒ©a c√°c ph√©p to√°n tr√™n ch√∫ng. N√≥ ho·∫°t ƒë·ªông nh∆∞ m·ªôt l·ªõp bao b·ªçc (wrapper) cho `SparkContext`.
-   **SparkContext**: L√† ƒë·ªëi t∆∞·ª£ng ƒë·∫°i di·ªán cho k·∫øt n·ªëi t·ªõi m·ªôt c·ª•m Spark (Spark Cluster). N√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o ra c√°c RDDs, Accumulators v√† Broadcast Variables.

M·ªëi quan h·ªá: `StreamingContext` l√† l·ªõp m·ªü r·ªông c·ªßa `SparkContext`. B·∫°n c·∫ßn c√≥ `SparkContext` ƒë·ªÉ t·∫°o ra `StreamingContext`.

### Code M·∫´u (Python)

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# 1. Kh·ªüi t·∫°o SparkContext (l√µi c·ªßa Spark)
# "local[2]" nghƒ©a l√† ch·∫°y tr√™n local machine v·ªõi 2 worker threads
# "AppName" l√† t√™n ·ª©ng d·ª•ng ƒë·ªÉ hi·ªÉn th·ªã tr√™n Spark UI
sc = SparkContext("local[2]", "NetworkWordCount")

# 2. Kh·ªüi t·∫°o StreamingContext v·ªõi batch interval l√† 1 gi√¢y (1 second)
ssc = StreamingContext(sc, 1)

# Sau khi c√≥ ssc, b·∫°n c√≥ th·ªÉ ƒë·ªãnh nghƒ©a c√°c ngu·ªìn d·ªØ li·ªáu (DStreams) v√† b·∫Øt ƒë·∫ßu x·ª≠ l√Ω
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Khi b·∫°n c·∫ßn x√¢y d·ª±ng m·ªôt ·ª©ng d·ª•ng x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c (streaming application) b·∫±ng Spark.
    *   L√† b∆∞·ªõc b·∫Øt bu·ªôc ph·∫£i c√≥ ƒë·ªÉ ƒë·ªãnh nghƒ©a b·∫•t k·ª≥ lu·ªìng d·ªØ li·ªáu n√†o.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   T·∫°o m·ªôt `SparkContext` v·ªõi c·∫•u h√¨nh c·ª•m (cluster config).
    *   Truy·ªÅn `SparkContext` ƒë√≥ v√†o h√†m kh·ªüi t·∫°o c·ªßa `StreamingContext` c√πng v·ªõi kho·∫£ng th·ªùi gian batch (batch interval).
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **∆Øu ƒëi·ªÉm:** T√≠ch h·ª£p ch·∫∑t ch·∫Ω v·ªõi Spark Core, cho ph√©p s·ª≠ d·ª•ng l·∫°i c√°c c·∫•u h√¨nh v√† th∆∞ vi·ªán c·ªßa Spark.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** Vi·ªác c·∫•u h√¨nh sai batch interval c√≥ th·ªÉ g√¢y ra t√¨nh tr·∫°ng qu√° t·∫£i (overload) ho·∫∑c ƒë·ªô tr·ªÖ cao.

---

## 2. DStream (Discretized Stream)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
**DStream (Discretized Stream)** l√† l·ªõp tr·ª´u t∆∞·ª£ng ch√≠nh ƒë·∫°i di·ªán cho lu·ªìng d·ªØ li·ªáu trong Spark Streaming.

-   **C√°ch ho·∫°t ƒë·ªông:** DStream coi d·ªØ li·ªáu streaming li√™n t·ª•c nh∆∞ m·ªôt chu·ªói c√°c **RDD** (Resilient Distributed Datasets) r·ªùi r·∫°c theo th·ªùi gian.
-   **C·∫•u tr√∫c:** M·ªói RDD trong chu·ªói n√†y ch·ª©a d·ªØ li·ªáu c·ªßa m·ªôt kho·∫£ng th·ªùi gian c·ª• th·ªÉ (v√≠ d·ª•: d·ªØ li·ªáu trong 1 gi√¢y).
-   **Ngu·ªìn g·ªëc:** D·ªØ li·ªáu trong DStream c√≥ th·ªÉ ƒë·∫øn t·ª´:
    1.  **Ngu·ªìn b√™n ngo√†i:** Nh∆∞ Kafka, Socket, Flume.
    2.  **Ph√©p bi·∫øn ƒë·ªïi (Transformation):** K·∫øt qu·∫£ c·ªßa vi·ªác x·ª≠ l√Ω m·ªôt DStream kh√°c (v√≠ d·ª•: `map`, `filter`, `reduce`).

### V√≠ d·ª• Tr·ª±c quan
H√£y t∆∞·ªüng t∆∞·ª£ng m·ªôt d√≤ng s√¥ng (Stream). DStream chia d√≤ng s√¥ng ƒë√≥ th√†nh c√°c x√¥ n∆∞·ªõc (RDDs), m·ªói x√¥ ch·ª©a n∆∞·ªõc c·ªßa m·ªôt gi√¢y nh·∫•t ƒë·ªãnh. B·∫°n c√≥ th·ªÉ x·ª≠ l√Ω t·ª´ng x√¥ n∆∞·ªõc m·ªôt.

### Code M·∫´u (Minh h·ªça t·∫°o DStream t·ª´ Socket)

```python
# S·ª≠ d·ª•ng ssc ƒë√£ t·∫°o ·ªü tr√™n
# T·∫°o m·ªôt DStream k·∫øt n·ªëi t·ªõi localhost tr√™n c·ªïng 9999
lines = ssc.socketTextStream("localhost", 9999)

# 'lines' b√¢y gi·ªù l√† m·ªôt DStream.
# N√≥ li√™n t·ª•c nh·∫≠n d·ªØ li·ªáu text t·ª´ socket v√† t·∫°o RDD m·ªõi m·ªói gi√¢y.
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Khi b·∫°n c·∫ßn x·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫øn li√™n t·ª•c, v√≠ d·ª•: ph√¢n t√≠ch log server theo th·ªùi gian th·ª±c, x·ª≠ l√Ω t√≠n hi·ªáu IoT, theo d√µi giao d·ªãch t√†i ch√≠nh.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   ƒê·ªãnh nghƒ©a ngu·ªìn v√†o (InputDStream).
    *   √Åp d·ª•ng c√°c ph√©p to√°n transformation (map, filter, flatMap) ho·∫∑c action (foreachRDD) l√™n DStream ƒë√≥.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **∆Øu ƒëi·ªÉm:** M√¥ h√¨nh l·∫≠p tr√¨nh gi·ªëng RDD (th√¢n thi·ªán v·ªõi ng∆∞·ªùi d√πng Spark), t·ª± ƒë·ªông x·ª≠ l√Ω fault tolerance (ch·ªãu l·ªói), kh·∫£ nƒÉng m·ªü r·ªông cao.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** ƒê·ªô tr·ªÖ (latency) ph·ª• thu·ªôc v√†o batch interval (th∆∞·ªùng t√≠nh b·∫±ng gi√¢y), kh√¥ng ph√π h·ª£p cho c√°c t√°c v·ª• y√™u c·∫ßu ƒë·ªô tr·ªÖ mili gi√¢y (sub-second latency).

---

## 3. DStream Operation

### Gi·∫£i th√≠ch Kh√°i ni·ªám
C√°c ph√©p to√°n tr√™n DStream ƒë∆∞·ª£c chia l√†m hai lo·∫°i ch√≠nh: **Transformation** v√† **Output Operation**.

-   **Nguy√™n l√Ω:** Khi b·∫°n √°p d·ª•ng m·ªôt ph√©p to√°n (v√≠ d·ª•: `flatMap`) l√™n m·ªôt DStream, Spark Streaming s·∫Ω √°p d·ª•ng ph√©p to√°n ƒë√≥ l√™n **m·ªói RDD** trong chu·ªói RDDs c·ªßa DStream ƒë√≥.
-   **V√≠ d·ª• trong slide:** Chuy·ªÉn ƒë·ªïi lu·ªìng d·ªØ li·ªáu d√≤ng (lines) th√†nh lu·ªìng t·ª´ (words).
    1.  Input: DStream ch·ª©a c√°c c√¢u vƒÉn (m·ªói gi√¢y m·ªôt RDD m·ªõi).
    2.  Operation: `flatMap` c·∫Øt c√¢u vƒÉn th√†nh c√°c t·ª´.
    3.  Output: M·ªôt DStream m·ªõi ch·ª©a c√°c t·ª´ (m·ªói gi√¢y m·ªôt RDD m·ªõi ch·ª©a c√°c t·ª´).

### Code M·∫´u (Transformation)

```python
# lines: DStream input t·ª´ socket (v√≠ d·ª•: "hello world")
# √Åp d·ª•ng flatMap ƒë·ªÉ t√°ch c√°c c√¢u th√†nh t·ª´
words = lines.flatMap(lambda line: line.split(" "))

# words: DStream output (v√≠ d·ª•: ["hello", "world"])
# M·ªói RDD trong 'words' l√† k·∫øt qu·∫£ c·ªßa vi·ªác √°p d·ª•ng flatMap l√™n RDD t∆∞∆°ng ·ª©ng trong 'lines'
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Khi c·∫ßn l√†m s·∫°ch, l·ªçc, ho·∫∑c chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ tr∆∞·ªõc khi l∆∞u tr·ªØ ho·∫∑c ph√¢n t√≠ch s√¢u h∆°n.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   S·ª≠ d·ª•ng c√°c h√†m t∆∞∆°ng t·ª± nh∆∞ RDD API: `map`, `flatMap`, `filter`, `reduceByKey`, `groupByKey`, v.v.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **∆Øu ƒëi·ªÉm:** ƒêa d·∫°ng h√≥a logic x·ª≠ l√Ω, d·ªÖ d√†ng t√≠ch h·ª£p v·ªõi c√°c thu·∫≠t to√°n Machine Learning (MLlib).
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** C√°c ph√©p to√°n stateful (nh∆∞ `updateStateByKey`) ƒë√≤i h·ªèi l∆∞u tr·ªØ state (tr·∫°ng th√°i) tr∆∞·ªõc ƒë√≥, c√≥ th·ªÉ g√¢y t·ªën b·ªô nh·ªõ v√† ph·ª©c t·∫°p trong qu·∫£n l√Ω checkpoint.

---

## 4. InputDStreams

### Gi·∫£i th√≠ch Kh√°i ni·ªám
**InputDStream** l√† m·ªôt lo·∫°i DStream ƒë·∫∑c bi·ªát ƒë·∫°i di·ªán cho lu·ªìng d·ªØ li·ªáu ƒë·∫ßu v√†o t·ª´ c√°c ngu·ªìn b√™n ngo√†i Apache Spark.

Slide ph√¢n lo·∫°i c√°c ngu·ªìn n√†y th√†nh 2 nh√≥m:
1.  **Basic Sources:** C√°c ngu·ªìn c∆° b·∫£n c√≥ s·∫µn trong API Spark Streaming.
    *   **Socket:** Nh·∫≠n d·ªØ li·ªáu qua TCP socket (th∆∞·ªùng d√πng cho test).
    *   **File Systems:** ƒê·ªçc d·ªØ li·ªáu t·ª´ c√°c file m·ªõi ƒë∆∞·ª£c th√™m v√†o m·ªôt th∆∞ m·ª•c (HDFS, local file system).
2.  **Advanced Sources:** C√°c ngu·ªìn t·ª´ c√°c h·ªá sinh th√°i b√™n ngo√†i, ƒë√≤i h·ªèi c√°c th∆∞ vi·ªán k·∫øt n·ªëi ri√™ng.
    *   **Kafka:** H·ªá th·ªëng message queue ph·ªï bi·∫øn nh·∫•t.
    *   **Flume:** Framework ƒë·ªÉ thu th·∫≠p, aggregating v√† chuy·ªÉn d·ªØ li·ªáu log.
    *   **Kinesis:** D·ªãch v·ª• streaming c·ªßa AWS.

### Code M·∫´u (Kafka Integration - Advanced Source)

ƒê√¢y l√† v√≠ d·ª• ph·ªï bi·∫øn nh·∫•t trong th·ª±c t·∫ø.

```python
from pyspark.streaming.kafka import KafkaUtils

# C·∫•u h√¨nh k·∫øt n·ªëi t·ªõi Kafka
kafkaParams = {
    "bootstrap.servers": "localhost:9092",
    "auto.offset.reset": "latest",
    "group.id": "spark-streaming-group"
}

# T·∫°o DStream t·ª´ topic "test-topic" c·ªßa Kafka
directKafkaStream = KafkaUtils.createDirectStream(
    ssc,
    ["test-topic"],  # Topic c·∫ßn l·∫Øng nghe
    kafkaParams
)

# DStream n√†y s·∫Ω tr·∫£ v·ªÅ m·ªôt c·∫∑p (key, value) t·ª´ Kafka
lines = directKafkaStream.map(lambda x: x[1])
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   **Socket/File:** D√πng cho ch·∫°y th·ª≠ (debug), proof of concept, ho·∫∑c x·ª≠ l√Ω file batch ƒë∆°n gi·∫£n.
    *   **Kafka/Flume/Kinesis:** D√πng trong production environment ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn, ƒë·∫£m b·∫£o ƒë·ªô tin c·∫≠y, kh·∫£ nƒÉng ph·ª•c h·ªìi sau l·ªói (fault tolerance) v√† load balancing.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   **Basic:** S·ª≠ d·ª•ng h√†m `ssc.socketTextStream` ho·∫∑c `ssc.textFileStream`.
    *   **Advanced:** C·∫ßn th√™m th∆∞ vi·ªán (dependency) v√†o project, sau ƒë√≥ s·ª≠ d·ª•ng c√°c l·ªõp `KafkaUtils`, `FlumeUtils` ƒë·ªÉ t·∫°o stream.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **Basic Sources:**
        *   *∆Øu:* ƒê∆°n gi·∫£n, d·ªÖ setup.
        *   *Nh∆∞·ª£c:* Kh√¥ng ƒë·∫£m b·∫£o exactly-once semantics, x·ª≠ l√Ω l·ªói k√©m.
    *   **Advanced Sources (Kafka):**
        *   *∆Øu:* ƒê·∫£m b·∫£o d·ªØ li·ªáu kh√¥ng m·∫•t (durable), x·ª≠ l√Ω offset linh ho·∫°t, throughput cao.
        *   *Nh∆∞·ª£c:* C·∫•u h√¨nh ph·ª©c t·∫°p h∆°n, ph·ª• thu·ªôc v√†o external system.

---

## 5. V√≠ d·ª• Th·ª±c t·∫ø trong Industry

D∆∞·ªõi ƒë√¢y l√† c√°c k·ªãch b·∫£n √°p d·ª•ng Spark Streaming trong c√°c ng√†nh c√¥ng nghi·ªáp:

| Ng√†nh | Use Case | C√¥ng ngh·ªá k·∫øt h·ª£p |
| :--- | :--- | :--- |
| **E-commerce (TMƒêT)** | **Real-time Recommendation:** Ph√¢n t√≠ch h√†nh vi clickstream c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ ƒë·ªÅ xu·∫•t s·∫£n ph·∫©m t·ª©c th√¨. | **Kafka** (nh·∫≠n log click) + **Spark Streaming** (t√≠nh to√°n score) + **Redis** (l∆∞u k·∫øt qu·∫£). |
| **Fintech (T√†i ch√≠nh)** | **Fraud Detection (Ph√°t hi·ªán gian l·∫≠n):** Ki·ªÉm tra c√°c giao d·ªãch th·∫ª t√≠n d·ª•ng ƒë·ªÉ ph√°t hi·ªán h√†nh vi b·∫•t th∆∞·ªùng theo th·ªùi gian th·ª±c. | **Kafka** (giao d·ªãch) + **Spark MLlib** (m√¥ h√¨nh d·ª± ƒëo√°n) + **HDFS/Cassandra** (l∆∞u log). |
| **IoT (V·∫≠n t·∫£i)** | **Telematics (Theo d√µi xe):** X·ª≠ l√Ω h√†ng tri·ªáu t√≠n hi·ªáu GPS v√† c·∫£m bi·∫øn t·ª´ xe √¥ t√¥ ƒë·ªÉ c·∫£nh b√°o nguy c∆° ho·∫∑c t·ªëi ∆∞u h√≥a tuy·∫øn ƒë∆∞·ªùng. | **Kinesis/Kafka** (d·ªØ li·ªáu c·∫£m bi·∫øn) + **Spark Streaming** (t√≠nh to√°n v·∫≠n t·ªëc, ƒë·ªãa ƒëi·ªÉm) + **Dashboard** (hi·ªÉn th·ªã). |
| **Social Media** | **Trending Analysis:** ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa c√°c hashtag ƒë·ªÉ c·∫≠p nh·∫≠t xu h∆∞·ªõng trending trong 5 ph√∫t g·∫ßn nh·∫•t. | **Flume/Kafka** (d·ªØ li·ªáu tweet) + **Spark Streaming** (window operations) + **Elasticsearch** (search & visual). |

---

## T√≥m t·∫Øt Ph√¢n t√≠ch

N·ªôi dung slide n√†y gi·ªõi thi·ªáu ki·∫øn tr√∫c c∆° b·∫£n c·ªßa Spark Streaming:
1.  **C·∫•u tr√∫c:** B·∫Øt ƒë·∫ßu t·ª´ `SparkContext` -> `StreamingContext`.
2.  **M√¥ h√¨nh d·ªØ li·ªáu:** D·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω d∆∞·ªõi d·∫°ng chu·ªói c√°c `DStream` (g·ªìm nhi·ªÅu `RDD`).
3.  **X·ª≠ l√Ω:** C√°c ph√©p to√°n ƒë∆∞·ª£c apply tu·∫ßn t·ª± l√™n t·ª´ng RDD trong chu·ªói.
4.  **Ngu·ªìn v√†o:** Linh ho·∫°t t·ª´ c√°c ngu·ªìn c∆° b·∫£n (Socket) ƒë·∫øn c√°c ngu·ªìn ph·ª©c t·∫°p trong s·∫£n xu·∫•t (Kafka, Flume).

ƒê√¢y l√† n·ªÅn t·∫£ng v·ªØng ch·∫Øc ƒë·ªÉ x√¢y d·ª±ng c√°c h·ªá th·ªëng x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c c√≥ kh·∫£ nƒÉng m·ªü r·ªông v√† ch·ªãu l·ªói cao.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "8_spark_streaming.pdf" c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp v√† chi ti·∫øt theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Chi ti·∫øt v·ªÅ Spark Streaming (DStreams & Transformations)

T√†i li·ªáu n√†y gi·ªõi thi·ªáu c√°c kh√°i ni·ªám c·ªët l√µi c·ªßa **Spark Streaming** (c·ª• th·ªÉ l√† API DStream - Discretized Stream), bao g·ªìm c∆° ch·∫ø thu nh·∫≠n d·ªØ li·ªáu v√† c√°c ph√©p bi·∫øn ƒë·ªïi d·ªØ li·ªáu c∆° b·∫£n.

## 1. Receiver: C∆° ch·∫ø Thu nh·∫≠n D·ªØ li·ªáu (Ingestion)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
**Receiver** l√† th√†nh ph·∫ßn quan tr·ªçng trong ki·∫øn tr√∫c DStream c·ªßa Spark Streaming. N√≥ ƒë√≥ng vai tr√≤ l√† c·∫ßu n·ªëi gi·ªØa ngu·ªìn d·ªØ li·ªáu b√™n ngo√†i (nh∆∞ Kafka, Kafka, Flume, ho·∫∑c file log) v√† Spark Engine.

*   **Discretized Stream (DStream):** L√† l·ªõp tr·ª´u t∆∞·ª£ng ch√≠nh, ƒë·∫°i di·ªán cho m·ªôt lu·ªìng d·ªØ li·ªáu li√™n t·ª•c.
*   **C∆° ch·∫ø ho·∫°t ƒë·ªông:** Receiver s·∫Ω "nghe" d·ªØ li·ªáu t·ª´ ngu·ªìn, sau ƒë√≥ **discretize** (chia nh·ªè) lu·ªìng d·ªØ li·ªáu li√™n t·ª•c th√†nh c√°c **Batch** (kh·ªëi d·ªØ li·ªáu theo th·ªùi gian).
*   **L∆∞u tr·ªØ:** C√°c batch n√†y ƒë∆∞·ª£c l∆∞u v√†o b·ªô nh·ªõ (Memory) c·ªßa Spark v√† ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng **RDD**. DStream th·ª±c ch·∫•t l√† m·ªôt chu·ªói c√°c RDD li√™n ti·∫øp nhau.

### Khi n√†o s·ª≠ d·ª•ng?
*   Khi b·∫°n c·∫ßn k·∫øt n·ªëi v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu tr·ª±c ti·∫øp (Direct Stream) nh∆∞ Kafka, ZeroMQ, Flume.
*   Khi d·ªØ li·ªáu c·∫ßn ƒë∆∞·ª£c buffer t·∫°m th·ªùi tr∆∞·ªõc khi x·ª≠ l√Ω ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t d·ªØ li·ªáu n·∫øu l·ªói x·∫£y ra.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
Trong Spark Streaming, b·∫°n th∆∞·ªùng t·∫°o m·ªôt `StreamingContext` v√† t·∫°o InputDStream t·ª´ Receiver.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | ƒê∆°n gi·∫£n ƒë·ªÉ thi·∫øt l·∫≠p v·ªõi nhi·ªÅu ngu·ªìn d·ªØ li·ªáu (Kafka, Flume...). H·ªó tr·ª£ c∆° ch·∫ø **Write Ahead Logs (WAL)** ƒë·ªÉ ph·ª•c h·ªìi sau l·ªói. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | C√≥ th·ªÉ g√¢y ra **Single Point of Failure (SPOF)** n·∫øu Receiver b·ªã l·ªói (m·∫∑c d√π c√≥ th·ªÉ kh·∫Øc ph·ª•c b·∫±ng ReceiverÂÜó‰Ωô). C√≥ th·ªÉ g√¢y m·∫•t d·ªØ li·ªáu (At-least-once semantics) n·∫øu kh√¥ng c·∫•u h√¨nh k·ªπ l∆∞·ª°ng. |

---

## 2. Transformations tr√™n DStreams

### Gi·∫£i th√≠ch Kh√°i ni·ªám
T∆∞∆°ng t·ª± nh∆∞ RDD, **DStream** cung c·∫•p nhi·ªÅu ph√©p bi·∫øn ƒë·ªïi (Transformation) cho ph√©p chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ tr·∫°ng th√°i n√†y sang tr·∫°ng th√°i kh√°c. C√°c ph√©p bi·∫øn ƒë·ªïi n√†y ƒë∆∞·ª£c √°p d·ª•ng l√™n t·ª´ng RDD trong chu·ªói DStream.

**C√°c ph√©p bi·∫øn ƒë·ªïi ph·ªï bi·∫øn:**
*   **Map:** √Ånh x·∫° m·ªói ph·∫ßn t·ª≠ th√†nh m·ªôt ph·∫ßn t·ª≠ m·ªõi.
*   **FlatMap:** √Ånh x·∫° m·ªói ph·∫ßn t·ª≠ th√†nh 0 ho·∫∑c nhi·ªÅu ph·∫ßn t·ª≠.
*   **Filter:** L·ªçc d·ªØ li·ªáu d·ª±a tr√™n ƒëi·ªÅu ki·ªán.
*   **Reduce:** T√≠ch h·ª£p d·ªØ li·ªáu (t·ªïng h·ª£p) trong m·ªói batch.
*   **GroupBy:** Nh√≥m d·ªØ li·ªáu theo key.

### V√≠ d·ª• Th·ª±c t·∫ø trong Industry
*   **Log Processing:** D√πng `Filter` ƒë·ªÉ lo·∫°i b·ªè c√°c log kh√¥ng quan tr·ªçng, `Map` ƒë·ªÉ tr√≠ch xu·∫•t timestamp v√† l·ªói.
*   **Real-time ETL:** D√πng `Map` ƒë·ªÉ chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng JSON sang Object, `Reduce` ƒë·ªÉ t√≠nh t·ªïng doanh thu theo t·ª´ng ph√∫t.

---

## 3. Map vs. FlatMap

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê√¢y l√† hai ph√©p bi·∫øn ƒë·ªïi c∆° b·∫£n nh·∫•t ƒë·ªÉ x·ª≠ l√Ω t·ª´ng d√≤ng d·ªØ li·ªáu.

*   **`map(func)`:** √Åp d·ª•ng h√†m `func` l√™n t·ª´ng ph·∫ßn t·ª≠ c·ªßa DStream ngu·ªìn v√† tr·∫£ v·ªÅ m·ªôt DStream m·ªõi v·ªõi c√πng s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ (m·ªói ph·∫ßn t·ª≠ ƒë·∫ßu v√†o t∆∞∆°ng ·ª©ng 1 ph·∫ßn t·ª≠ ƒë·∫ßu ra).
*   **`flatMap(func)`:** T∆∞∆°ng t·ª± `map`, nh∆∞ng h√†m `func` tr·∫£ v·ªÅ m·ªôt **sequence (danh s√°ch)** c√°c ph·∫ßn t·ª≠ thay v√¨ m·ªôt ph·∫ßn t·ª≠ ƒë∆°n l·∫ª. Sau ƒë√≥, sequence n√†y‰ºöË¢´ "ph·∫≥ng" h√≥a (flatten) th√†nh c√°c ph·∫ßn t·ª≠ ri√™ng l·∫ª trong DStream ƒë·∫ßu ra.

### V√≠ d·ª• Minh h·ªça
*   **Input:** `[ [1,2,3], [4,5,6], [7,8,9] ]` (M·ªôt DStream ch·ª©a 3 RDD con, m·ªói RDD l√† m·ªôt list).
*   **Map:** N·∫øu d√πng `map(x => x.sum)`, Output s·∫Ω l√† `[6, 15, 24]`.
*   **FlatMap:** N·∫øu d√πng `flatMap(x => x)`, Output s·∫Ω l√† `[1, 2, 3, 4, 5, 6, 7, 8, 9]`.

### Code M·∫´u (PySpark)

```python
from pyspark.streaming import StreamingContext

# Kh·ªüi t·∫°o StreamingContext (Batch Duration = 1 gi√¢y)
ssc = StreamingContext(sc, 1)

# Gi·∫£ l·∫≠p DStream ƒë·∫ßu v√†o (v√≠ d·ª•: list c√°c list)
data = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]
input_rdd = sc.parallelize(data)
dstream = ssc.queueStream([input_rdd])

# 1. S·ª≠ d·ª•ng MAP
# M·ªói list s·∫Ω ƒë∆∞·ª£c c·ªông l·∫°i th√†nh 1 s·ªë
def map_func(rdd):
    return rdd.map(lambda x: sum(x))
mapped_stream = dstream.map(map_func)

# 2. S·ª≠ d·ª•ng FLATMAP
# M·ªü ph·∫≥ng list th√†nh c√°c ph·∫ßn t·ª≠ ri√™ng l·∫ª
def flatmap_func(rdd):
    return rdd.flatMap(lambda x: x)
flatmapped_stream = dstream.flatMap(flatmap_func)

# In k·∫øt qu·∫£
mapped_stream.pprint()      # Output: 6, 15, 24
flatmapped_stream.pprint()  # Output: 1, 2, 3, 4, 5, 6, 7, 8, 9

ssc.start()
ssc.awaitTermination()
```

---

## 4. Filter & Reduce

### Gi·∫£i th√≠ch Kh√°i ni·ªám

#### A. Filter(func)
*   **M·ª•c ƒë√≠ch:** L·ªçc d·ªØ li·ªáu.
*   **C∆° ch·∫ø:** Duy·ªát qua t·ª´ng ph·∫ßn t·ª≠ trong RDD c·ªßa batch. N·∫øu h√†m `func(partition)` tr·∫£ v·ªÅ `True`, ph·∫ßn t·ª≠ ƒë√≥ ƒë∆∞·ª£c gi·ªØ l·∫°i. N·∫øu `False`, b·ªã lo·∫°i b·ªè.
*   **K·∫øt qu·∫£:** DStream m·ªõi ch·ª©a √≠t ph·∫ßn t·ª≠ h∆°n (ho·∫∑c c√≥ th·ªÉ r·ªóng).

#### B. Reduce(func)
*   **M·ª•c ƒë√≠ch:** T√≠ch h·ª£p d·ªØ li·ªáu (Aggregation) trong m·ªói batch.
*   **C∆° ch·∫ø:** √Åp d·ª•ng h√†m `func` (c√≥ t√≠nh giao ho√°n v√† k·∫øt h·ª£p) ƒë·ªÉ k·∫øt h·ª£p c√°c ph·∫ßn t·ª≠ trong c√πng m·ªôt RDD th√†nh m·ªôt gi√° tr·ªã duy nh·∫•t.
*   **K·∫øt qu·∫£:** M·ªói RDD trong DStream s·∫Ω ch·ªâ c√≤n l·∫°i **1 ph·∫ßn t·ª≠**.

### Code M·∫´u (PySpark)

```python
# Gi·∫£ l·∫≠p d·ªØ li·ªáu ƒë·∫ßu v√†o: C√°c s·ª± ki·ªán c√≥ c·∫•u tr√∫c (id, value)
# Format: (event_type, amount)
raw_data = [
    ("sale", 100), ("refund", -20), ("sale", 50),
    ("sale", 200), ("error", 0), ("refund", -10)
]
input_rdd = sc.parallelize(raw_data)
dstream = ssc.queueStream([input_rdd])

# 1. FILTER: L·ªçc ch·ªâ l·∫•y c√°c giao d·ªãch 'sale'
def is_sale(event):
    return event[0] == "sale"

filtered_stream = dstream.filter(is_sale)
# K·∫øt qu·∫£: [("sale", 100), ("sale", 50), ("sale", 200)]

# 2. REDUCE: T√≠nh t·ªïng ti·ªÅn c·ªßa t·∫•t c·∫£ giao d·ªãch trong batch
# H√†m reduce: lambda acc, new: acc + new_value
# C·∫ßn tr√≠ch xu·∫•t value (index 1) tr∆∞·ªõc khi reduce
def reduce_func(rdd):
    # B∆∞·ªõc 1: Map ƒë·ªÉ l·∫•y value
    values = rdd.map(lambda x: x[1])
    # B∆∞·ªõc 2: Reduce ƒë·ªÉ c·ªông
    return values.reduce(lambda a, b: a + b)

reduced_stream = dstream.map(lambda x: x[1]).reduce(lambda a, b: a + b)

# Ho·∫∑c vi·∫øt g·ªçn trong 1 d√≤ng:
reduced_stream = dstream.map(lambda x: x[1]).reduce(lambda a, b: a + b)

# In k·∫øt qu·∫£
filtered_stream.pprint()
reduced_stream.pprint() # Output: 100 + 50 + 200 - 20 - 10 = 320 (To√†n b·ªô batch)

ssc.start()
ssc.awaitTermination()
```

### Khi n√†o s·ª≠ d·ª•ng?
*   **Filter:** Khi c·∫ßn lo·∫°i b·ªè r√°c (spam, log debug), ho·∫∑c ph√¢n t√°ch lu·ªìng d·ªØ li·ªáu (v√≠ d·ª•: t√°ch ri√™ng stream "Order" v√† stream "Payment").
*   **Reduce:** Khi c·∫ßn th·ªëng k√™ nhanh trong m·ªôt kho·∫£ng th·ªùi gian ng·∫Øn (Batch), v√≠ d·ª•: S·ªë l∆∞·ª£ng request m·ªói 5 gi√¢y, T·ªïng ti·ªÅn giao d·ªãch m·ªói 10 gi√¢y.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ph√©p to√°n | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
| :--- | :--- | :--- |
| **Filter** | Gi·∫£m dung l∆∞·ª£ng d·ªØ li·ªáu x·ª≠ l√Ω ·ªü c√°c b∆∞·ªõc sau, t·ªëi ∆∞u hi·ªáu nƒÉng. | C√≥ th·ªÉ l√†m m·∫•t d·ªØ li·ªáu n·∫øu ƒëi·ªÅu ki·ªán l·ªçc sai. |
| **Reduce** | R·∫•t nhanh, gi·∫£m b·ªô nh·ªõ (ch·ªâ tr·∫£ v·ªÅ 1 ph·∫ßn t·ª≠). | Ch·ªâ t√≠nh to√°n tr√™n t·ª´ng Batch ri√™ng l·∫ª (kh√¥ng t√≠nh to√†n c·ª•c li√™n t·ª•c tr·ª´ khi d√πng `updateStateByKey`). |

---

## T√≥m t·∫Øt T·ªïng quan

| Kh√°i ni·ªám | M√¥ t·∫£ | V√≠ d·ª• ƒëi·ªÉn h√¨nh |
| :--- | :--- | :--- |
| **Receiver** | Component thu nh·∫≠n d·ªØ li·ªáu, chia lu·ªìng th√†nh Batch RDD. | ƒê·ªçc log t·ª´ Kafka. |
| **Map** | 1-1 transformation. | Chuy·ªÉn ƒë·ªïi JSON string sang Object. |
| **FlatMap** | 1-N transformation (ph·∫≥ng h√≥a). | T√°ch m·ªôt c√¢u vƒÉn th√†nh c√°c t·ª´ (Word Count). |
| **Filter** | Lo·∫°i b·ªè d·ªØ li·ªáu kh√¥ngÊª°Ë∂≥ ƒëi·ªÅu ki·ªán. | L·ªçc c√°c d√≤ng log c√≥ level l√† "ERROR". |
| **Reduce** | Aggregation (1-1 trong batch). | T√≠nh t·ªïng s·ªë l∆∞·ª£ng view trong 10 gi√¢y. |

---

Ch√†o b·∫°n, v·ªõi vai tr√≤ l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n, t√¥i s·∫Ω ph√¢n t√≠ch v√† tr√¨nh b√†y chi ti·∫øt n·ªôi dung t·ª´ c√°c slide b·∫°n cung c·∫•p v·ªÅ Apache Spark Streaming.

D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu ƒë∆∞·ª£c h·ªá th·ªëng l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát, tu√¢n th·ªß c√°c y√™u c·∫ßu b·∫°n ƒë√£ ƒë·ªÅ ra.

***

# Apache Spark Streaming: Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Chi ti·∫øt

T√†i li·ªáu n√†y cung c·∫•p c√°i nh√¨n t·ªïng quan v·ªÅ c√°c thao t√°c quan tr·ªçng trong Spark Streaming, bao g·ªìm `groupBy`, Window Operations v√† Output Operations.

---

## 1. Ph√©p to√°n `groupBy`

### Kh√°i ni·ªám
Trong Spark (c·∫£ RDD v√† DStream), `groupBy(func)` l√† m·ªôt ph√©p to√°n transformation m·∫°nh m·∫Ω. N√≥ l·∫•y m·ªôt RDD/DStream ch·ª©a c√°c c·∫∑p `(K, V)` ho·∫∑c c√°c ph·∫ßn t·ª≠ ƒë∆°n l·∫ª, √°p d·ª•ng m·ªôt h√†m ƒë·ªÉ tr√≠ch xu·∫•t kh√≥a (`key`), sau ƒë√≥ nh√≥m t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c√≥ c√πng kh√≥a l·∫°i v·ªõi nhau.

K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† m·ªôt RDD/DStream m·ªõi c√≥ ki·ªÉu `(K, Iterable<V>)`, t·ª©c l√† m·ªói kh√≥a s·∫Ω √°nh x·∫° ƒë·∫øn m·ªôt t·∫≠p h·ª£p c√°c gi√° tr·ªã thu·ªôc v·ªÅ kh√≥a ƒë√≥.

### Gi·∫£i th√≠ch Chi ti·∫øt
- **Input**: M·ªôt t·∫≠p h·ª£p d·ªØ li·ªáu b·∫•t k·ª≥.
- **H√†m `func`**: H√†m n√†y ƒë·ªãnh nghƒ©a c√°ch tr√≠ch xu·∫•t kh√≥a t·ª´ m·ªôt ph·∫ßn t·ª≠ d·ªØ li·ªáu.
- **Output**: M·ªôt nh√≥m c√°c ph·∫ßn t·ª≠ ƒë∆∞·ª£c gom l·∫°i theo kh√≥a.

### Code M·∫´u (PySpark - DStream)

Gi·∫£ s·ª≠ ch√∫ng ta c√≥ m·ªôt DStream c·ªßa c√°c d√≤ng vƒÉn b·∫£n, v√† ch√∫ng ta mu·ªën ƒë·∫øm s·ªë l∆∞·ª£ng t·ª´ xu·∫•t hi·ªán (Word Count), ƒë√¢y ch√≠nh l√† m·ªôt b√†i to√°n kinh ƒëi·ªÉn s·ª≠ d·ª•ng `groupByKey` (ho·∫∑c `reduceByKey` hi·ªáu qu·∫£ h∆°n).

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Kh·ªüi t·∫°o Spark Context v√† Streaming Context
sc = SparkContext("local[2]", "GroupByApp")
ssc = StreamingContext(sc, 1) # Batch interval = 1 gi√¢y

# T·∫°o DStream t·ª´ socket (gi·∫£ s·ª≠ d·ªØ li·ªáu g·ª≠i v√†o port 9999)
lines = ssc.socketTextStream("localhost", 9999)

# 1. T√°ch c√°c d√≤ng th√†nh t·ª´
words = lines.flatMap(lambda line: line.split(" "))

# 2. Map m·ªói t·ª´ th√†nh m·ªôt c·∫∑p (from, 1)
pairs = words.map(lambda word: (word, 1))

# 3. S·ª≠ d·ª•ng groupByKey ƒë·ªÉ nh√≥m c√°c gi√° tr·ªã theo t·ª´ (key)
# K·∫øt qu·∫£ tr·∫£ v·ªÅ: (word, [1, 1, 1, ...])
grouped = pairs.groupByKey()

# 4. T√≠nh to√°n t·ªïng s·ªë l∆∞·ª£ng t·ª´ b·∫±ng c√°ch l·∫•y ƒë·ªô d√†i c·ªßa list values
word_counts = grouped.map(lambda x: (x[0], len(x[1])))

# In k·∫øt qu·∫£
word_counts.pprint()

# B·∫Øt ƒë·∫ßu lu·ªìng x·ª≠ l√Ω
ssc.start()
ssc.awaitTermination()
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

| Ti√™u ch√≠ | M√¥ t·∫£ |
| :--- | :--- |
| **Khi n√†o s·ª≠ d·ª•ng?** | Khi b·∫°n c·∫ßn gom t·∫•t c·∫£ c√°c gi√° tr·ªã t∆∞∆°ng ·ª©ng v·ªõi m·ªôt kh√≥a duy nh·∫•t l·∫°i v·ªõi nhau ƒë·ªÉ x·ª≠ l√Ω sau ƒë√≥ (v√≠ d·ª•: t√≠nh t·ªïng, trung b√¨nh, ho·∫∑c l∆∞u tr·ªØ danh s√°ch). |
| **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** | √Åp d·ª•ng tr·ª±c ti·∫øp l√™n RDD/DStream. C·∫ßn ƒë·ªãnh nghƒ©a h√†m `key` (n·∫øu input l√† object) ho·∫∑c map v·ªÅ d·∫°ng `(key, value)` tr∆∞·ªõc. |
| **∆Øu ƒëi·ªÉm** | Linh ho·∫°t, cho ph√©p x·ª≠ l√Ω logic ph·ª©c t·∫°p tr√™n t·∫≠p h·ª£p c√°c gi√° tr·ªã c·ªßa c√πng m·ªôt kh√≥a. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | **C·∫©n th·∫≠n v·ªõi b·ªô nh·ªõ**: N·∫øu m·ªôt kh√≥a c√≥ qu√° nhi·ªÅu gi√° tr·ªã (skewed data), n√≥ c√≥ th·ªÉ g√¢y tr√†n b·ªô nh·ªõ (OOM). Hi·ªáu su·∫•t th∆∞·ªùng th·∫•p h∆°n `reduceByKey` v√¨ kh√¥ng t·ªëi ∆∞u h√≥a tr∆∞·ªõc (shuffling to√†n b·ªô d·ªØ li·ªáu thay v√¨ aggregation c·ª•c b·ªô). |

---

## 2. DStream Window Operations (C·ª≠a s·ªï tr∆∞·ª£t)

### Kh√°i ni·ªám
Spark Streaming cung c·∫•p c∆° ch·∫ø **Window Operations** (thao t√°c c·ª≠a s·ªï), cho ph√©p th·ª±c thi c√°c transformation tr√™n m·ªôt "c·ª≠a s·ªï" d·ªØ li·ªáu tr∆∞·ª£t theo th·ªùi gian. C·ª≠a s·ªï n√†y ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·ªüi hai th√¥ng s·ªë ch√≠nh:
1.  **Window length**: ƒê·ªô d√†i c·ªßa c·ª≠a s·ªï (v√≠ d·ª•: 30 gi√¢y).
2.  **Sliding interval**: Kho·∫£ng th·ªùi gian c·ª≠a s·ªï tr∆∞·ª£t ƒëi (v√≠ d·ª•: 10 gi√¢y).

### Gi·∫£i th√≠ch Chi ti·∫øt
- **H√¨nh dung**: T∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang quan s√°t d·ªØ li·ªáu trong m·ªôt khung c·ª≠a s·ªï di chuy·ªÉn v·ªÅ ph√≠a tr∆∞·ªõc theo th·ªùi gian.
- **V√≠ d·ª•**: `windowLength = 30s`, `slideInterval = 10s`.
    *   T·∫°i th·ªùi ƒëi·ªÉm T=0s: X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ T=0 ƒë·∫øn T=30s.
    *   T·∫°i th·ªùi ƒëi·ªÉm T=10s: X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ T=10 ƒë·∫øn T=40s.
    *   T·∫°i th·ªùi ƒëi·ªÉm T=20s: X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ T=20 ƒë·∫øn T=50s.
- C√°c ph√©p to√°n ph·ªï bi·∫øn: `reduceByWindow`, `reduceByKeyAndWindow`, `countByWindow`.

### Code M·∫´u (PySpark - Window)

V√≠ d·ª•: ƒê·∫øm s·ªë l∆∞·ª£ng t·ª´ trong m·ªôt c·ª≠a s·ªï 30 gi√¢y, tr∆∞·ª£t m·ªói 10 gi√¢y.

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "WindowOpsApp")
ssc = StreamingContext(sc, 1) # Batch interval = 1 gi√¢y

lines = ssc.socketTextStream("localhost", 9999)
words = lines.flatMap(lambda line: line.split(" "))

# √Åp d·ª•ng Window Operation
# windowDuration: 30 gi√¢y
# slidingDuration: 10 gi√¢y
windowedWordCounts = words.countByWindow(windowDuration=30, slidingDuration=10)

windowedWordCounts.pprint()

ssc.start()
ssc.awaitTermination()
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

| Ti√™u ch√≠ | M√¥ t·∫£ |
| :--- | :--- |
| **Khi n√†o s·ª≠ d·ª•ng?** | Khi b·∫°n c·∫ßn ph√¢n t√≠ch xu h∆∞·ªõng, th·ªëng k√™ theo kho·∫£ng th·ªùi gian li√™n t·ª•c (v√≠ d·ª•: L∆∞·ª£t view trong 1 gi·ªù qua, L∆∞·ª£ng giao d·ªãch trung b√¨nh 5 ph√∫t). |
| **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** | S·ª≠ d·ª•ng c√°c h√†m c√≥ t·ª´ kh√≥a `Window` nh∆∞ `.window()`, `.countByWindow()`, `.reduceByKeyAndWindow()`. |
| **∆Øu ƒëi·ªÉm** | R·∫•t m·∫°nh m·∫Ω cho ph√¢n t√≠ch th·ªùi gian th·ª±c, gi√∫p l√†m m∆∞·ª£t d·ªØ li·ªáu v√† ph√°t hi·ªán c√°c s·ª± ki·ªán k√©o d√†i. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | **Chi ph√≠ t√≠nh to√°n cao**: C·∫ßn l∆∞u tr·ªØ d·ªØ li·ªáu c·ªßa to√†n b·ªô c·ª≠a s·ªï (ho·∫∑c √≠t nh·∫•t l√† c√°c batch trong c·ª≠a s·ªï) trong b·ªô nh·ªõ. N·∫øu c·ª≠a s·ªï qu√° l·ªõn, h·ªá th·ªëng c√≥ th·ªÉ qu√° t·∫£i. |

---

## 3. Output Operations (ƒê∆∞a d·ªØ li·ªáu ra ngo√†i)

### Kh√°i ni·ªám
**Output Operations** l√† c√°c thao t√°c cu·ªëi c√πng trong pipeline Spark Streaming. N·∫øu kh√¥ng c√≥ Output Operation, DStream s·∫Ω kh√¥ng ƒë∆∞·ª£c k√≠ch ho·∫°t th·ª±c thi (lazily evaluated). C√°c thao t√°c n√†y cho ph√©p d·ªØ li·ªáu t·ª´ DStream ƒë∆∞·ª£c ghi ra c√°c h·ªá th·ªëng b√™n ngo√†i (External Systems) nh∆∞ c∆° s·ªü d·ªØ li·ªáu (CSDL), file system, ho·∫∑c dashboard.

### Gi·∫£i th√≠ch Chi ti·∫øt
- **T·∫°i sao c·∫ßn?**: Spark Streaming l√† m·ªôt h·ªá th·ªëng x·ª≠ l√Ω lu·ªìng. D·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω v√† c·∫ßn ƒë∆∞·ª£c l∆∞u tr·ªØ ho·∫∑c g·ª≠i ƒëi ƒë·ªÉ c√°c h·ªá th·ªëng kh√°c s·ª≠ d·ª•ng.
- **K√≠ch ho·∫°t Execution**: C√°c ph√©p to√°n transformation (map, filter, groupBy) ch·ªâ x√¢y d·ª±ng ƒë·ªì th·ªã t√≠nh to√°n (logical plan). Ch·ªâ khi m·ªôt Output Operation ƒë∆∞·ª£c g·ªçi, Spark m·ªõi b·∫Øt ƒë·∫ßu nh·∫≠n d·ªØ li·ªáu v√† x·ª≠ l√Ω (physical execution).

### Code M·∫´u (PySpark - foreachRDD)

`foreachRDD` l√† m·ªôt trong nh·ªØng Output Operations linh ho·∫°t nh·∫•t, cho ph√©p b·∫°n truy c·∫≠p v√†o RDD b√™n trong c·ªßa DStream t·∫°i m·ªói batch ƒë·ªÉ ghi d·ªØ li·ªáu ra ngo√†i.

```python
# Gi·∫£ s·ª≠ ƒë√£ c√≥ DStream 'word_counts' t·ª´ v√≠ d·ª• tr∆∞·ªõc
# word_counts.pprint() l√† m·ªôt Output Operation c∆° b·∫£n

# S·ª≠ d·ª•ng foreachRDD ƒë·ªÉ ghi d·ªØ li·ªáu v√†o m·ªôt file ho·∫∑c database
def save_to_db(rdd):
    if not rdd.isEmpty():
        # V√≠ d·ª•: Ghi v√†o file text
        # L∆∞u √Ω: M·ªói batch s·∫Ω ghi ƒë√® file n·∫øu d√πng mode 'w'
        # ƒê·ªÉ append, c·∫ßn d√πng th∆∞ vi·ªán ho·∫∑c c∆° ch·∫ø kh√°c
        with open("output.txt", "a") as f:
            for (word, count) in rdd.collect():
                f.write(f"Word: {word}, Count: {count}\n")

# √Åp d·ª•ng Output Operation
word_counts.foreachRDD(save_to_db)

ssc.start()
ssc.awaitTermination()
```

### V√≠ d·ª• Th·ª±c t·∫ø trong Ng√†nh C√¥ng Nghi·ªáp

1.  **Real-time Dashboard**: D·ªØ li·ªáu ƒë∆∞·ª£c t√≠nh to√°n theo c·ª≠a s·ªï (Window) v√† ghi v√†o Elasticsearch. Kibana s·∫Ω truy v·∫•n Elasticsearch ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì realtime.
2.  **Fraud Detection (Ph√°t hi·ªán gian l·∫≠n)**: D·ªØ li·ªáu giao d·ªãch ƒë∆∞·ª£c x·ª≠ l√Ω, n·∫øu ph√°t hi·ªán h√†nh vi b·∫•t th∆∞·ªùng (d√πng `groupBy` theo user ID v√† ki·ªÉm tra t·∫ßn su·∫•t), h·ªá th·ªëng s·∫Ω ghi ngay v√†o Cassandra ho·∫∑c g·ª≠i c·∫£nh b√°o qua Kafka.
3.  **IoT Monitoring**: H√†ng tri·ªáu c·∫£m bi·∫øn g·ª≠i d·ªØ li·ªáu. Spark Streaming nh·∫≠n t·ª´ Kafka, d√πng `Window` ƒë·ªÉ t√≠nh trung b√¨nh nhi·ªát ƒë·ªô m·ªói 5 ph√∫t, v√† l∆∞u v√†o InfluxDB ƒë·ªÉ gi√°m s√°t.

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng Output Operations

| Ti√™u ch√≠ | M√¥ t·∫£ |
| :--- | :--- |
| **Khi n√†o s·ª≠ d·ª•ng?** | Lu√¥n lu√¥n l√† b∆∞·ªõc cu·ªëi c√πng c·ªßa pipeline. Khi b·∫°n c·∫ßn l∆∞u k·∫øt qu·∫£ ho·∫∑c ph·∫£n h·ªìi l·∫°i h·ªá th·ªëng. |
| **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** | C√°c h√†m: `print()`, `saveAsTextFiles()`, `foreachRDD()`. Trong ƒë√≥ `foreachRDD` l√† t√πy bi·∫øn nh·∫•t. |
| **∆Øu ƒëi·ªÉm** | Cho ph√©p t√≠ch h·ª£p v·ªõi b·∫•t k·ª≥ h·ªá th·ªëng b√™n ngo√†i n√†o (JDBC, NoSQL, API). |
| **Nh∆∞·ª£c ƒëi·ªÉm** | **L·ªói ghi d·ªØ li·ªáu (Exactly-once semantics)**: N·∫øu x·ª≠ l√Ω kh√¥ng c·∫©n th·∫≠n trong `foreachRDD`, b·∫°n c√≥ th·ªÉ ghi tr√πng l·∫∑p d·ªØ li·ªáu ho·∫∑c m·∫•t d·ªØ li·ªáu khi l·ªói. C·∫ßn ƒë·∫£m b·∫£o k·∫øt n·ªëi c∆° s·ªü d·ªØ li·ªáu an to√†n v√† c√≥ c∆° ch·∫ø transaction. |

---

## T√≥m t·∫Øt Ph√¢n t√≠ch

Slide n√†y ƒë√£ gi·ªõi thi·ªáu 3 tr·ª• c·ªôt quan tr·ªçng c·ªßa Spark Streaming:
1.  **`groupBy`**: ƒê·ªÉ gom nh√≥m d·ªØ li·ªáu theo kh√≥a.
2.  **Window**: ƒê·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu tr√™n c√°c khung th·ªùi gian tr∆∞·ª£t.
3.  **Output Ops**: ƒê·ªÉ xu·∫•t k·∫øt qu·∫£ ra ngo√†i h·ªá th·ªëng, k√≠ch ho·∫°t to√†n b·ªô qu√° tr√¨nh x·ª≠ l√Ω.

Vi·ªác k·∫øt h·ª£p nhu·∫ßn nhuy·ªÖn c√°c kh√°i ni·ªám n√†y cho ph√©p x√¢y d·ª±ng c√°c h·ªá th·ªëng x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c ph·ª©c t·∫°p v√† m·∫°nh m·∫Ω.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "8_spark_streaming.pdf" c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát theo y√™u c·∫ßu.

---

# Ph√¢n T√≠ch v√† H∆∞·ªõng D·∫´n Chi Ti·∫øt v·ªÅ C√°c T√≠nh NƒÉng N√¢ng Cao trong Spark Streaming

T√†i li·ªáu n√†y t·∫≠p trung v√†o c√°c c∆° ch·∫ø t·ªëi ∆∞u h√≥a v√† ƒë·∫£m b·∫£o ƒë·ªô tin c·∫≠y (Reliability) cho c√°c ·ª©ng d·ª•ng Spark Streaming. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt t·ª´ng kh√°i ni·ªám.

## 1. Caching and Persistence (B·ªô nh·ªõ ƒë·ªám v√† L∆∞u tr·ªØ)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Trong x·ª≠ l√Ω d·ªØ li·ªáu stream, d·ªØ li·ªáu th∆∞·ªùng ƒë∆∞·ª£c x·ª≠ l√Ω theo t·ª´ng batch (l√¥). N·∫øu m·ªôt DStream (Discretized Stream) c·ª• th·ªÉ c·∫ßn ƒë∆∞·ª£c s·ª≠ d·ª•ng l·∫°i trong nhi·ªÅu pipeline t√≠nh to√°n kh√°c nhau (v√≠ d·ª•: v·ª´a d√πng ƒë·ªÉ c·∫≠p nh·∫≠t model Machine Learning, v·ª´a d√πng ƒë·ªÉ ghi log), vi·ªác t√≠nh to√°n l·∫°i n√≥ t·ª´ ƒë·∫ßu m·ªói l·∫ßn l√† v√¥ c√πng t·ªën k√©m.

**Caching/Persistence** cho ph√©p l∆∞u tr·ªØ d·ªØ li·ªáu c·ªßa DStream v√†o b·ªô nh·ªõ (Memory) ho·∫∑c l∆∞u tr·ªØ b·ªÅn v·ªØng (Disk). Khi c·∫ßn, Spark c√≥ th·ªÉ truy xu·∫•t d·ªØ li·ªáu n√†y m√† kh√¥ng c·∫ßn ph·∫£i x·ª≠ l√Ω l·∫°i t·ª´ ngu·ªìn.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
- **Ph∆∞∆°ng th·ª©c:** S·ª≠ d·ª•ng h√†m `persist()` tr√™n ƒë·ªëi t∆∞·ª£ng DStream.
- **C·∫•u h√¨nh:** V·ªõi c√°c ngu·ªìn d·ªØ li·ªáu m·∫°ng (Kafka, Flume, Sockets), d·ªØ li·ªáu th∆∞·ªùng ƒë∆∞·ª£c sao ch√©p (replicate) ƒë·∫øn √≠t nh·∫•t 2 n√∫t ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªãu l·ªói n·∫øu m·ªôt n√∫t b·ªã h·ªèng.

### Code M·∫´u (Python - PySpark)

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Kh·ªüi t·∫°o Context
sc = SparkContext("local[2]", "CacheExample")
ssc = StreamingContext(sc, 1)

# T·∫°o DStream t·ª´ socket
lines = ssc.socketTextStream("localhost", 9999)

# Th·ª±c hi·ªán bi·∫øn ƒë·ªïi (Transformation)
words = lines.flatMap(lambda line: line.split(" "))

# **PERSISTENCE**: L∆∞u tr·ªØ DStream 'words' v√†o b·ªô nh·ªõ
# MEMORY_ONLY l√† m·∫∑c ƒë·ªãnh, nh∆∞ng c√≥ th·ªÉ ch·ªçn DISK_ONLY n·∫øu d·ªØ li·ªáu l·ªõn
words.persist()

# S·ª≠ d·ª•ng l·∫°i DStream ƒë√£ l∆∞u tr·ªØ cho nhi·ªÅu t√°c v·ª• kh√°c nhau
# 1. ƒê·∫øm t·ª´
word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)
word_counts.pprint()

# 2. L·ªçc c√°c t·ª´ d√†i (s·ª≠ d·ª•ng l·∫°i bi·∫øn 'words' m√† kh√¥ng c·∫ßn t√≠nh to√°n l·∫°i t·ª´ 'lines')
long_words = words.filter(lambda word: len(word) > 5)
long_words.pprint()

ssc.start()
ssc.awaitTermination()
```

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | - **TƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω:** Tr√°nh t√≠nh to√°n l·∫°i c√°c ph√©p bi·∫øn ƒë·ªïi l·∫∑p l·∫°i.<br>- **Ph·ª•c v·ª• nhi·ªÅu m·ª•c ƒë√≠ch:** C√πng m·ªôt d·ªØ li·ªáu th√¥ c√≥ th·ªÉ d√πng cho nhi·ªÅu logic nghi·ªáp v·ª• kh√°c nhau. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **Ti√™u t·ªën t√†i nguy√™n RAM/Storage:** C·∫ßn c√¢n ƒë·ªëi b·ªô nh·ªõ c·ªßa cluster.<br>- **ƒê·ªô tr·ªÖ (Latency):** Qu√° tr√¨nh ghi v√†o b·ªô nh·ªõ ƒë·ªám c√≥ th·ªÉ l√†m tƒÉng th·ªùi gian x·ª≠ l√Ω m·ªôt batch nh·ªè. |

### V√≠ d·ª• th·ª±c t·∫ø
H·ªá th·ªëng **Real-time Fraud Detection (Ph√°t hi·ªán gian l·∫≠n)**:
- D·ªØ li·ªáu giao d·ªãch (Transaction) ƒë∆∞·ª£c nh·∫≠n t·ª´ Kafka.
- D·ªØ li·ªáu ƒë∆∞·ª£c `persist()` ƒë·ªÉ v·ª´a d√πng t√≠nh to√°n score gian l·∫≠n (Model 1), v·ª´a d√πng ƒë·ªÉ ghi log nghi·ªáp v·ª• (Model 2) m√† kh√¥ng c·∫ßn parse JSON l·∫°i t·ª´ Kafka.

---

## 2. Accumulators (B·ªô t√≠ch l≈©y)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
**Accumulator** l√† m·ªôt bi·∫øn chia s·∫ª (Shared Variable) ch·ªâ cho ph√©p th·ª±c hi·ªán ph√©p c·ªông (ho·∫∑c c√°c ph√©p to√°n k·∫øt h·ª£p/commutative kh√°c). N√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ theo d√µi c√°c ch·ªâ s·ªë trong su·ªët qu√° tr√¨nh th·ª±c thi Job, v√≠ d·ª• nh∆∞ ƒë·∫øm s·ªë l∆∞·ª£ng l·ªói, t·ªïng s·ªë b·∫£n ghi ƒë√£ x·ª≠ l√Ω, ho·∫∑c s·ªë l∆∞·ª£ng c·∫£nh b√°o.

Spark UI hi·ªÉn th·ªã gi√° tr·ªã c·ªßa Accumulator, gi√∫p l·∫≠p tr√¨nh vi√™n d·ªÖ d√†ng debug v√† monitor qu√° tr√¨nh t√≠nh to√°n.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
- Kh·ªüi t·∫°o Accumulator v·ªõi gi√° tr·ªã ban ƒë·∫ßu (0 ho·∫∑c r·ªóng).
- Trong c√°c h√†m map/reduce, s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c `add()` ƒë·ªÉ c·∫≠p nh·∫≠t gi√° tr·ªã.
- Gi√° tr·ªã cu·ªëi c√πng ƒë∆∞·ª£c l·∫•y v·ªÅ Driver sau khi Job ho√†n th√†nh.

### Code M·∫´u (Python - PySpark)

```python
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("AccumulatorExample").setMaster("local[2]")
sc = SparkContext(conf=conf)

# 1. Kh·ªüi t·∫°o Accumulator (Bi·∫øn t√≠ch l≈©y)
# D√πng ƒë·ªÉ ƒë·∫øm s·ªë l∆∞·ª£ng t·ª´ c√≥ ƒë·ªô d√†i > 5
long_word_count = sc.accumulator(0)

def count_long_words(word):
    global long_word_count
    if len(word) > 5:
        # Ch·ªâ c·∫≠p nh·∫≠t accumulator n·∫øu ƒëi·ªÅu ki·ªán th·ªèa m√£n
        long_word_count.add(1)
    return word

data = ["spark", "streaming", "bigdata", "processing", "batch"]
rdd = sc.parallelize(data)

# Th·ª±c thi h√†nh ƒë·ªông
rdd.foreach(count_long_words)

# In k·∫øt qu·∫£ t·∫°i Driver
print(f"S·ªë l∆∞·ª£ng t·ª´ d√†i h∆°n 5 k√Ω t·ª±: {long_word_count.value}")
# K·∫øt qu·∫£: 3 (streaming, bigdata, processing)
```

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | - **Theo d√µi (Monitoring):** R·∫•t h·ªØu √≠ch ƒë·ªÉ ƒë·∫øm c√°c s·ª± ki·ªán hi·∫øm (errors, warnings).<br>- **T√≠ch h·ª£p UI:** Hi·ªÉn th·ªã tr·ª±c ti·∫øp tr√™n Spark UI ƒë·ªÉ theo d√µi ti·∫øn ƒë·ªô. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **Kh√¥ng ƒë·∫£m b·∫£o Atomic:** Trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p hi·∫øm (race conditions), gi√° tr·ªã c√≥ th·ªÉ b·ªã sai n·∫øu c·∫≠p nh·∫≠t qu√° nhi·ªÅu.<br>- **Ch·ªâ d√πng cho ph√©p to√°n c·ªông/ƒë∆°n gi·∫£n.** |

### V√≠ d·ª• th·ª±c t·∫ø
H·ªá th·ªëng **Log Analysis (Ph√¢n t√≠ch log server)**:
- Khi x·ª≠ l√Ω stream log l·ªói, d√πng Accumulator ƒë·ªÉ ƒë·∫øm t·ªïng s·ªë `ERROR`Á∫ßÂà´ÁöÑ log xu·∫•t hi·ªán trong 1 ph√∫t. N·∫øu v∆∞·ª£t ng∆∞·ª°ng, g·ª≠i c·∫£nh b√°o ngay l·∫≠p t·ª©c.

---

## 3. Broadcast Variables (Bi·∫øn Qu·∫£ng b√°)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Khi th·ª±c thi c√°c ph√©p to√°n ph√¢n t√°n, n·∫øu m·ªôt Task c·∫ßn truy c·∫≠p m·ªôt d·ªØ li·ªáu l·ªõn t·ª´ Driver (v√≠ d·ª•: m·ªôt b·∫£ng tra c·ª©u l·ªõn - Lookup Table), Spark s·∫Ω g·ª≠i b·∫£n sao d·ªØ li·ªáu ƒë√≥ ƒë·∫øn m·ªói Executor.

N·∫øu kh√¥ng d√πng **Broadcast Variable**, d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c g·ª≠i ƒëi nhi·ªÅu l·∫ßn (m·ªói l·∫ßn m·ªôt partition). V·ªõi Broadcast Variable, d·ªØ li·ªáu ch·ªâ ƒë∆∞·ª£c g·ª≠i **m·ªôt l·∫ßn** ƒë·∫øn m·ªói Executor v√† ƒë∆∞·ª£c l∆∞u trong b·ªô nh·ªõ ƒë·ªám (Memory Cache), c√°c Task sau ƒë√≥ s·∫Ω s·ª≠ d·ª•ng l·∫°i b·∫£n sao n√†y.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
- Kh·ªüi t·∫°o bi·∫øn Broadcast t·ª´ Driver b·∫±ng `sc.broadcast(value)`.
- Truy c·∫≠p bi·∫øn trong c√°c h√†m th·ª±c thi tr√™n Executor th√¥ng qua thu·ªôc t√≠nh `.value`.

### Code M·∫´u (Python - PySpark)

```python
from pyspark import SparkContext

sc = SparkContext("local[2]", "BroadcastExample")

# Gi·∫£ s·ª≠ c√≥ m·ªôt b·∫£ng tra c·ª©u (Lookup Table) nh·ªè nh∆∞ng c·∫ßn d√πng nhi·ªÅu
lookup_map = {
    "A": "Apple",
    "B": "Banana",
    "C": "Cherry"
}

# 1. T·∫°o Broadcast Variable
# D·ªØ li·ªáu n√†y s·∫Ω ƒë∆∞·ª£c g·ª≠i ƒë·∫øn Executor m·ªôt l·∫ßn duy nh·∫•t
broadcast_map = sc.broadcast(lookup_map)

# D·ªØ li·ªáu stream (RDD)
data = [("A", 1), ("B", 2), ("C", 3), ("A", 4)]
rdd = sc.parallelize(data)

# 2. S·ª≠ d·ª•ng Broadcast Variable trong transformation
# H√†m n√†y s·∫Ω truy c·∫≠p broadcast_map.value thay v√¨ g·ª≠i l·∫°i dict lookup_map
def expand_code(code_amount):
    code, amount = code_amount
    # Truy c·∫≠p gi√° tr·ªã ƒë√£ ƒë∆∞·ª£c broadcast
    full_name = broadcast_map.value.get(code, "Unknown")
    return (full_name, amount)

result = rdd.map(expand_code).collect()
print(result)
# K·∫øt qu·∫£: [('Apple', 1), ('Banana', 2), ('Cherry', 3), ('Apple', 4)]
```

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | - **Ti·∫øt ki·ªám bƒÉng th√¥ng m·∫°ng:** Gi·∫£m ƒë√°ng k·ªÉ l∆∞·ª£ng d·ªØ li·ªáu truy·ªÅn t·∫£i gi·ªØa Driver v√† Executor.<br>- **T·ªëi ∆∞u h√≥a Memory:** Tr√°nh t·∫°o qu√° nhi·ªÅu b·∫£n sao d·ªØ li·ªáu tr√πng l·∫∑p. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **Ph·∫£i c·∫©n th·∫≠n v·ªõi d·ªØ li·ªáu thay ƒë·ªïi:** N·∫øu d·ªØ li·ªáu trong Broadcast Variable thay ƒë·ªïi, ph·∫£i h·ªßy (unpersist) v√† broadcast l·∫°i. |

### V√≠ d·ª• th·ª±c t·∫ø
**Recommendation System (H·ªá th·ªëng ƒë·ªÅ xu·∫•t)**:
- Khi x·ª≠ l√Ω stream h√†nh vi ng∆∞·ªùi d√πng, c·∫ßn ƒë·ªëi chi·∫øu v·ªõi danh s√°ch c√°c "S·∫£n ph·∫©m c·∫•m" (Blacklist) ho·∫∑c "T·ª´ kh√≥a nh·∫°y c·∫£m". Danh s√°ch n√†y ƒë∆∞·ª£c Broadcast ƒë·∫øn c√°c Executor ƒë·ªÉ l·ªçc d·ªØ li·ªáu stream si√™u t·ªëc.

---

## 4. Checkpoints (ƒêi·ªÉm ki·ªÉm tra)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Trong Spark Streaming, Job ch·∫°y li√™n t·ª•c 24/7. N·∫øu h·ªá th·ªëng g·∫∑p l·ªói (crash), vi·ªác kh·ªüi ƒë·ªông l·∫°i t·ª´ ƒë·∫ßu l√† kh√¥ng kh·∫£ thi (d·ªØ li·ªáu c√≥ th·ªÉ b·ªã m·∫•t ho·∫∑c b·ªã tr√πng l·∫∑p).

**Checkpointing** l√† c∆° ch·∫ø l∆∞u tr·∫°ng th√°i c·ªßa ·ª©ng d·ª•ng xu·ªëng Storage b·ªÅn v·ªØng (HDFS, S3...). Spark h·ªó tr·ª£ 2 lo·∫°i Checkpoint:

1.  **Metadata Checkpointing:** L∆∞u th√¥ng tin c·∫•u h√¨nh, c√°c t√°c v·ª• (DStream operations) ƒë√£ ƒë·ªãnh nghƒ©a. D√πng ƒë·ªÉ ph·ª•c h·ªìi Driver n·∫øu n√≥ b·ªã l·ªói.
2.  **Data Checkpointing:** L∆∞u d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa RDD/DStream xu·ªëng disk. D√πng ƒë·ªÉ ph·ª•c h·ªìi d·ªØ li·ªáu khi l·ªói x·∫£y ra.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
- ƒê·ªãnh nghƒ©a m·ªôt th∆∞ m·ª•c l∆∞u tr·ªØ an to√†n (HDFS, S3).
- G·ªçi `ssc.checkpoint(path)` tr∆∞·ªõc khi ƒë·ªãnh nghƒ©a c√°c d√≤ng d·ªØ li·ªáu.

### Code M·∫´u (Python - PySpark)

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c checkpoint (Ph·∫£i l√† ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi, th∆∞·ªùng l√† HDFS/S3)
checkpoint_dir = "hdfs://localhost:9000/streaming_checkpoint"

def create_context():
    sc = SparkContext("local[2]", "CheckpointExample")
    ssc = StreamingContext(sc, 1)
    
    # Thi·∫øt l·∫≠p Checkpoint cho Context
    ssc.checkpoint(checkpoint_dir)
    
    # T·∫°o DStream v√† th·ª±c hi·ªán updateStateByKey (B·∫Øt bu·ªôc ph·∫£i c√≥ checkpoint)
    lines = ssc.socketTextStream("localhost", 9999)
    words = lines.flatMap(lambda line: line.split(" "))
    pairs = words.map(lambda word: (word, 1))
    
    # H√†m update state
    def update_func(new_values, last_sum):
        return sum(new_values) + (last_sum or 0)
    
    # ƒê√¢y l√† ph√©p t√≠nh y√™u c·∫ßu checkpoint (Stateful transformation)
    running_counts = pairs.updateStateByKey(update_func)
    running_counts.pprint()
    
    return ssc

# Ki·ªÉm tra xem c√≥ checkpoint c≈© hay kh√¥ng ƒë·ªÉ ph·ª•c h·ªìi
if __name__ == "__main__":
    # N·∫øu c√≥ checkpoint c≈©, Spark s·∫Ω t·ª± ƒë·ªông ph·ª•c h·ªìi context t·ª´ ƒë√≥
    # N·∫øu kh√¥ng, n√≥ s·∫Ω g·ªçi h√†m create_context()
    ssc = StreamingContext.getOrCreate(checkpoint_dir, create_context)
    
    ssc.start()
    ssc.awaitTermination()
```

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | - **Fault Tolerance (Ch·ªãu l·ªói):** ƒê·∫£m b·∫£o h·ªá th·ªëng c√≥ th·ªÉ t·ª± ƒë·ªông ph·ª•c h·ªìi sau khi crash m√† kh√¥ng m·∫•t d·ªØ li·ªáu.<br>- **Live Recovery:** Ph·ª•c h·ªìi tr·∫°ng th√°i ch√≠nh x√°c t·∫°i th·ªùi ƒëi·ªÉm l·ªói. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **T·ªën I/O:** Vi·ªác ghi d·ªØ li·ªáu ra disk th∆∞·ªùng xuy√™n l√†m tƒÉng ƒë·ªô tr·ªÖ (Latency).<br>- **T·ªën dung l∆∞·ª£ng l∆∞u tr·ªØ:** C·∫ßn d·ªçn d·∫πp c√°c file checkpoint c≈© ƒë·ªãnh k·ª≥. |

### V√≠ d·ª• th·ª±c t·∫ø
**Payment Gateway (C·ªïng thanh to√°n)**:
- H·ªá th·ªëng ƒë·∫øm s·ªë l∆∞·ª£ng giao d·ªãch theo th·ªùi gian th·ª±c (Running Count).
- N·∫øu server b·ªã l·ªói gi·ªØa ch·ª´ng, khi kh·ªüi ƒë·ªông l·∫°i, h·ªá th·ªëng ph·∫£i bi·∫øt ch√≠nh x√°c t·ªïng s·ªë giao d·ªãch ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥ ƒë·ªÉ ti·∫øp t·ª•c ƒë·∫øm ƒë√∫ng s·ªë li·ªáu, thay v√¨ reset v·ªÅ 0. Checkpointing l√† b·∫Øt bu·ªôc trong tr∆∞·ªùng h·ª£p n√†y.

---

## 5. Structured Streaming

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê√¢y l√† m·ªôt m√¥ h√¨nh x·ª≠ l√Ω stream cao c·∫•p h∆°n so v·ªõi Spark Streaming (DStream). Structured Streaming ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n Spark SQL Engine.

**Core Concept:** Xem d·ªØ li·ªáu stream nh∆∞ m·ªôt b·∫£ng (Table) li√™n t·ª•c ƒë∆∞·ª£c c·∫≠p nh·∫≠t. Ng∆∞·ªùi d√πng vi·∫øt c√°c truy v·∫•n SQL/DataFrame nh∆∞ th·ªÉ h·ªç ƒëang x·ª≠ l√Ω d·ªØ li·ªáu tƒ©nh, v√† Spark s·∫Ω t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi n√≥ th√†nh job ch·∫°y real-time.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
- S·ª≠ d·ª•ng API `spark.readStream` ƒë·ªÉ t·∫°o Input Source.
- S·ª≠ d·ª•ng c√°c ph√©p bi·∫øn ƒë·ªïi DataFrame/SQL th√¥ng th∆∞·ªùng.
- S·ª≠ d·ª•ng `writeStream` ƒë·ªÉ xu·∫•t k·∫øt qu·∫£.

### Code M·∫´u (Python - PySpark)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split

spark = SparkSession \
    .builder \
    .appName("StructuredNetworkWordCount") \
    .getOrCreate()

# 1. T·∫°o Stream t·ª´ Socket (Xem nh∆∞ m·ªôt Table)
lines = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# 2. X·ª≠ l√Ω d·ªØ li·ªáu (DataFrame Operations)
# T√°ch d√≤ng th√†nh c√°c t·ª´
words = lines.select(
   explode(
       split(lines.value, " ")
   ).alias("word")
)

# ƒê·∫øm c√°c t·ª´
wordCounts = words.groupBy("word").count()

# 3. Xu·∫•t k·∫øt qu·∫£ (Console Sink)
query = wordCounts \
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()
```

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | - **ƒê∆°n gi·∫£n h√≥a code:** Kh√¥ng c·∫ßn x·ª≠ l√Ω RDD ph·ª©c t·∫°p, d√πng DataFrame/SQL.<br>- **Exactly-once Semantics:** ƒê·∫£m b·∫£o d·ªØ li·ªáu kh√¥ng b·ªã m·∫•t v√† kh√¥ng b·ªã x·ª≠ l√Ω tr√πng l·∫∑p.<br>- **T·ªëi ∆∞u h√≥a t·ª± ƒë·ªông (Tungsten):** T·∫≠n d·ª•ng Catalyst Optimizer ƒë·ªÉ t·ªëi ∆∞u query. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **M·ªõi h∆°n:** M·ªôt s·ªë connector/source c≈© ch∆∞a h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß.<br>- **C·∫ßn hi·ªÉu bi·∫øt v·ªÅ Spark SQL.** |

### V√≠ d·ª• th·ª±c t·∫ø
**Real-time Dashboard (B·∫£ng ƒëi·ªÅu khi·ªÉn th·ªùi gian th·ª±c)**:
- K·∫øt n·ªëi v·ªõi Kafka ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu log web.
- D√πng SQL ƒë·ªÉ l·ªçc l·ªói, group by API endpoint.
- Ghi k·∫øt qu·∫£ v√†o m·ªôt Sink (v√≠ d·ª•: Cassandra ho·∫∑c Update m·ªôt b·∫£ng trong Data Warehouse) ƒë·ªÉ Dashboard hi·ªÉn th·ªã ngay l·∫≠p t·ª©c.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "8_spark_streaming.pdf" m√† b·∫°n ƒë√£ cung c·∫•p, ƒë∆∞·ª£c tr√¨nh b√†y l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch v√† T·ªïng quan v·ªÅ Spark Streaming (T·ª´ Slide 8)

T√†i li·ªáu n√†y tr√¨nh b√†y s·ª± chuy·ªÉn ƒë·ªïi t·ª´ m√¥ h√¨nh x·ª≠ l√Ω streaming c≈© (DStreams) sang m√¥ h√¨nh Structured Streaming hi·ªán ƒë·∫°i h∆°n trong Apache Spark, c√πng v·ªõi ƒë√≥ l√† c√°ch ti·∫øp c·∫≠n Batch ETL s·ª≠ d·ª•ng DataFrame.

## 1. Nh·ªØng ƒëi·ªÉm y·∫øu c·ªßa DStreams (Pain points with DStreams)

**DStreams (Discretized Streams)** l√† m√¥ h√¨nh streaming ban ƒë·∫ßu c·ªßa Spark. M·∫∑c d√π hi·ªáu qu·∫£, n√≥ mang l·∫°i nhi·ªÅu th√°ch th·ª©c khi x√¢y d·ª±ng c√°c h·ªá th·ªëng ph·ª©c t·∫°p.

### Gi·∫£i th√≠ch kh√°i ni·ªám
*   **DStreams:** Xem d·ªØ li·ªáu stream nh∆∞ m·ªôt chu·ªói c√°c RDD (Resilient Distributed Datasets) ƒë∆∞·ª£c x·ª≠ l√Ω theo t·ª´ng kho·∫£ng th·ªùi gian c·ªë ƒë·ªãnh (batch interval).
*   **Event-time vs Processing-time:**
    *   *Event-time:* Th·ªùi gian s·ª± ki·ªán th·ª±c t·∫ø x·∫£y ra (v√≠ d·ª•: l√∫c 10:00 m·ªôt ng∆∞·ªùi d√πng click chu·ªôt).
    *   *Processing-time:* Th·ªùi gian h·ªá th·ªëng x·ª≠ l√Ω d·ªØ li·ªáu ƒë√≥ (v√≠ d·ª•: l√∫c 10:05 d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c x·ª≠ l√Ω).
*   **Late Data:** D·ªØ li·ªáu ƒë·∫øn mu·ªôn so v·ªõi th·ªùi gian x·ª≠ l√Ω d·ª± ki·∫øn.

### C√°c v·∫•n ƒë·ªÅ c·ªët l√µi
1.  **X·ª≠ l√Ω Event-time v√† D·ªØ li·ªáu Mu·ªôn:**
    *   DStream API ph∆°i b√†y `batch time` (th·ªùi gian batch), khi·∫øn vi·ªác x·ª≠ l√Ω theo `event-time` tr·ªü n√™n kh√≥ khƒÉn. Ng∆∞·ªùi d√πng ph·∫£i t·ª± qu·∫£n l√Ω watermarking (th·ªùi gian ch·ªù d·ªØ li·ªáu mu·ªôn) m·ªôt c√°ch th·ªß c√¥ng.
2.  **T√≠ch h·ª£p v·ªõi Batch v√† Interactive:**
    *   RDD/DStream c√≥ API t∆∞∆°ng t·ª± nhau, nh∆∞ng v·∫´n c·∫ßn m·ªôt l·ªõp "d·ªãch chuy·ªÉn" (translation) ƒë·ªÉ k·∫øt h·ª£p ch√∫ng v·ªõi nhau, d·∫´n ƒë·∫øn codebase kh√¥ng ƒë·ªìng nh·∫•t.
3.  **ƒê·∫£m b·∫£o End-to-end (E2E):**
    *   Vi·ªác suy lu·∫≠n v·ªÅ c√°c ƒë·∫£m b·∫£o d·ªØ li·ªáu (exactly-once, at-least-once) r·∫•t ph·ª©c t·∫°p. Ng∆∞·ªùi d√πng ph·∫£i t·ª± x√¢y d·ª±ng c√°c "sink" (ƒëi·ªÉm xu·∫•t d·ªØ li·ªáu) m·ªôt c√°ch c·∫©n th·∫≠n ƒë·ªÉ x·ª≠ l√Ω l·ªói ƒë√∫ng c√°ch.
4.  **T√≠nh nh·∫•t qu√°n d·ªØ li·ªáu (Data Consistency):**
    *   D·ªØ li·ªáu trong storage li√™n t·ª•c ƒë∆∞·ª£c c·∫≠p nh·∫≠t (overwrite/append), vi·ªác ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n trong qu√° tr√¨nh n√†y l√† m·ªôt b√†i to√°n kh√≥.

### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** DStream ph√π h·ª£p cho c√°c b√†i to√°n streaming ƒë∆°n gi·∫£n, y√™u c·∫ßu ƒë·ªô tr·ªÖ c·ª±c th·∫•p (low latency) v√† kh√¥ng c·∫ßn x·ª≠ l√Ω qu√° ph·ª©c t·∫°p v·ªÅ event-time ho·∫∑c d·ªØ li·ªáu mu·ªôn.
*   **C√°ch s·ª≠ d·ª•ng:** T·∫°o `StreamingContext`, ƒë·ªãnh nghƒ©a c√°c d√≤ng input (Receiver), v√† apply c√°c h√†m transform (map, reduceByKey).
*   **∆Øu ƒëi·ªÉm:** T·ªëi ∆∞u h√≥a t·ªët cho latency, API ƒë∆°n gi·∫£n cho c√°c b√†i to√°n c∆° b·∫£n.
*   **Nh∆∞·ª£c ƒëi·ªÉm:** Kh√≥ ƒë·∫£m b·∫£o exactly-once guarantee, x·ª≠ l√Ω event-time r∆∞·ªùm r√†, t√°ch bi·ªát v·ªõi batch processing.

---

## 2. M√¥ h√¨nh M·ªõi (Structured Streaming)

Spark gi·ªõi thi·ªáu **Structured Streaming** d·ª±a tr√™nÂºïÊìé (engine) c·ªßa DataFrame v√† SQL, gi·∫£i quy·∫øt c√°c v·∫•n ƒë·ªÅ c·ªßa DStreams b·∫±ng c√°ch coi stream nh∆∞ m·ªôt b·∫£ng (Table) li√™n t·ª•c ƒë∆∞·ª£c c·∫≠p nh·∫≠t.

### Gi·∫£i th√≠ch kh√°i ni·ªám
*   **Input as Append-only Table:** D·ªØ li·ªáu ƒë·∫øn ƒë∆∞·ª£c xem nh∆∞ vi·ªác li√™n t·ª•c append (th√™m v√†o) m·ªôt b·∫£ng.
*   **Trigger:** Kho·∫£ng th·ªùi gian ƒë·ªãnh k·ª≥ ƒë·ªÉ h·ªá th·ªëng ki·ªÉm tra d·ªØ li·ªáu m·ªõi v√† c·∫≠p nh·∫≠t k·∫øt qu·∫£.
*   **Continuous Table:** K·∫øt qu·∫£ cu·ªëi c√πng l√† m·ªôt b·∫£ng li√™n t·ª•c ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau m·ªói trigger.

### C√°c Output Modes (Ch·∫ø ƒë·ªô xu·∫•t d·ªØ li·ªáu)
ƒê√¢y l√† ph·∫ßn quan tr·ªçng quy·∫øt ƒë·ªãnh d·ªØ li·ªáu ƒë∆∞·ª£c ghi ra nh∆∞ th·∫ø n√†o sau m·ªói batch.

| Ch·∫ø ƒë·ªô (Mode) | M√¥ t·∫£ | Khi n√†o s·ª≠ d·ª•ng? |
| :--- | :--- | :--- |
| **Complete Mode** | Ghi to√†n b·ªô b·∫£ng k·∫øt qu·∫£ (Full Result Table) ra sink sau m·ªói trigger. | D√πng cho c√°c truy v·∫•n aggregation to√†n c·ª•c (v√≠ d·ª•: t·ªïng s·ªë ng∆∞·ªùi d√πng online hi·ªán t·∫°i). |
| **Delta Mode** | Ch·ªâ ghi c√°c h√†ng ƒë√£ thay ƒë·ªïi (changed rows) so v·ªõi batch tr∆∞·ªõc ƒë√≥. | Hi·∫øm d√πng, th∆∞·ªùng cho c√°c b√†i to√°n c·∫ßn tracking s·ª± thay ƒë·ªïi tr·∫°ng th√°i. |
| **Append Mode** | Ch·ªâ ghi c√°c h√†ng m·ªõi ƒë∆∞·ª£c th√™m v√†o k·∫øt qu·∫£ sau trigger. | Ph·ªï bi·∫øn nh·∫•t cho c√°c truy v·∫•n filter, map, ho·∫∑c windowing (v√≠ d·ª•: log m·ªõi). |

> **L∆∞u √Ω:** Kh√¥ng ph·∫£i t·∫•t c·∫£ c√°c output mode ƒë·ªÅu kh·∫£ thi v·ªõi m·ªçi lo·∫°i truy v·∫•n.

### Code Sample: M√¥ h√¨nh Structured Streaming (Pseudo-code)
*D·ª±a tr√™n logic trong slide, ƒë√¢y l√† c√°ch vi·∫øt l·∫°i b·∫±ng PySpark hi·ªán ƒë·∫°i:*

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("StructuredStreamingExample").getOrCreate()

# 1. Input: Data from source as an append-only table
# ƒê·ªçc d·ªØ li·ªáu stream t·ª´ JSON
input_df = spark.readStream \
    .format("json") \
    .schema(user_schema) \
    .load("source-path")

# 2. Query: Operations (Map/Filter/Window)
# L·ªçc c√°c thi·∫øt b·ªã c√≥ t√≠n hi·ªáu > 15
result_df = input_df \
    .select("device", "signal") \
    .filter("signal > 15")

# 3. Output & Trigger
# Ghi ra sink (v√≠ d·ª•: Parquet) v·ªõi ch·∫ø ƒë·ªô Append
query = result_df.writeStream \
    .format("parquet") \
    .outputMode("append") \
    .option("path", "dest-path") \
    .option("checkpointLocation", "checkpoint-dir") \
    .trigger(processingTime='1 minute') \
    .start()

query.awaitTermination()
```

---

## 3. Batch ETL v·ªõi DataFrames

Tr∆∞·ªõc khi ƒëi s√¢u v√†o streaming, slide nh·∫Øc l·∫°i quy tr√¨nh Batch ETL truy·ªÅn th·ªëng s·ª≠ d·ª•ng DataFrame API, v·ªën l√† n·ªÅn t·∫£ng c·ªßa Structured Streaming.

### Gi·∫£i th√≠ch kh√°i ni·ªám
*   **Batch Processing:** X·ª≠ l√Ω d·ªØ li·ªáu gi·ªõi h·∫°n trong m·ªôt kh·ªëi l∆∞·ª£ng (batch) c·ªë ƒë·ªãnh, kh√¥ng li√™n t·ª•c.
*   **ETL (Extract, Transform, Load):** Quy tr√¨nh tr√≠ch xu·∫•t d·ªØ li·ªáu, bi·∫øn ƒë·ªïi (l·ªçc, chuy·ªÉn ƒë·ªïi), v√† t·∫£i d·ªØ li·ªáu v√†o kho l∆∞u tr·ªØ.

### Ph√¢n t√≠ch Code trong Slide
Slide cung c·∫•p m·ªôt v√≠ d·ª• v·ªÅ pipeline ETL x·ª≠ l√Ω d·ªØ li·ªáu JSON.

#### Code m·∫´u (Python/PySpark)
ƒêo·∫°n code trong slide l√† m√£ gi·∫£, d∆∞·ªõi ƒë√¢y l√† phi√™n b·∫£n ho√†n ch·ªânh v√† chu·∫©n c√∫ ph√°p:

```python
from pyspark.sql import SparkSession

# Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder.appName("BatchETL").getOrCreate()

# 1. Read from Json file (Extract)
input_df = spark.read \
    .format("json") \
    .load("source-path")

# 2. Select & Filter (Transform)
# Ch·ªçn c·ªôt 'device' v√† 'signal', l·ªçc nh·ªØng t√≠n hi·ªáu > 15
result_df = input_df.select("device", "signal") \
                   .where("signal > 15")

# 3. Write to Parquet file (Load)
result_df.write \
    .format("parquet") \
    .save("dest-path")
```

### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Batch ETL
*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Khi d·ªØ li·ªáu c√≥ kh·ªëi l∆∞·ª£ng l·ªõn (Big Data).
    *   Khi y√™u c·∫ßu ƒë·ªô ch√≠nh x√°c tuy·ªát ƒë·ªëi (kh√¥ng c√≥ d·ªØ li·ªáu b·ªã m·∫•t).
    *   Khi ƒë·ªô tr·ªÖ v√†i ph√∫t ho·∫∑c v√†i gi·ªù l√† ch·∫•p nh·∫≠n ƒë∆∞·ª£c (kh√¥ng c·∫ßn real-time).
*   **C√°ch s·ª≠ d·ª•ng:** S·ª≠ d·ª•ng `spark.read` ƒë·ªÉ load d·ªØ li·ªáu, c√°c thao t√°c DataFrame (`select`, `filter`, `groupBy`, `join`) ƒë·ªÉ x·ª≠ l√Ω, v√† `df.write` ƒë·ªÉ l∆∞u k·∫øt qu·∫£.
*   **∆Øu ƒëi·ªÉm:** X·ª≠ l√Ω song song m·∫°nh m·∫Ω, dung n·∫°p ƒë∆∞·ª£c l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì, API ƒë∆°n gi·∫£n v√† linh ho·∫°t.
*   **Nh∆∞·ª£c ƒëi·ªÉm:** ƒê·ªô tr·ªÖ cao (latency), kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c d·ªØ li·ªáu ƒë·ªông li√™n t·ª•c.

---

## 4. V√≠ d·ª• Th·ª±c t·∫ø trong C√¥ng nghi·ªáp

### B√†i to√°n 1: Streaming Fraud Detection (Ph√°t hi·ªán gian l·∫≠n)
*   **V·∫•n ƒë·ªÅ:** Ng√¢n h√†ng c·∫ßn ph√°t hi·ªán giao d·ªãch gian l·∫≠n ngay l·∫≠p t·ª©c khi n√≥ x·∫£y ra.
*   **√Åp d·ª•ng:**
    *   S·ª≠ d·ª•ng **Structured Streaming** (M√¥ h√¨nh m·ªõi).
    *   **Input:** D√≤ng giao d·ªãch t·ª´ Kafka.
    *   **Query:** Ki·ªÉm tra giao d·ªãch c√≥ v∆∞·ª£t ng∆∞·ª°ng t√≠n d·ª•ng ho·∫∑c ƒë·∫øn t·ª´ v·ªã tr√≠ b·∫•t th∆∞·ªùng kh√¥ng (Logic ph·ª©c t·∫°p).
    *   **Output Mode:** `Append` (ch·ªâ ghi c√°c giao d·ªãch ƒë√°ng ng·ªù m·ªõi).
    *   **Late Data:** X·ª≠ l√Ω giao d·ªãch ƒë·∫øn mu·ªôn do m·∫•t m·∫°ng b·∫±ng Watermarking.

### B√†i to√°n 2: Daily User Activity Report (B√°o c√°o ho·∫°t ƒë·ªông ng∆∞·ªùi d√πng h√†ng ng√†y)
*   **V·∫•n ƒë·ªÅ:** H·ªá th·ªëng c·∫ßn t·ªïng h·ª£p s·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng ho·∫°t ƒë·ªông (DAU) m·ªói ng√†y.
*   **√Åp d·ª•ng:**
    *   S·ª≠ d·ª•ng **Batch ETL** (ho·∫∑c Streaming Window).
    *   **Input:** Log ng∆∞·ªùi d√πng ƒë∆∞·ª£c l∆∞u v√†o HDFS/S3 m·ªói ng√†y.
    *   **Query:** `groupBy("date", "user_id").count()`.
    *   **Output Mode:** `Complete` (n·∫øu d√πng Streaming) ho·∫∑c ghi ƒë√® b·∫£ng b√°o c√°o (n·∫øu d√πng Batch).

---

## T√≥m t·∫Øt Ph√¢n t√≠ch

| ƒê·∫∑c ƒëi·ªÉm | DStreams (C≈©) | Structured Streaming (M·ªõi) | Batch ETL |
| :--- | :--- | :--- | :--- |
| **ƒê∆°n v·ªã x·ª≠ l√Ω** | RDD | DataFrame / Dataset | DataFrame / Dataset |
| **Th·ªùi gian x·ª≠ l√Ω** | Processing-time | Event-time (linh ho·∫°t) | Kh√¥ng √°p d·ª•ng (To√†n b·ªô d·ªØ li·ªáu) |
| **ƒê·ªô tr·ªÖ** | R·∫•t th·∫•p (Low) | Th·∫•p ƒë·∫øn Trung b√¨nh | Cao (High) |
| **ƒê·ªô ph·ª©c t·∫°p** | Cao (t·ª± x·ª≠ l√Ω l·ªói, state) | Th·∫•p (API kh√©p k√≠n) | Trung b√¨nh |
| **ƒê·∫£m b·∫£o d·ªØ li·ªáu** | Kh√≥ khƒÉn | Exactly-once (m·∫∑c ƒë·ªãnh) | Exactly-once |

T√†i li·ªáu n√†y nh·∫•n m·∫°nh s·ª± ti·∫øn h√≥a c·ªßa Spark t·ª´ m·ªôt c√¥ng c·ª• x·ª≠ l√Ω batch v√† stream ri√™ng bi·ªát sang m·ªôt m√¥ h√¨nh th·ªëng nh·∫•t (Unified) m·∫°nh m·∫Ω v√† d·ªÖ s·ª≠ d·ª•ng h∆°n.

---

Ch√†o b·∫°n, t√¥i l√† chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "8_spark_streaming.pdf" c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp v√† chi ti·∫øt theo y√™u c·∫ßu.

---

# Spark Streaming: ETL v√† Ph√¢n t√≠ch N√¢ng cao v·ªõi DataFrames

T√†i li·ªáu n√†y tr√¨nh b√†y c√°ch ti·∫øp c·∫≠n hi·ªán ƒë·∫°i (Structured Streaming) trong Apache Spark ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu streaming, s·ª≠ d·ª•ng DataFrame API - m·ªôt trong nh·ªØng API m·∫°nh m·∫Ω v√† ph·ªï bi·∫øn nh·∫•t c·ªßa Spark.

## 1. Streaming ETL v·ªõi DataFrames

Ph·∫ßn n√†y t·∫≠p trung v√†o vi·ªác chuy·ªÉn ƒë·ªïi c√°c t√°c v·ª• ETL (Extract, Transform, Load) truy·ªÅn th·ªëng th√†nh c√°c t√°c v·ª• streaming th·ªùi gian th·ª±c.

### Kh√°i ni·ªám ch√≠nh

-   **Streaming DataFrame**: Trong Spark Structured Streaming, m·ªôt DataFrame streaming v·ªÅ m·∫∑t ng·ªØ nghƒ©a gi·ªëng nh∆∞ m·ªôt DataFrame batch, nh∆∞ng n√≥ li√™n t·ª•c nh·∫≠n d·ªØ li·ªáu m·ªõi t·ª´ ngu·ªìn (source).
-   **Triggers v√† Checkpointing**: C∆° ch·∫ø ƒë·ªÉ Spark theo d√µi tr·∫°ng th√°i x·ª≠ l√Ω v√† ƒë·∫£m b·∫£o x·ª≠ l√Ω d·ªØ li·ªáu m·ªôt c√°ch ch√≠nh x√°c (exactly-once semantics).
-   **Micro-batch Processing**: D·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω theo c√°c batch nh·ªè li√™n t·ª•c.

### Ph√¢n t√≠ch Code v√† C·∫£i ti·∫øn

N·ªôi dung slide cho th·∫•y s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa x·ª≠ l√Ω batch v√† streaming. S·ª± thay ƒë·ªïi n·∫±m ·ªü c√°c ph∆∞∆°ng th·ª©c:
-   `load()` -> `stream()`
-   `save()` -> `startStream()`

D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n code ho√†n ch·ªânh, ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn v√† chu·∫©n h√≥a theo best practices c·ªßa PySpark.

#### Code M·∫´u: Basic Streaming ETL

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# 1. Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder \
    .appName("StreamingETL") \
    .getOrCreate()

# 2. ƒê·ªçc d·ªØ li·ªáu t·ª´ ngu·ªìn Streaming (JSON)
# Thay v√¨ read.load(), ta d√πng read.stream()
# D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c ƒë·ªçc li√™n t·ª•c t·ª´ th∆∞ m·ª•c source-path
input_stream = spark.readStream \
    .format("json") \
    .schema("device STRING, signal DOUBLE, event_time TIMESTAMP") # Lu√¥n ƒë·ªãnh nghƒ©a schema ƒë·ªÉ tr√°nh l·ªói
    .option("maxFilesPerTrigger", 1) # X·ª≠ l√Ω 1 file m·ªói trigger ƒë·ªÉ d·ªÖ debug
    .load("source-path")

# 3. Bi·∫øn ƒë·ªïi d·ªØ li·ªáu (Transform)
# Logic t∆∞∆°ng t·ª± nh∆∞ batch DataFrame
result_stream = input_stream \
    .select("device", "signal") \
    .where(col("signal") > 15)

# 4. Ghi d·ªØ li·ªáu ra (Load)
# Thay v√¨ save(), ta d√πng startStream()
query = result_stream.writeStream \
    .format("parquet") \
    .outputMode("append") # Ghi th√™m d·ªØ li·ªáu m·ªõi
    .option("checkpointLocation", "/tmp/checkpoints/etl_demo") # B·∫Øt bu·ªôc ƒë·ªÉ fault-tolerance
    .start("dest-path")

# 5. Ch·ªù query ch·∫°y (ho·∫∑c d√πng query.awaitTermination())
query.awaitTermination()
```

### Gi·∫£i th√≠ch chi ti·∫øt c√°c thu·∫≠t ng·ªØ

| Thu·∫≠t ng·ªØ | Gi·∫£i th√≠ch |
| :--- | :--- |
| **`readStream`** | Kh·ªüi t·∫°o m·ªôt DataFrame streaming. N√≥ ƒë·ªãnh nghƒ©a ngu·ªìn d·ªØ li·ªáu nh∆∞ng **kh√¥ng** b·∫Øt ƒë·∫ßu t√≠nh to√°n ngay l·∫≠p t·ª©c. N√≥ gi·ªëng nh∆∞ m·ªôt "k·∫ø ho·∫°ch" ch·ªù k√≠ch ho·∫°t. |
| **`writeStream`** | K√≠ch ho·∫°t lu·ªìng x·ª≠ l√Ω. N√≥ x√°c ƒë·ªãnh n∆°i d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c ghi, c√°ch ghi (output mode) v√† c√°c th√¥ng s·ªë ch·∫°y (trigger, checkpoint). |
| **`checkpointLocation`** | ƒê∆∞·ªùng d·∫´n l∆∞u tr·ªØ tr·∫°ng th√°i c·ªßa job v√† d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω. N·∫øu job b·ªã l·ªói v√† kh·ªüi ƒë·ªông l·∫°i, n√≥ s·∫Ω ƒë·ªçc t·ª´ ƒë√¢y ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω m√† kh√¥ng m·∫•t d·ªØ li·ªáu (Fault Tolerance). |
| **`outputMode`** | X√°c ƒë·ªãnh c√°ch d·ªØ li·ªáu ƒë∆∞·ª£c ghi ra:<br>- `append`: Ch·ªâ ghi d·ªØ li·ªáu m·ªõi.<br>- `complete`: Ghi to√†n b·ªô k·∫øt qu·∫£ (d√πng cho aggregations).<br>- `update`: C·∫≠p nh·∫≠t c√°c d√≤ng ƒë√£ thay ƒë·ªïi. |

### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Khi b·∫°n c·∫ßn chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ (JSON, CSV, Kafka) sang ƒë·ªãnh d·∫°ng t·ªëi ∆∞u h∆°n (Parquet) theo th·ªùi gian th·ª±c.
    *   Khi c·∫ßn l·ªçc, l√†m s·∫°ch d·ªØ li·ªáu (cleaning) tr∆∞·ªõc khi l∆∞u v√†o Data Lake ho·∫∑c Data Warehouse.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   S·ª≠ d·ª•ng `spark.readStream` ƒë·ªÉ k·∫øt n·ªëi ngu·ªìn.
    *   √Åp d·ª•ng c√°c thao t√°c DataFrame (`select`, `filter`, `withColumn`).
    *   S·ª≠ d·ª•ng `writeStream` v·ªõi `checkpointLocation` ƒë·ªÉ l∆∞u k·∫øt qu·∫£.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **∆Øu ƒëi·ªÉm:** Code ƒë∆°n gi·∫£n, gi·ªëng h·ªát batch processing; Fault-tolerance m·∫°nh m·∫Ω; T·ªëi ∆∞u h√≥a t·ª± ƒë·ªông (Tungsten Engine).
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** ƒê·ªô tr·ªÖ (latency) th∆∞·ªùng cao h∆°n so v·ªõi Spark DStreams c≈© ho·∫∑c Flink (do c∆° ch·∫ø micro-batch).

---

## 2. Continuous Aggregations (T·ªïng h·ª£p li√™n t·ª•c)

ƒê√¢y l√† c√°ch t√≠nh to√°n c√°c gi√° tr·ªã t t·ªïng h·ª£p (aggregate) tr√™n to√†n b·ªô lu·ªìng d·ªØ li·ªáu ho·∫∑c theo c√°c nh√≥m.

### Kh√°i ni·ªám ch√≠nh

-   **Global Aggregation**: T√≠nh to√°n tr√™n to√†n b·ªô d·ªØ li·ªáu streaming (v√≠ d·ª•: t·ªïng s·ªë signal trung b√¨nh c·ªßa to√†n b·ªô thi·∫øt b·ªã).
-   **Grouped Aggregation**: Ph√¢n chia d·ªØ li·ªáu theo m·ªôt key (v√≠ d·ª•: `device-type`) v√† t√≠nh to√°n aggregate cho t·ª´ng key.

### Code M·∫´u: Continuous Aggregations

```python
from pyspark.sql.functions import avg

# Gi·∫£ s·ª≠ input_stream ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ ph·∫ßn tr√™n

# 1. Global Average: T√≠nh trung b√¨nh signal tr√™n to√†n b·ªô d·ªØ li·ªáu
global_avg = input_stream.select(avg("signal"))

# 2. Grouped Average: T√≠nh trung b√¨nh theo t·ª´ng lo·∫°i thi·∫øt b·ªã
device_type_avg = input_stream \
    .groupBy("device-type") \
    .agg(avg("signal").alias("avg_signal"))

# Ghi k·∫øt qu·∫£ (v√≠ d·ª• cho grouped aggregation)
query_agg = device_type_avg.writeStream \
    .format("console") \
    .outputMode("complete") # Ph·∫£i d√πng complete mode cho aggregations
    .start()
```

### Gi·∫£i th√≠ch chi ti·∫øt

Khi th·ª±c hi·ªán `groupBy` tr√™n m·ªôt streaming DataFrame, Spark s·∫Ω duy tr√¨ m·ªôt "state" (tr·∫°ng th√°i) cho m·ªói key. Khi d·ªØ li·ªáu m·ªõi ƒë·∫øn, Spark c·∫≠p nh·∫≠t state n√†y v√† emit (ph√°t ra) k·∫øt qu·∫£ m·ªõi.

### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Gi√°m s√°t h·ªá th·ªëng th·ªùi gian th·ª±c (Real-time dashboard): Hi·ªÉn th·ªã t·ªïng s·ªë l·ªói theo lo·∫°i, t·ªïng doanh thu theo gi√¢y.
    *   C·∫£nh b√°o b·∫•t th∆∞·ªùng: N·∫øu gi√° tr·ªã trung b√¨nh c·ªßa m·ªôt group v∆∞·ª£t ng∆∞·ª°ng.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   D√πng `groupBy()` sau ƒë√≥ g·ªçi c√°c h√†m aggregate (`sum`, `count`, `avg`, `max`).
    *   **L∆∞u √Ω quan tr·ªçng:** Khi ghi k·∫øt qu·∫£ aggregations, `outputMode` ph·∫£i l√† `complete` (hi·ªÉn th·ªã to√†n b·ªô k·∫øt qu·∫£) ho·∫∑c `update` (ch·ªâ hi·ªÉn th·ªã thay ƒë·ªïi).
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **∆Øu ƒëi·ªÉm:** R·∫•t linh ho·∫°t, h·ªó tr·ª£ h·∫ßu h·∫øt c√°c h√†m aggregate gi·ªëng nh∆∞ batch.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** C√≥ th·ªÉ g√¢y tr√†n b·ªô nh·ªõ (Out of Memory) n·∫øu s·ªë l∆∞·ª£ng key qu√° l·ªõn (cardinality cao) v√¨ Spark ph·∫£i l∆∞u tr·∫°ng th√°i c·ªßa t·∫•t c·∫£ c√°c key.

---

## 3. Continuous Windowed Aggregations (T·ªïng h·ª£p theo C·ª≠a s·ªï th·ªùi gian)

ƒê√¢y l√† t√≠nh nƒÉng m·∫°nh m·∫Ω nh·∫•t c·ªßa Structured Streaming so v·ªõi c√°c th·∫ø h·ªá tr∆∞·ªõc (DStreams), cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu d·ª±a tr√™n **Event Time** (th·ªùi gian s·ª± ki·ªán x·∫£y ra) thay v√¨ Processing Time (th·ªùi gian x·ª≠ l√Ω).

### Kh√°i ni·ªám ch√≠nh

-   **Event Time**: D·∫•u th·ªùi gian ghi trong d·ªØ li·ªáu (v√≠ d·ª•: `2023-10-27 10:00:05`).
-   **Windowing**: Chia th·ªùi gian th√†nh c√°c kho·∫£ng (v√≠ d·ª•: 10 ph√∫t, 1 gi·ªù) ƒë·ªÉ t·ªïng h·ª£p d·ªØ li·ªáu.
-   **Watermarking**: C∆° ch·∫ø x·ª≠ l√Ω d·ªØ li·ªáu tr·ªÖ (late data) b·∫±ng c√°ch ƒë·∫∑t gi·ªõi h·∫°n th·ªùi gian ch·ªù (v√≠ d·ª•: 2 gi·ªù). Sau 2 gi·ªù, Spark s·∫Ω "ƒë√≥ng" c·ª≠a s·ªï ƒë√≥ v√† kh√¥ng nh·∫≠n d·ªØ li·ªáu tr·ªÖ n·ªØa.

### Code M·∫´u: Windowed Aggregations

```python
from pyspark.sql.functions import window

# Gi·∫£ s·ª≠ input_stream c√≥ c·ªôt event_time

# T√≠nh trung b√¨nh signal c·ªßa t·ª´ng lo·∫°i thi·∫øt b·ªã trong c·ª≠a s·ªï 10 ph√∫t
# D·ª±a tr√™n event-time, kh√¥ng ph·∫£i th·ªùi gian nh·∫≠n d·ªØ li·ªáu
windowed_avg = input_stream \
    .groupBy(
        col("device-type"),
        window(col("event-time-col"), "10 minutes") # ƒê·ªãnh nghƒ©a c·ª≠a s·ªï 10 ph√∫t
    ) \
    .agg(avg("signal").alias("avg_signal"))

# Ghi k·∫øt qu·∫£
query_window = windowed_avg.writeStream \
    .format("console") \
    .outputMode("update") \
    .option("checkpointLocation", "/tmp/checkpoints/window_demo") \
    .start()
```

### Gi·∫£i th√≠ch chi ti·∫øt

*   **`window($"event-time-col", "10 min")`**: H√†m n√†y t·∫°o ra m·ªôt c·ªôt `window` m·ªõi ch·ª©a c·∫∑p `(start_time, end_time)`. D·ªØ li·ªáu ƒë∆∞·ª£c gom v√†o c·ª≠a s·ªï t∆∞∆°ng ·ª©ng v·ªõi `event-time` c·ªßa n√≥.
*   **T·∫°i sao quan tr·ªçng?** Trong c√°c h·ªá th·ªëng c≈© (DStreams), d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω theo m·ªëc th·ªùi gian nh·∫≠n (Processing Time). N·∫øu m·∫°ng lag 5 ph√∫t, d·ªØ li·ªáu s·∫Ω b·ªã t√≠nh sai gi·ªù. V·ªõi Event Time Windowing, d√π d·ªØ li·ªáu ƒë·∫øn tr·ªÖ, n√≥ v·∫´n ƒë∆∞·ª£c g√°n v√†o ƒë√∫ng gi·ªù 10:00 - 10:10.

### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Ph√¢n t√≠ch xu h∆∞·ªõng theo th·ªùi gian: "Trong 1 gi·ªù qua, c√≥ bao nhi√™u ng∆∞·ªùi d√πng truy c·∫≠p?".
    *   X·ª≠ l√Ω d·ªØ li·ªáu IoT: "Nhi·ªát ƒë·ªô trung b√¨nh c·ªßa c·∫£m bi·∫øn trong 5 ph√∫t qua".
    *   Khi d·ªØ li·ªáu ƒë·∫øn kh√¥ng ƒë·ªìng ƒë·ªÅu (do m·∫°ng lag, l·ªói h·ªá th·ªëng).
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   S·ª≠ d·ª•ng h√†m `window()` b√™n trong `groupBy()`.
    *   **B·∫Øt bu·ªôc:** Ph·∫£i c√≥ c·ªôt timestamp trong d·ªØ li·ªáu ngu·ªìn.
    *   N√™n k·∫øt h·ª£p v·ªõi **Watermarking** ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu tr·ªÖ v√† tr√°nh l∆∞u tr·ªØ state vƒ©nh vi·ªÖn.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **∆Øu ƒëi·ªÉm:** X·ª≠ l√Ω ch√≠nh x√°c theo th·ªùi gian th·ª±c t·∫ø c·ªßa s·ª± ki·ªán; H·ªó tr·ª£ d·ªØ li·ªáu tr·ªÖ; Code c·ª±c k·ª≥ ng·∫Øn g·ªçn.
    *   **Nh∆∞·ª£c ƒëi·ªÉm:** T·ªën t√†i nguy√™n t√≠nh to√°n v√† b·ªô nh·ªõ h∆°n do ph·∫£i duy tr√¨ state cho c√°c c·ª≠a s·ªï ƒëang m·ªü; Ph·∫£i c·∫•u h√¨nh watermarking c·∫©n th·∫≠n ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu ho·∫∑c treo h·ªá th·ªëng.

---

## 4. V√≠ d·ª• Th·ª±c t·∫ø trong Ng√†nh C√¥ng Nghi·ªáp

D∆∞·ªõi ƒë√¢y l√† c√°c k·ªãch b·∫£n √°p d·ª•ng th·ª±c ti·ªÖn c√°c kh√°i ni·ªám tr√™n:

### V√≠ d·ª• 1: Gi√°m s√°t Log Server (Streaming ETL)
*   **Scenario**: M·ªôt h·ªá th·ªëng web t·∫°o ra h√†ng tri·ªáu log d√≤ng m·ªói ng√†y (JSON).
*   **√Åp d·ª•ng**: S·ª≠ d·ª•ng `readStream` ƒë·ªÉ ƒë·ªçc log t·ª´ Kafka ho·∫∑c S3. D√πng `select/where` ƒë·ªÉ l·ªçc c√°c log c√≥ level l√† `ERROR`. Ghi l·∫°i v√†o file Parquet ƒë·ªÉ ph√¢n t√≠ch sau n√†y.
*   **Code**: T∆∞∆°ng t·ª± ph·∫ßn 1.

### V√≠ d·ª• 2: Dashboard Doanh thu (Continuous Aggregation)
*   **Scenario**: C√¥ng ty th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ c·∫ßn hi·ªÉn th·ªã doanh thu theo gi√¢y tr√™n m√†n h√¨nh TV l·ªõn t·∫°i ph√≤ng kinh doanh.
*   **√Åp d·ª•ng**: ƒê·ªçc d·ªØ li·ªáu giao d·ªãch t·ª´ Kafka. D√πng `groupBy("product_category").agg(sum("amount"))`.
*   **Code**: T∆∞∆°ng t·ª± ph·∫ßn 2.

### V√≠ d·ª• 3: Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng (Windowed Aggregation)
*   **Scenario**: ·ª®ng d·ª•ng di ƒë·ªông mu·ªën ƒë·∫øm s·ªë l·∫ßn ng∆∞·ªùi d√πng click v√†o n√∫t "Mua" trong **cu·ªëi tu·∫ßn** (v√≠ d·ª•: t·ª´ 00:00 th·ª© 7 ƒë·∫øn 23:59 ch·ªß nh·∫≠t).
*   **√Åp d·ª•ng**: S·ª≠ d·ª•ng `window(col("timestamp"), "3 days")` ho·∫∑c `window(col("timestamp"), "1 week")`. D·ªØ li·ªáu click c√≥ th·ªÉ ƒë·∫øn tr·ªÖ do ng∆∞·ªùi d√πng offline (m·∫•t m·∫°ng), nh∆∞ng Spark v·∫´n g√°n click ƒë√≥ v√†o ƒë√∫ng c·ª≠a s·ªï cu·ªëi tu·∫ßn nh·ªù Event Time.
*   **Code**:
    ```python
    df.groupBy(window(col("timestamp"), "1 week")).count()
    ```

---

## T√≥m t·∫Øt So s√°nh

| T√≠nh nƒÉng | Batch Processing | Structured Streaming |
| :--- | :--- | :--- |
| **API** | `spark.read.load()` | `spark.readStream.load()` |
| **Ghi d·ªØ li·ªáu** | `df.write.save()` | `df.writeStream.start()` |
| **T·ªïng h·ª£p** | `df.groupBy().agg()` | `df.groupBy().agg()` (T∆∞∆°ng t·ª±) |
| **C·ª≠a s·ªï th·ªùi gian** | Ph·ª©c t·∫°p, c·∫ßn t·ª± x·ª≠ l√Ω | ƒê∆°n gi·∫£n v·ªõi `window()` |
| **ƒê·ªô tr·ªÖ** | Kh√¥ng c√≥ (X·ª≠ l√Ω theo batch) | Milliseconds ƒë·∫øn Seconds |

T√†i li·ªáu n√†y nh·∫•n m·∫°nh r·∫±ng **Structured Streaming** cho ph√©p c√°c l·∫≠p tr√¨nh vi√™n s·ª≠ d·ª•ng l·∫°i ki·∫øn th·ª©c DataFrame/Batch c·ªßa h·ªç ƒë·ªÉ x√¢y d·ª±ng c√°c h·ªá th·ªëng streaming ph·ª©c t·∫°p m·ªôt c√°ch d·ªÖ d√†ng v√† m·∫°nh m·∫Ω.

---

Ch√†o b·∫°n, v·ªõi vai tr√≤ l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n, t√¥i s·∫Ω ph√¢n t√≠ch v√† tr√¨nh b√†y chi ti·∫øt n·ªôi dung t·ª´ slide "8_spark_streaming.pdf" m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát.

---

# Apache Spark Structured Streaming: Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Chi ti·∫øt

T√†i li·ªáu n√†y gi·ªõi thi·ªáu c√°c kh√°i ni·ªám n√¢ng cao v·ªÅ Apache Spark Structured Streaming, t·∫≠p trung v√†o vi·ªác k·∫øt h·ª£p d·ªØ li·ªáu, c√°c ch·∫ø ƒë·ªô ƒë·∫ßu ra, qu·∫£n l√Ω truy v·∫•n v√† ki·∫øn tr√∫c t·ªïng quan.

## 1. K·∫øt h·ª£p D·ªØ li·ªáu Stream v·ªõi D·ªØ li·ªáu Tƒ©nh (Joining Streams with Static Data)

ƒê√¢y l√† m·ªôt t√≠nh nƒÉng m·∫°nh m·∫Ω c·ªßa Structured Streaming cho ph√©p l√†m gi√†u (enrich) d·ªØ li·ªáu stream th·ªùi gian th·ª±c b·∫±ng c√°ch k·∫øt h·ª£p n√≥ v·ªõi m·ªôt b·∫£ng d·ªØ li·ªáu tƒ©nh (static) ho·∫∑c tham chi·∫øu (reference), v√≠ d·ª• nh∆∞ th√¥ng tin thi·∫øt b·ªã IoT.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
- **Streaming Data**: D√≤ng d·ªØ li·ªáu li√™n t·ª•c, v√¥ t·∫≠n (v√≠ d·ª•: d·ªØ li·ªáu c·∫£m bi·∫øn t·ª´ Kafka).
- **Static Data**: D·ªØ li·ªáu kh√¥ng ƒë·ªïi, th∆∞·ªùng l√† b·∫£ng trong c∆° s·ªü d·ªØ li·ªáu quan h·ªá (JDBC) ho·∫∑c file (CSV, Parquet), ch·ª©a th√¥ng tin tham chi·∫øu (metadata).
- **Join Operation**: Spark s·∫Ω √°nh x·∫° m·ªói b·∫£n ghi trong stream v·ªõi c√°c b·∫£n ghi t∆∞∆°ng ·ª©ng trong b·∫£ng tƒ©nh ƒë·ªÉ t·∫°o ra b·∫£n ghi gi√†u th√¥ng tin h∆°n.

### Code M·∫´u (Python)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("StreamStaticJoin") \
    .getOrCreate()

# 1. ƒê·ªçc d·ªØ li·ªáu tƒ©nh t·ª´ JDBC (v√≠ d·ª•: b·∫£ng th√¥ng tin thi·∫øt b·ªã)
# ƒê√¢y l√† m·ªôt DataFrame batch th√¥ng th∆∞·ªùng
static_df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://db_host:5432/iot_db") \
    .option("dbtable", "iot_device_info") \
    .option("user", "admin") \
    .option("password", "secret") \
    .load()

# 2. ƒê·ªçc d·ªØ li·ªáu stream t·ª´ Kafka
# ƒê√¢y l√† m·ªôt Streaming DataFrame
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "iot-updates") \
    .load()

# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu Kafka (d·∫°ng binary) sang String v√† Struct
stream_df = kafka_df.selectExpr("CAST(value AS STRING) as json_value") \
    .selectExpr("json_value") # C·∫ßn parse JSON c·ª• th·ªÉ h∆°n t√πy c·∫•u tr√∫c

# 3. Th·ª±c hi·ªán Join
# L∆∞u √Ω: Trong Spark Structured Streaming, m·ªôt trong 2 DataFrame ph·∫£i l√† stream,
# DataFrame c√≤n l·∫°i ph·∫£i l√† static (batch).
# Spark s·∫Ω t·ª± ƒë·ªông broadcast static DataFrame n·∫øu n√≥ nh·ªè, ho·∫∑c shuffle stream data.
joined_df = kafka_df.join(static_df, kafka_df.device_id == static_df.id, "inner")

# 4. Ghi k·∫øt qu·∫£ ra sink (v√≠ d·ª•: console ƒë·ªÉ ki·ªÉm tra)
query = joined_df.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
- **Khi n√†o s·ª≠ d·ª•ng?**
    - Khi b·∫°n c·∫ßn l√†m gi√†u d·ªØ li·ªáu event th·ªùi gian th·ª±c (v√≠ d·ª•: log truy c·∫≠p) v·ªõi th√¥ng tin user (l·∫•y t·ª´ DB).
    - Khi d·ªØ li·ªáu tham chi·∫øu thay ƒë·ªïi ch·∫≠m (v√≠ d·ª•: danh s√°ch s·∫£n ph·∫©m, c·∫•u h√¨nh thi·∫øt b·ªã).
- **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    - ƒê·ªçc d·ªØ li·ªáu tƒ©nh b·∫±ng `spark.read` (batch).
    - ƒê·ªçc d·ªØ li·ªáu stream b·∫±ng `spark.readStream`.
    - S·ª≠ d·ª•ng to√°n t·ª≠ `join` th√¥ng th∆∞·ªùng nh∆∞ batch.
- **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    - **∆Øu**: ƒê∆°n gi·∫£n h√≥a pipeline, kh√¥ng c·∫ßn ETL ri√™ng cho d·ªØ li·ªáu tham chi·∫øu.
    - **Nh∆∞·ª£c**: N·∫øu b·∫£ng tƒ©nh qu√° l·ªõn, Spark ph·∫£i shuffle d·ªØ li·ªáu stream li√™n t·ª•c, g√¢y ch·∫≠m tr·ªÖ (latency). C·∫ßn l∆∞u √Ω ƒë·∫øn vi·ªác c·∫≠p nh·∫≠t d·ªØ li·ªáu tƒ©nh (data versioning).

### V√≠ d·ª• Th·ª±c t·∫ø
Trong h·ªá th·ªëng **Smart Home**: D·ªØ li·ªáu stream l√† nhi·ªát ƒë·ªô v√† ƒë·ªô ·∫©m g·ª≠i l√™n li√™n t·ª•c t·ª´ c·∫£m bi·∫øn. D·ªØ li·ªáu tƒ©nh l√† b·∫£ng `device_room_mapping` (ID thi·∫øt b·ªã -> T√™n ph√≤ng). Join gi√∫p ta c√≥ ngay b√°o c√°o "Nhi·ªát ƒë·ªô ph√≤ng kh√°ch l√† bao nhi√™u?" thay v√¨ ch·ªâ th·∫•y "ID thi·∫øt b·ªã ABC".

---

## 2. C√°c Ch·∫ø ƒë·ªô ƒê·∫ßu ra (Output Modes)

Output Modes x√°c ƒë·ªãnh d·ªØ li·ªáu n√†o ƒë∆∞·ª£c ghi ra sink (ƒë√≠ch ƒë·∫øn) m·ªói khi Spark x·ª≠ l√Ω m·ªôt batch d·ªØ li·ªáu m·ªõi.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
- **Trigger**: S·ª± ki·ªán k√≠ch ho·∫°t vi·ªác t√≠nh to√°n v√† ghi d·ªØ li·ªáu (v√≠ d·ª•: m·ªói 1 gi√¢y, m·ªói khi c√≥ 1000 event).
- **Output Mode**: C√°ch th·ª©c d·ªØ li·ªáu ƒë∆∞·ª£c xu·∫•t ra.

### C√°c Ch·∫ø ƒë·ªô ch√≠nh

| Ch·∫ø ƒë·ªô (Mode) | M√¥ t·∫£ | Ph√π h·ª£p cho |
| :--- | :--- | :--- |
| **Append** | Ch·ªâ ghi c√°c b·∫£n ghi **m·ªõi** ƒë∆∞·ª£c th√™m v√†o k·∫øt qu·∫£ t·ª´ batch cu·ªëi. | C√°c truy v·∫•n kh√¥ng c√≥ aggregation (nh∆∞ filter, map) ho·∫∑c aggregation v·ªõi gi·ªõi h·∫°n c·ª≠a s·ªï (window) ch∆∞a k·∫øt th√∫c. |
| **Complete** | Ghi **to√†n b·ªô** k·∫øt qu·∫£ c·∫≠p nh·∫≠t c·ªßa truy v·∫•n (k·ªÉ c·∫£ b·∫£n ghi c≈©). | C√°c truy v·∫•n aggregation (t·ªïng h·ª£p) c·∫ßn bi·∫øt t·ªïng s·ªë li·ªáu hi·ªán t·∫°i (v√≠ d·ª•: t·ªïng s·ªë ng∆∞·ªùi d√πng online). |
| **Update** | Ch·ªâ ghi c√°c b·∫£n ghi ƒë√£ **thay ƒë·ªïi** so v·ªõi batch tr∆∞·ªõc (delta). | Truy v·∫•n aggregation, t·ªëi ∆∞u h√≥a bƒÉng th√¥ng n·∫øu sink h·ªó tr·ª£ c·∫≠p nh·∫≠t (nh∆∞ Cassandra, JDBC). |

### Code M·∫´u

#### V√≠ d·ª• 1: Append Mode (Kh√¥ng Aggregation)
```python
# input l√† Streaming DataFrame t·ª´ Kafka/Source
# L·ªçc c√°c s·ª± ki·ªán c√≥ t√≠n hi·ªáu > 50
filtered_df = input.select("device", "signal").filter("signal > 50")

# Ghi ra Parquet, ch·ªâ th√™m m·ªõi b·∫£n ghi
query = filtered_df.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "/path/to/dest-folder") \
    .option("checkpointLocation", "/path/to/checkpoint") \
    .start()
```

#### V√≠ d·ª• 2: Complete Mode (C√≥ Aggregation)
```python
# input l√† Streaming DataFrame
# ƒê·∫øm s·ªë l∆∞·ª£ng s·ª± ki·ªán theo t·ª´ng lo·∫°i thi·∫øt b·ªã
agg_df = input.groupBy("device_type").count()

# Ghi ra Parquet, ghi l·∫°i to√†n b·ªô b·∫£ng t·ªïng h·ª£p m·ªói l·∫ßn trigger
query = agg_df.writeStream \
    .outputMode("complete") \
    .format("parquet") \
    .option("path", "/path/to/dest-folder") \
    .option("checkpointLocation", "/path/to/checkpoint") \
    .start()
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
- **Khi n√†o s·ª≠ d·ª•ng?**
    - **Append**: Khi b·∫°n ch·ªâ quan t√¢m ƒë·∫øn c√°c s·ª± ki·ªán m·ªõi (v√≠ d·ª•: log l·ªói m·ªõi ph√°t sinh).
    - **Complete**: Khi b·∫°n c·∫ßn dashboard t·ªïng h·ª£p realtime (v√≠ d·ª•: doanh thu theo gi·ªù).
- **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    - **Append**: T·ªëi ∆∞u b·ªô nh·ªõ v√† IO, nh∆∞ng kh√¥ng ph√π h·ª£p v·ªõi aggregation t·ªïng th·ªÉ.
    - **Complete**: D·ªÖ nh√¨n th·∫•y tr·∫°ng th√°i cu·ªëi c√πng, nh∆∞ng t·ªën IO (ghi ƒë√® d·ªØ li·ªáu c≈©) v√† kh√¥ng scale t·ªët v·ªõi s·ªë l∆∞·ª£ng nh√≥m (group) l·ªõn.

---

## 3. Qu·∫£n l√Ω Truy v·∫•n (Query Management)

Spark Structured Streaming cho ph√©p b·∫°n ki·ªÉm so√°t v√≤ng ƒë·ªùi c·ªßa lu·ªìng x·ª≠ l√Ω th√¥ng qua ƒë·ªëi t∆∞·ª£ng `StreamingQuery`.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
- **StreamingQuery**: M·ªôt ƒë·ªëi t∆∞·ª£ng tr·∫£ v·ªÅ khi g·ªçi `.start()` tr√™n m·ªôt writer. N√≥ ƒë·∫°i di·ªán cho m·ªôt lu·ªìng x·ª≠ l√Ω ƒëang ch·∫°y.
- **Checkpoint Location**: V·ªã tr√≠ l∆∞u tr·∫°ng th√°i c·ªßa truy v·∫•n (offset, metadata) ƒë·ªÉ c√≥ th·ªÉ kh·ªüi ƒë·ªông l·∫°i t·ª´ ƒë√∫ng ƒëi·ªÉm ƒë√£ ch·∫øt.

### Code M·∫´u (Qu·∫£n l√Ω v√≤ng ƒë·ªùi)

```python
# Gi·∫£ s·ª≠ ƒë√£ c√≥ result l√† DataFrame x·ª≠ l√Ω xong
# Kh·ªüi t·∫°o writer
writer = result.writeStream \
    .format("parquet") \
    .outputMode("append") \
    .option("checkpointLocation", "/tmp/checkpoints/my_query") \
    .option("path", "/tmp/output/data")

# B·∫Øt ƒë·∫ßu stream v√† l·∫•y handle
query = writer.start()

# --- C√°c thao t√°c qu·∫£n l√Ω ---

# 1. Ki·ªÉm tra tr·∫°ng th√°i
print(query.status) 
# Tr·∫£ v·ªÅ: { "message": "Processing", "isDataAvailable": true, ... }

# 2. Ch·ªù cho ƒë·∫øn khi stream d·ª´ng (blocking call)
# query.awaitTermination() 

# 3. D·ª´ng stream m·ªôt c√°ch ch·ªß ƒë·ªông
# query.stop()

# 4. Ki·ªÉm tra ngo·∫°i l·ªá n·∫øu stream b·ªã ch·∫øt
try:
    query.awaitTermination()
except Exception as e:
    print(f"Query failed: {e}")
    # Ho·∫∑c ki·ªÉm tra sau:
    if query.exception().isDefined():
        print(query.exception().get())
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
- **Khi n√†o s·ª≠ d·ª•ng?**
    - Khi b·∫°n c·∫ßn build ·ª©ng d·ª•ng t·ª± ƒë·ªông h√≥a (submit job, monitor).
    - Khi c·∫ßn x·ª≠ l√Ω l·ªói ho·∫∑c n√¢ng c·∫•p code m√† kh√¥ng m·∫•t d·ªØ li·ªáu.
- **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    - Lu√¥n ƒë·∫∑t `checkpointLocation` ƒë·ªÉ l∆∞u tr·∫°ng th√°i.
    - S·ª≠ d·ª•ng `query.stop()` ƒë·ªÉ d·ª´ng gracefull (gi·ªØ nguy√™n tr·∫°ng th√°i).
    - S·ª≠ d·ª•ng `query.exception()` ƒë·ªÉ b·∫Øt l·ªói n·∫øu stream crash.
- **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    - **∆Øu**: Cho ph√©p gi√°m s√°t v√† ki·ªÉm so√°t linh ho·∫°t, ƒë·∫£m b·∫£o ƒë·ªô tin c·∫≠y (Exactly-once processing).
    - **Nh∆∞·ª£c**: C·∫ßn x·ª≠ l√Ω logic ph·ª©c t·∫°p trong code ·ª©ng d·ª•ng ƒë·ªÉ qu·∫£n l√Ω c√°c query n√†y.

---

## 4. Query Execution (C√°ch ho·∫°t ƒë·ªông c·ªßa Query)

Ph·∫ßn n√†y gi·∫£i th√≠ch s·ª± kh√°c bi·ªát gi·ªØa logic v√† v·∫≠t l√Ω trong Spark Structured Streaming.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
- **Logical Plan (K·∫ø ho·∫°ch Logic)**: T∆∞∆°ng t·ª± nh∆∞ Spark Batch. B·∫°n ƒë·ªãnh nghƒ©a c√°c thao t√°c tr√™n Dataset/Table (select, filter, join). D·ªÖ hi·ªÉu cho l·∫≠p tr√¨nh vi√™n.
- **Physical Plan (K·∫ø ho·∫°ch V·∫≠t l√Ω)**: Spark t·ª± ƒë·ªông t·ªëi ∆∞u h√≥a v√† chia nh·ªè Logical Plan th√†nh c√°c micro-batch ho·∫∑c continuous processing. N√≥ bi·∫øt c√°ch x·ª≠ l√Ω d·ªØ li·ªáu v√¥ t·∫≠n n√†y m·ªôt c√°ch incremental (t·ª´ng ph·∫ßn).

### Minh h·ªça
1.  **Developer View**: `df.filter(...).groupBy(...).count()` (Xem nh∆∞ b·∫£ng).
2.  **Spark View**: Duy·ªát qua stream, t√≠nh to√°n delta, c·∫≠p nh·∫≠t tr·∫°ng th√°i, ghi ra sink.

### V√≠ d·ª• Th·ª±c t·∫ø
Khi b·∫°n vi·∫øt:
```python
df.filter("value > 10").writeStream.start()
```
Spark kh√¥ng load to√†n b·ªô stream v√†o RAM. Thay v√†o ƒë√≥, n√≥ s·∫Ω:
1. Nh·∫≠n batch m·ªõi (v√≠ d·ª•: 1000 event).
2. √Åp d·ª•ng filter l√™n 1000 event ƒë√≥.
3. Ghi k·∫øt qu·∫£ ra sink.
4. Qu√™n ƒëi 1000 event ƒë√≥ (tr·ª´ khi c·∫ßn state).
Qu√° tr√¨nh n√†y l·∫∑p l·∫°i v√¥ h·∫°n.

---

## 5. T·ªïng quan Structured Streaming

ƒê√¢y l√† ph·∫ßn t√≥m t·∫Øt c√°c gi√° tr·ªã c·ªët l√µi c·ªßa API Structured Streaming.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Structured Streaming ƒë∆∞·ª£c thi·∫øt k·∫ø nh∆∞ m·ªôt l·ªõp bao b·ªçc quanh Spark SQL Engine, gi√∫p x·ª≠ l√Ω d·ªØ li·ªáu stream nh∆∞ th·ªÉ n√≥ ƒëang x·ª≠ l√Ω m·ªôt b·∫£ng (table) ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c.

### C√°c T√≠nh nƒÉng N·ªïi b·∫≠t
- **High-level API**: D·ª±a tr√™n Dataset/DataFrame, d·ªÖ s·ª≠ d·ª•ng.
- **Event Time & Windowing**: X·ª≠ l√Ω d·ªØ li·ªáu theo th·ªùi gian th·ª±c c·ªßa s·ª± ki·ªán (kh√¥ng ph·∫£i th·ªùi gian nh·∫≠n), h·ªó tr·ª£ c·ª≠a s·ªï th·ªùi gian (tumbling, sliding).
- **End-to-end Exactly Once**: ƒê·∫£m b·∫£o d·ªØ li·ªáu kh√¥ng b·ªã m·∫•t hay tr√πng l·∫∑p d√π l·ªói h·ªá th·ªëng.
- **Unified Engine**: C√πng m·ªôt code c√≥ th·ªÉ ch·∫°y batch ho·∫∑c stream.

### Code M·∫´u: Machine Learning tr√™n Stream
```python
# 1. ƒê·ªçc model ƒë√£ train (batch)
model = PipelineModel.load("/models/anomaly_detection")

# 2. ƒê·ªçc stream
stream_df = spark.readStream.schema(schema).json("/data/stream")

# 3. Apply model (Transform)
predictions = model.transform(stream_df)

# 4. Ghi k·∫øt qu·∫£ (Output)
query = predictions.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .start("/data/anomalies")
```

### V√≠ d·ª• Th·ª±c t·∫ø trong Ng√†nh
- **Fraud Detection (Ph√°t hi·ªán gian l·∫≠n)**: D·ªØ li·ªáu giao d·ªãch ƒë·∫øn t·ª´ Kafka -> √Åp d·ª•ng model ML ƒë√£ train s·∫µn -> N·∫øu ƒëi·ªÉm r·ªßi ro cao -> G·ª≠i c·∫£nh b√°o ngay l·∫≠p t·ª©c.
- **Real-time Dashboard**: Log server -> Aggregation theo gi√¢y -> Ghi v√†o Cassandra -> Dashboard hi·ªÉn th·ªã t·ª´ Cassandra.
- **IoT**: N·ªëi d·ªØ li·ªáu c·∫£m bi·∫øn (Stream) v·ªõi th√¥ng tin b·∫£o h√†nh (Static DB) ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ t·∫Øt m√°y hay kh√¥ng.

---

## K·∫øt lu·∫≠n

T√†i li·ªáu n√†y nh·∫•n m·∫°nh r·∫±ng Spark Structured Streaming kh√¥ng ch·ªâ l√† m·ªôt c√¥ng c·ª• nh·∫≠n d·ªØ li·ªáu, m√† l√† m·ªôt **Engin x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c m·∫°nh m·∫Ω**. N√≥ cho ph√©p:
1.  **L√†m gi√†u d·ªØ li·ªáu** qua vi·ªác join v·ªõi ngu·ªìn d·ªØ li·ªáu tham chi·∫øu.
2.  **Ki·ªÉm so√°t ƒë·∫ßu ra** linh ho·∫°t qua c√°c Output Modes.
3.  **Qu·∫£n l√Ω v√≤ng ƒë·ªùi** query chuy√™n nghi·ªáp.
4.  **X·ª≠ l√Ω logic ph·ª©c t·∫°p** (ML, Windowing) b·∫±ng c√°ch che gi·∫•u s·ª± ph·ª©c t·∫°p c·ªßa vi·ªác x·ª≠ l√Ω stream v·∫≠t l√Ω.

---

Ch√†o b·∫°n, t√¥i l√† chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide b·∫°n cung c·∫•p, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp v√† chi ti·∫øt theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Chi ti·∫øt v·ªÅ Spark Streaming Execution

T√†i li·ªáu slide b·∫°n cung c·∫•p t·∫≠p trung v√†o hai m√¥ h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu ch√≠nh trong h·ªá sinh th√°i Apache Spark: **X·ª≠ l√Ω theo Batch** (X·ª≠ l√Ω h√†ng lo·∫°t) v√† **X·ª≠ l√Ω Li√™n t·ª•c/Ti·∫øn tr√¨nh** (Continuous/Incremental Processing). ƒê√¢y l√† n·ªÅn t·∫£ng c·ªßa ki·∫øn tr√∫c h·ªá th·ªëng d·ªØ li·ªáu hi·ªán ƒë·∫°i.

## 1. Batch Execution on Spark SQL (Th·ª±c thi H√†ng lo·∫°t tr√™n Spark SQL)

### Gi·∫£i th√≠ch Kh√°i ni·ªám

**Batch Execution** (Th·ª±c thi H√†ng lo·∫°t) l√† m√¥ h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu truy·ªÅn th·ªëng, trong ƒë√≥ d·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p trong m·ªôt kho·∫£ng th·ªùi gian (batch interval) r·ªìi m·ªõi ƒë∆∞·ª£c x·ª≠ l√Ω m·ªôt l·∫ßn. M√¥ h√¨nh n√†y ph√π h·ª£p cho c√°c t√°c v·ª• kh√¥ng c·∫ßn k·∫øt qu·∫£ t·ª©c th√¨, nh∆∞ng c·∫ßn ƒë·ªô ch√≠nh x√°c cao v√† kh·∫£ nƒÉng x·ª≠ l√Ω l∆∞·ª£ng d·ªØ li·ªáu l·ªõn.

Spark SQL l√† m·ªôt module c·ªßa Spark cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu c·∫•u tr√∫c (structured data) b·∫±ng c√°ch s·ª≠ d·ª•ng API DataFrame/DataSet ho·∫∑c SQL queries. Khi k·∫øt h·ª£p v·ªõi m√¥ h√¨nh Batch, n√≥ tr·ªü th√†nh c√¥ng c·ª• m·∫°nh m·∫Ω cho ETL (Extract, Transform, Load) v√† b√°o c√°o.

### Khi n√†o s·ª≠ d·ª•ng? (Use Cases)
- **ETL (Extract, Transform, Load)**: Tr√≠ch xu·∫•t, l√†m s·∫°ch v√† t·∫£i d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn kh√°c nhau (nh∆∞ database, file logs) v√†o Data Warehouse (nh∆∞ HDFS, S3, Hive).
- **Ph√¢n t√≠ch Kinh doanh (Business Intelligence)**: T·∫°o c√°c b√°o c√°o ƒë·ªãnh k·ª≥ (h√†ng ng√†y, h√†ng tu·∫ßn, h√†ng th√°ng) t·ª´ d·ªØ li·ªáu l·ªãch s·ª≠.
- **X·ª≠ l√Ω d·ªØ li·ªáuÂéÜÂè≤ (Offline Data Processing)**: ƒê√†o t·∫°o m√¥ h√¨nh Machine Learning tr√™n to√†n b·ªô d·ªØ li·ªáu c√≥ s·∫µn.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o? (How to use)
1.  **ƒê·ªçc d·ªØ li·ªáu (Read)**: T·∫°o m·ªôt `DataFrame` t·ª´ ngu·ªìn d·ªØ li·ªáu (CSV, Parquet, JSON, JDBC...).
2.  **X·ª≠ l√Ω (Transform)**: S·ª≠ d·ª•ng c√°c thao t√°c DataFrame ho·∫∑c c√¢u l·ªánh SQL ƒë·ªÉ bi·∫øn ƒë·ªïi d·ªØ li·ªáu (filter, join, aggregate, window function).
3.  **Ghi d·ªØ li·ªáu (Write)**: Ghi k·∫øt qu·∫£ v√†o m·ªôt ƒë√≠ch ƒë·∫øn (data sink) nh∆∞ file, database, ho·∫∑c data warehouse.
4.  **Ch·∫°y Job**: Submit job l√™n Spark Cluster (s·ª≠ d·ª•ng `spark-submit`).

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
|  ∆Øu ƒëi·ªÉm (Pros) | Nh∆∞·ª£c ƒëi·ªÉm (Cons) |
| :--- | :--- |
| **ƒê·ªô ch√≠nh x√°c cao (High Accuracy)**: X·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu trong batch, kh√¥ng b·ªè s√≥t. | **ƒê·ªô tr·ªÖ cao (High Latency)**: Ph·∫£i ƒë·ª£i d·ªØ li·ªáu t√≠ch l≈©y ƒë·ªß m·ªõi x·ª≠ l√Ω, kh√¥ng ph√π h·ª£p cho ·ª©ng d·ª•ng th·ªùi gian th·ª±c. |
| **Kh·∫£ nƒÉng m·ªü r·ªông t·ªët (Scalability)**: D·ªÖ d√†ng x·ª≠ l√Ω PB d·ªØ li·ªáu b·∫±ng c√°ch th√™m node. | **X·ª≠ l√Ω ph·ª©c t·∫°p (Complexity)**: Vi·ªác t·ªëi ∆∞u h√≥a c√°c query ph·ª©c t·∫°p c√≥ th·ªÉ kh√≥ khƒÉn. |
| **D·ªÖ fault-tolerance**: Kh√¥i ph·ª•c l·∫°i job t·ª´ checkpoint n·∫øu c√≥ l·ªói. | **Kh√¥ng ph·∫£n h·ªìi t·ª©c th√¨**: Kh√¥ng th·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n d·ªØ li·ªáu m·ªõi nh·∫•t ngay l·∫≠p t·ª©c. |

### V√≠ d·ª• Th·ª±c t·∫ø
M·ªôt c√¥ng ty th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ c·∫ßn t·ªïng h·ª£p doanh s·ªë b√°n h√†ng h√†ng ng√†y. V√†o l√∫c 00:00 m·ªói ng√†y, m·ªôt Spark Job Batch ƒë∆∞·ª£c ch·∫°y ƒë·ªÉ ƒë·ªçc to√†n b·ªô d·ªØ li·ªáu ƒë∆°n h√†ng c·ªßa ng√†y h√¥m tr∆∞·ªõc t·ª´ database, t√≠nh to√°n doanh thu theo t·ª´ng danh m·ª•c s·∫£n ph·∫©m v√† ghi k·∫øt qu·∫£ v√†o m·ªôt b·∫£ng b√°o c√°o.

### Code M·∫´u (PySpark - Batch Processing)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, col

# 1. Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder \
    .appName("DailySalesReport") \
    .getOrCreate()

# 2. ƒê·ªçc d·ªØ li·ªáu t·ª´ ngu·ªìn (v√≠ d·ª•: file Parquet)
# D·ªØ li·ªáu c√≥ th·ªÉ l√† logs c·ªßa ng√†y h√¥m tr∆∞·ªõc
df_sales = spark.read.parquet("s3://my-bucket/sales_data/2023-10-26/*.parquet")

# 3. X·ª≠ l√Ω d·ªØ li·ªáu (Transform)
# T√≠nh t·ªïng doanh thu theo t·ª´ng category
df_result = df_sales.groupBy("category") \
    .agg(sum("amount").alias("total_revenue"))

# 4. Ghi k·∫øt qu·∫£ v√†o Data Warehouse (Write)
df_result.write \
    .mode("overwrite") \
    .saveAsTable("daily_sales_summary")

# D·ª´ng Spark Session
spark.stop()
```

---

## 2. Continuous & Incremental Execution (Th·ª±c thi Li√™n t·ª•c & Ti·∫øn tr√¨nh)

### Gi·∫£i th√≠ch Kh√°i ni·ªám

ƒê√¢y l√† hai m√¥ h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu g·∫ßn th·ªùi gian th·ª±c (near real-time) ho·∫∑c th·ªùi gian th·ª±c (real-time), ƒë∆∞·ª£c ph√°t tri·ªÉn ƒë·ªÉ gi·∫£i quy·∫øt nh∆∞·ª£c ƒëi·ªÉm c·ªßa Batch Processing l√† ƒë·ªô tr·ªÖ cao.

-   **Continuous Execution (Th·ª±c thi Li√™n t·ª•c)**: M√¥ h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng ng·ª´ng ngh·ªâ, d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω ngay khi n√≥ ƒë·∫øn. ƒê√¢y l√† m√¥ h√¨nh c·ªßa **Spark Structured Streaming** (trong mode `continuous`), mang l·∫°i ƒë·ªô tr·ªÖ r·∫•t th·∫•p (milisecond).
-   **Incremental Execution (Th·ª±c thi Ti·∫øn tr√¨nh)**: M√¥ h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu m·ªõi d·ª±a tr√™n k·∫øt qu·∫£ c·ªßa l·∫ßn x·ª≠ l√Ω tr∆∞·ªõc. D·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c x·ª≠ l√Ω v√† c·∫≠p nh·∫≠t v√†o k·∫øt qu·∫£ c≈©. ƒê√¢y l√† c√°ch ti·∫øp c·∫≠n c·ªßa **Spark Streaming (DStream)** v√† **Structured Streaming** (trong mode `micro-batch`).

M·ª•c ti√™u chung l√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn li√™n t·ª•c (data streams) nh∆∞ Kafka, Kinesis, ho·∫∑c logs t·ª´ server.

### Khi n√†o s·ª≠ d·ª•ng? (Use Cases)
-   **Real-time Monitoring (Gi√°m s√°t th·ªùi gian th·ª±c)**: Theo d√µi h·ªá th·ªëng, c·∫£nh b√°o ngay khi c√≥ l·ªói (v√≠ d·ª•: l·ªói 5xx t·ª´ server web).
-   **Live Dashboards (B·∫£ng ƒëi·ªÅu khi·ªÉn tr·ª±c ti·∫øp)**: Hi·ªÉn th·ªã s·ªë li·ªáu th·ªëng k√™ (s·ªë ng∆∞·ªùi d√πng ƒëang online, doanh s·ªë trong 1 ph√∫t qua) tr√™n m√†n h√¨nh l·ªõn.
-   **Fraud Detection (Ph√°t hi·ªán gian l·∫≠n)**: Ki·ªÉm tra giao d·ªãch ngay l·∫≠p t·ª©c ƒë·ªÉ ph√°t hi·ªán h√†nh vi b·∫•t th∆∞·ªùng.
-   **IoT Data Processing**: X·ª≠ l√Ω d·ªØ li·ªáu c·∫£m bi·∫øn t·ª´ c√°c thi·∫øt b·ªã k·∫øt n·ªëi.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o? (How to use)
1.  **T·∫°o Source**: K·∫øt n·ªëi v·ªõi m·ªôt streaming source (v√≠ d·ª•: Kafka topic).
2.  **X·ª≠ l√Ω (Transformations)**: √Åp d·ª•ng c√°c ph√©p bi·∫øn ƒë·ªïi t∆∞∆°ng t·ª± nh∆∞ DataFrame (select, filter, groupBy, window).
3.  **Ch·ªçn Output Mode**: Ch·ªâ ƒë·ªãnh c√°ch ghi k·∫øt qu·∫£ (Append, Update, Complete).
4.  **B·∫Øt ƒë·∫ßu Stream**: G·ªçi `query.start()` v√† theo d√µi tr·∫°ng th√°i.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| ∆Øu ƒëi·ªÉm (Pros) | Nh∆∞·ª£c ƒëi·ªÉm (Cons) |
| :--- | :--- |
| **ƒê·ªô tr·ªÖ th·∫•p (Low Latency)**: Cung c·∫•p k·∫øt qu·∫£ g·∫ßn nh∆∞ ngay l·∫≠p t·ª©c (t·ª´ mili-gi√¢y ƒë·∫øn v√†i gi√¢y). | **ƒê·ªô ch√≠nh x√°c c√≥ th·ªÉ th·∫•p h∆°n**: Trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p, c√≥ th·ªÉ x·ª≠ l√Ω "at-least-once" ho·∫∑c "at-most-once" n·∫øu kh√¥ng c·∫•u h√¨nh ƒë√∫ng exactly-once. |
| **Ph·∫£n h·ªìi nhanh (Responsive)**: Ph√π h·ª£p cho c√°c h·ªá th·ªëng c·∫ßn quy·∫øt ƒë·ªãnh t·ª©c th√¨. | **N·ªôi dung ph·ª©c t·∫°p (Stateful Management)**: Qu·∫£n l√Ω tr·∫°ng th√°i (state) gi·ªØa c√°c batch (v√≠ d·ª•: session user) c√≥ th·ªÉ ph·ª©c t·∫°p v√† t·ªën t√†i nguy√™n. |
| **Ti·∫øt ki·ªám t√†i nguy√™n**: X·ª≠ l√Ω incremental gi√∫p kh√¥ng c·∫ßn ch·∫°y l·∫°i to√†n b·ªô d·ªØ li·ªáu. | **Kh√≥ debug**: L·ªói trong streaming job kh√≥ ph√°t hi·ªán v√† s·ª≠a ch·ªØa h∆°n batch job. |

### V√≠ d·ª• Th·ª±c t·∫ø
M·ªôt c√¥ng ty giao ƒë·ªì ƒÉn c·∫ßn theo d√µi s·ªë l∆∞·ª£ng ƒë∆°n h√†ng ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω theo th·ªùi gian th·ª±c. D·ªØ li·ªáu ƒë∆°n h√†ng ƒë∆∞·ª£c ƒë·∫©y v√†o Kafka. M·ªôt Spark Streaming Job s·∫Ω ƒë·ªçc t·ª´ Kafka, ƒë·∫øm s·ªë ƒë∆°n h√†ng trong m·ªói 10 gi√¢y v√† c·∫≠p nh·∫≠t l√™n dashboard cho nh√† h√†ng ƒë·ªÉ h·ªç ƒëi·ªÅu ch·ªânh nh√¢n s·ª±.

### Code M·∫´u (PySpark - Structured Streaming)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window, count
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType

# 1. Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder \
    .appName("RealTimeOrderMonitor") \
    .getOrCreate()

# 2. ƒê·ªãnh nghƒ©a Schema cho d·ªØ li·ªáu JSON t·ª´ Kafka
order_schema = StructType([
    StructField("order_id", StringType(), True),
    StructField("restaurant_id", IntegerType(), True),
    StructField("order_time", TimestampType(), True)
])

# 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka (Source)
orders_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "orders_topic") \
    .load()

# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu binary t·ª´ Kafka sang string v√† parse JSON
parsed_orders = orders_df.select(
    from_json(col("value").cast("string"), order_schema).alias("data")
).select("data.*")

# 4. X·ª≠ l√Ω d·ªØ li·ªáu (Transform) - Windowing (C·ª≠a s·ªï th·ªùi gian)
# ƒê·∫øm s·ªë ƒë∆°n h√†ng theo t·ª´ng nh√† h√†ng, trong c·ª≠a s·ªï 1 ph√∫t tr∆∞·ª£t
order_counts = parsed_orders \
    .groupBy(
        window(col("order_time"), "1 minute"),
        col("restaurant_id")
    ) \
    .agg(count("order_id").alias("total_orders"))

# 5. Ghi k·∫øt qu·∫£ ra console (Sink)
query = order_counts.writeStream \
    .outputMode("update") \
    .format("console") \
    .start()

# Ch·∫°y stream cho ƒë·∫øn khi b·ªã d·ª´ng
query.awaitTermination()
```

---

## T√≥m t·∫Øt So s√°nh

| ƒê·∫∑c ƒëi·ªÉm | Batch Execution | Continuous/Incremental Execution |
| :--- | :--- | :--- |
| **ƒê·ªãnh nghƒ©a** | X·ª≠ l√Ω d·ªØ li·ªáu theo t·ª´ng "l√¥" l·ªõn | X·ª≠ l√Ω d·ªØ li·ªáu li√™n t·ª•c ho·∫∑c theo c√°c l√¥ nh·ªè |
| **ƒê·ªô tr·ªÖ (Latency)** | Ph√∫t, Gi·ªù, Ng√†y | Mili-gi√¢y, Gi√¢y, Ph√∫t |
| **D·ªØ li·ªáu x·ª≠ l√Ω** | To√†n b·ªô d·ªØ li·ªáu trong m·ªôt kho·∫£ng th·ªùi gian | D·ªØ li·ªáu m·ªõi ƒë·∫øn (m·ªõi nh·∫•t) |
| **C√¥ng ngh·ªá** | Spark SQL, MapReduce | Spark Streaming (DStream), Structured Streaming |
| **Ph√π h·ª£p** | Ph√¢n t√≠ch l·ªãch s·ª≠, ETL, B√°o c√°o | Gi√°m s√°t, C·∫£nh b√°o, Ph√¢n t√≠ch th·ªùi gian th·ª±c |

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "8_spark_streaming.pdf" c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp theo y√™u c·∫ßu.

---

# Ph√¢n T√≠ch Spark Streaming: T√≠nh Li√™n T·ª•c, T√≠nh Tolerant v√† K·∫ø Ho·∫°ch Th·ª±c Thi

T√†i li·ªáu n√†y t·∫≠p trung v√†o c√°c kh√°i ni·ªám c·ªët l√µi c·ªßa x·ª≠ l√Ω d·ªØ li·ªáu stream, ƒë·∫∑c bi·ªát l√† trong b·ªëi c·∫£nh c·ªßa c√°c h·ªá th·ªëng nh∆∞ Spark Streaming. Ch√∫ng ta s·∫Ω ƒëi s√¢u v√†o c√°c v·∫•n ƒë·ªÅ v·ªÅ t√≠nh li√™n t·ª•c c·ªßa aggregations v√† c√°ch h·ªá th·ªëng ƒë·∫£m b·∫£o t√≠nh tolerant (kh·∫£ nƒÉng ch·ªëng ch·ªãu l·ªói).

## 1. Continuous Aggregations (T·ªïng H·ª£p Li√™n T·ª•c)

Trong x·ª≠ l√Ω d·ªØ li·ªáu stream, c√°c ph√©p t√≠nh t·ªïng h·ª£p (aggregations) l√† v√¥ c√πng quan tr·ªçng. Tuy nhi√™n, ch√∫ng kh√°c bi·ªát so v·ªõi x·ª≠ l√Ω batch.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
- **Aggregations**: C√°c ph√©p t√≠nh nh∆∞ `count`, `sum`, `average`, `max`, `min` tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu.
- **Continuous**: D·ªØ li·ªáu ƒë·∫øn li√™n t·ª•c, kh√¥ng c√≥ ƒëi·ªÉm k·∫øt th√∫c r√µ r√†ng. Do ƒë√≥, k·∫øt qu·∫£ aggregations c≈©ng c·∫ßn ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c theo th·ªùi gian.
- **Challenge**: L√†m th·∫ø n√†o ƒë·ªÉ t√≠nh to√°n c√°c k·∫øt qu·∫£ n√†y m·ªôt c√°ch ch√≠nh x√°c v√† hi·ªáu qu·∫£, ngay c·∫£ khi c√≥ l·ªói x·∫£y ra?

### V√≠ d·ª• Th·ª±c t·∫ø
- **Th·ªëng k√™ doanh s·ªë theo th·ªùi gian th·ª±c**: C·ª≠a h√†ng tr·ª±c tuy·∫øn mu·ªën bi·∫øt t·ªïng doanh thu trong 1 gi·ªù qua. C·ª© m·ªói gi√¢y, d·ªØ li·ªáu giao d·ªãch m·ªõi l·∫°i ƒë∆∞·ª£c th√™m v√†o, v√† t·ªïng doanh thu c·∫ßn ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c.
- **Gi√°m s√°t h·ªá th·ªëng**: ƒê·∫øm s·ªë l∆∞·ª£ng l·ªói HTTP 500 trong m·ªói 5 ph√∫t ƒë·ªÉ k√≠ch ho·∫°t c·∫£nh b√°o.

### Code M·∫´u (Pseudo-code - Spark Structured Streaming)
```python
# Gi·∫£ s·ª≠ 'df' l√† DataFrame stream t·ª´ Kafka
from pyspark.sql.functions import window, sum

# Th·ª±c hi·ªán aggregations li√™n t·ª•c theo c·ª≠a s·ªï th·ªùi gian (window)
aggregated_stream = df \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        window(df.timestamp, "1 hour", "15 minutes"),
        df.product_category
    ) \
    .agg(sum("amount").alias("total_sales"))

# Ghi k·∫øt qu·∫£ ra sink (v√≠ d·ª•: Console, Database)
query = aggregated_stream.writeStream \
    .outputMode("update") \
    .format("console") \
    .start()

query.awaitTermination()
```

---

## 2. Fault-tolerance (Kh·∫£ nƒÉng Ch·ªëng ch·ªãu L·ªói)

ƒê√¢y l√† m·ªôt trong nh·ªØng y√™u c·∫ßu b·∫Øt bu·ªôc c·ªßa m·ªçi h·ªá th·ªëng ph√¢n t√°n x·ª≠ l√Ω d·ªØ li·ªáu th·ª±c t·∫ø.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
- **Definition**: Kh·∫£ nƒÉng c·ªßa h·ªá th·ªëng t·ª± ph·ª•c h·ªìi sau khi g·∫∑p l·ªói (hardware failure, network issue, software bug) m√† kh√¥ng l√†m m·∫•t d·ªØ li·ªáu ho·∫∑c t√≠nh to√°n sai.
- **Core Requirement**: *"All data and metadata in the system needs to be recoverable / replayable"* (T·∫•t c·∫£ d·ªØ li·ªáu v√† metadata trong h·ªá th·ªëng c·∫ßn c√≥ th·ªÉ ƒë∆∞·ª£c kh√¥i ph·ª•c / ph√°t l·∫°i).
    - **Data**: D·ªØ li·ªáu ƒë·∫ßu v√†o.
    - **Metadata**: Th√¥ng tin v·ªÅ tr·∫°ng th√°i c·ªßa qu√° tr√¨nh x·ª≠ l√Ω (ƒë√£ x·ª≠ l√Ω ƒë·∫øn ƒë√¢u, k·∫øt qu·∫£ t√≠nh to√°n t·∫°m th·ªùi, v.v.).
    - **Recoverable/Replayable**: H·ªá th·ªëng ph·∫£i c√≥ c∆° ch·∫ø ghi l·∫°i tr·∫°ng th√°i ƒë·ªÉ c√≥ th·ªÉ "ph√°t l·∫°i" (replay) c√°c b∆∞·ªõc t√≠nh to√°n t·ª´ ƒëi·ªÉm cu·ªëi c√πng an to√†n n·∫øu c√≥ l·ªói.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
| :--- | :--- |
| ƒê·∫£m b·∫£o t√≠nh to√†n v·∫πn d·ªØ li·ªáu (Data Integrity). | T·ªën t√†i nguy√™n l∆∞u tr·ªØ (cho checkpoint/WAL). |
| TƒÉng ƒë·ªô tin c·∫≠y c·ªßa h·ªá th·ªëng (Reliability). | C√≥ th·ªÉ g√¢y ƒë·ªô tr·ªÖ nh·ªè do ho·∫°t ƒë·ªông ghi log. |
| Cho ph√©p h·ªá th·ªëng t·ª± ƒë·ªông kh·ªüi ƒë·ªông l·∫°i (Self-healing). | C·∫•u h√¨nh ph·ª©c t·∫°p h∆°n. |

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
- **Khi n√†o s·ª≠ d·ª•ng?**: Lu√¥n lu√¥n. B·∫•t k·ª≥ h·ªá th·ªëng streaming production n√†o c≈©ng c·∫ßn c√≥ c∆° ch·∫ø Fault-tolerance.
- **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**: Trong Spark Streaming, b·∫°n b·∫≠t t√≠nh nƒÉng n√†y b·∫±ng c√°ch thi·∫øt l·∫≠p m·ªôt th∆∞ m·ª•c **Checkpoint**.
    ```scala
    ssc.checkpoint("hdfs://path/to/checkpoint/dir")
    ```

---

## 3. Fault-tolerant Planner (B·ªô L·∫≠p K·∫ø ho·∫°ch Ch·ªëng ch·ªãu L·ªói)

ƒê√¢y l√† th√†nh ph·∫ßn c·ªët l√µi c·ªßa Spark Streaming ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh tolerant. Slide m√¥ t·∫£ c∆° ch·∫ø ho·∫°t ƒë·ªông chi ti·∫øt.

### Gi·∫£i th√≠ch Kh√°i ni·ªám
- **Planner**: B·ªô l·∫≠p k·∫ø ho·∫°ch, ch·ªãu tr√°ch nhi·ªám bi√™n d·ªãch c√°c thao t√°c c·ªßa ng∆∞·ªùi d√πng th√†nh c√°c k·∫ø ho·∫°ch th·ª±c thi (execution plan).
- **Fault-tolerant**: C√≥ kh·∫£ nƒÉng ph·ª•c h·ªìi l·ªói.
- **Mechanism**: S·ª≠ d·ª•ng **Write Ahead Log (WAL)** ghi v√†o **HDFS**.

### Quy tr√¨nh ho·∫°t ƒë·ªông (Workflow)

1.  **Theo d√µi Offset**:
    -   Planner theo d√µi c√°c **offset** (v·ªã tr√≠ d·ªØ li·ªáu) c·ªßa m·ªói batch x·ª≠ l√Ω.
    -   Offset range (ph·∫°m vi offset) c·ªßa m·ªói l·∫ßn th·ª±c thi ƒë∆∞·ª£c ghi l·∫°i v√†o m·ªôt file log.

2.  **Ghi v√†o Write Ahead Log (WAL)**:
    -   **WAL**: L√† m·ªôt k·ªπ thu·∫≠t chu·∫©n trong c∆° s·ªü d·ªØ li·ªáu v√† h·ªá th·ªëng ph√¢n t√°n. Tr∆∞·ªõc khi th·ª±c thi m·ªôt t√°c v·ª•, h·ªá th·ªëng ghi "√Ω ƒë·ªãnh" th·ª±c thi ƒë√≥ v√†o log tr∆∞·ªõc.
    -   **Location**: Log ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n **HDFS** (Hadoop Distributed File System), m·ªôt h·ªá th·ªëng file c√≥ ƒë·ªô b·ªÅn cao (highly durable).

3.  **Ph·ª•c h·ªìi sau L·ªói (Recovery)**:
    -   Khi h·ªá th·ªëng g·∫∑p l·ªói v√† kh·ªüi ƒë·ªông l·∫°i:
        -   Planner ƒë·ªçc file log t·ª´ HDFS.
        -   N√≥ x√°c ƒë·ªãnh ƒë∆∞·ª£c batch n√†o ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng v√† batch n√†o b·ªã gi√°n ƒëo·∫°n.
        -   D·ª±a v√†o **offset range** ƒë∆∞·ª£c ghi trong log, Planner bi·∫øt ch√≠nh x√°c c·∫ßn ph·∫£i x·ª≠ l√Ω l·∫°i d·ªØ li·ªáu t·ª´ ƒë√¢u.
        -   **K·∫øt qu·∫£**: Th·ª±c thi l·∫°i ch√≠nh x√°c ph·∫°m vi offset b·ªã l·ªói, ƒë·∫£m b·∫£o kh√¥ng m·∫•t d·ªØ li·ªáu v√† kh√¥ng t√≠nh to√°n tr√πng l·∫∑p (n·∫øu c·∫•u h√¨nh ƒë√∫ng).

### Code M·∫´u: C·∫•u h√¨nh Checkpointing (WAL)
Trong Spark Streaming (API c≈©) ho·∫∑c Structured Streaming, vi·ªác n√†y ƒë∆∞·ª£c x·ª≠ l√Ω t·ª± ƒë·ªông khi b·∫°n b·∫≠t Checkpointing.

```scala
import org.apache.spark.streaming._

// T·∫°o StreamingContext v·ªõi Checkpointing enabled
// HDFS path l√† n∆°i l∆∞u WAL v√† metadata
val checkpointPath = "hdfs://namenode:8020/user/spark/checkpoint/my_app"

def creatingFunc(): StreamingContext = {
  val ssc = new StreamingContext(sc, Seconds(10))
  // C√°c ƒë·ªãnh nghƒ©a DStream ·ªü ƒë√¢y...
  // V√≠ d·ª•: DStream t·ª´ Kafka
  val lines = ssc.receiverStream(new CustomReceiver(host, port))
  val words = lines.flatMap(_.split(" "))
  val pairs = words.map(word => (word, 1))
  val wordCounts = pairs.reduceByKey(_ + _)
  
  wordCounts.print()
  ssc
}

val ssc = StreamingContext.getOrCreate(checkpointPath, creatingFunc)

ssc.start()
ssc.awaitTermination()
```

### V√≠ d·ª• Th·ª±c t·∫ø trong Ng√†nh
- **H·ªá th·ªëng Thanh to√°n (Payment Gateway)**:
    -   Imagine m·ªôt h·ªá th·ªëng x·ª≠ l√Ω giao d·ªãch th·∫ª t√≠n d·ª•ng.
    -   **Scenario**: M·ªôt batch giao d·ªãch ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω th√¨ server b·ªã crash.
    -   **·ª®ng d·ª•ng Fault-tolerant Planner**:
        1.  Khi server kh·ªüi ƒë·ªông l·∫°i, n√≥ ƒë·ªçc WAL t·ª´ HDFS.
        2.  N√≥ th·∫•y r·∫±ng batch c√≥ offset t·ª´ `1000` ƒë·∫øn `1050` ƒë√£ ƒë∆∞·ª£c ghi nh·∫≠n "√Ω ƒë·ªãnh" x·ª≠ l√Ω nh∆∞ng ch∆∞a nh·∫≠n ƒë∆∞·ª£c x√°c nh·∫≠n ho√†n th√†nh (commit).
        3.  N√≥ s·∫Ω **replay** ch√≠nh x√°c 50 giao d·ªãch ƒë√≥ ƒë·ªÉ ƒë·∫£m b·∫£o ti·ªÅn ƒë∆∞·ª£c chuy·ªÉn ƒë√∫ng v√† kh√¥ng b·ªã m·∫•t.
        4.  N·∫øu kh√¥ng c√≥ c∆° ch·∫ø n√†y, 50 giao d·ªãch ƒë√≥ c√≥ th·ªÉ b·ªã m·∫•t ho·∫∑c b·ªã x·ª≠ l√Ω tr√πng l·∫∑p (n·∫øu logic kh√¥ng t·ªët).

---

## T√≥m t·∫Øt Ph√¢n t√≠ch

N·ªôi dung slide n√†y nh·∫•n m·∫°nh s·ª± chuy·ªÉn ƒë·ªïi t·ª´ x·ª≠ l√Ω batch sang streaming y√™u c·∫ßu m·ªôt c∆° ch·∫ø l·∫≠p k·∫ø ho·∫°ch v√† qu·∫£n l√Ω tr·∫°ng th√°i ho√†n to√†n m·ªõi. Thay v√¨ x·ª≠ l√Ω m·ªôt kh·ªëi d·ªØ li·ªáu c·ªë ƒë·ªãnh, h·ªá th·ªëng ph·∫£i x·ª≠ l√Ω m·ªôt d√≤ng d·ªØ li·ªáu v√¥ t·∫≠n v√† li√™n t·ª•c.

**C·ªët l√µi c·ªßa gi·∫£i ph√°p l√†:**
1.  **Continuous Aggregations**: Cho ph√©p t·ªïng h·ª£p d·ªØ li·ªáu theo th·ªùi gian th·ª±c.
2.  **Write Ahead Log (WAL) on HDFS**: L√† "b·ªô nh·ªõ" an to√†n ƒë·ªÉ ghi l·∫°i m·ªçi √Ω ƒë·ªãnh th·ª±c thi.
3.  **Offset Tracking**: Bi·∫øt ch√≠nh x√°c ƒë√£ x·ª≠ l√Ω ƒë·∫øn ƒë√¢u ƒë·ªÉ c√≥ th·ªÉ b·∫Øt ƒë·∫ßu l·∫°i t·ª´ ƒë√∫ng ƒëi·ªÉm ƒë√≥.

ƒê√¢y l√† n·ªÅn t·∫£ng cho s·ª± ·ªïn ƒë·ªãnh v√† ƒë·ªô tin c·∫≠y c·ªßa c√°c h·ªá th·ªëng Big Data th·ªùi ƒë·∫°i m·ªõi nh∆∞ Spark Streaming, Flink, hay Kafka Streams.

---

Ch√†o b·∫°n, t√¥i l√† chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "8_spark_streaming.pdf" c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát theo y√™u c·∫ßu.

---

# Ph√¢n T√≠ch Spark Structured Streaming: Ki·∫øn Tr√∫c Ch·ªëng L·ªói (Fault-Tolerance)

T√†i li·ªáu n√†y t t·ªïng h·ª£p c√°c nguy√™n l√Ω c·ªët l√µi ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c ch·∫ø ƒë·ªô x·ª≠ l√Ω d·ªØ li·ªáu stream **Exactly-Once** (Ch√≠nh x√°c m·ªôt l·∫ßn) trong Apache Spark Structured Streaming. ƒê√¢y l√† ti√™u chu·∫©n v√†ng cho c√°c h·ªá th·ªëng d·ªØ li·ªáu th·ªùi gian th·ª±c y√™u c·∫ßu ƒë·ªô tin c·∫≠y cao.

## 1. Ngu·ªìn D·ªØ Li·ªáu Ch·ªëng L·ªói (Fault-tolerant Sources)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Trong x·ª≠ l√Ω stream, "ch·ªëng l·ªói" c√≥ nghƒ©a l√† h·ªá th·ªëng c√≥ th·ªÉ kh√¥i ph·ª•c l·∫°i tr·∫°ng th√°i sau khi g·∫∑p s·ª± c·ªë (nh∆∞ l·ªói m·∫°ng, l·ªói ph·∫ßn c·ª©ng) m√† kh√¥ng m·∫•t d·ªØ li·ªáu ho·∫∑c x·ª≠ l√Ω sai d·ªØ li·ªáu.

Spark Structured Streaming y√™u c·∫ßu c√°c ngu·ªìn d·ªØ li·ªáu (Sources) ph·∫£i c√≥ kh·∫£ nƒÉng **"replayable" (ph√°t l·∫°i ƒë∆∞·ª£c)**. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† n·∫øu Spark b·ªã l·ªói v√† kh·ªüi ƒë·ªông l·∫°i, n√≥ c√≥ th·ªÉ y√™u c·∫ßu ngu·ªìn d·ªØ li·ªáu g·ª≠i l·∫°i d·ªØ li·ªáu t·ª´ m·ªôt ƒëi·ªÉm c·ª• th·ªÉ (offset) m√† n√≥ ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥.

### C√°c thu·∫≠t ng·ªØ ch√≠nh:
*   **Replayable:** Kh·∫£ nƒÉng ƒë·ªçc l·∫°i d·ªØ li·ªáu t·ª´ m·ªôt th·ªùi ƒëi·ªÉm ho·∫∑c v·ªã tr√≠ (offset) x√°c ƒë·ªãnh trong qu√° kh·ª©.
*   **Offsets:** V·ªã tr√≠ con tr·ªè ƒë√°nh d·∫•u d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·∫øn ƒë√¢u (gi·ªëng nh∆∞ ƒë√°nh d·∫•u trang s√°ch).
*   **Planner:** B·ªô l·∫≠p k·∫ø ho·∫°ch c·ªßa Spark, ch·ªãu tr√°ch nhi·ªám t·ªëi ∆∞u h√≥a v√† l√™n l·ªãch th·ª±c thi c√°c t√°c v·ª•.

### V√≠ d·ª• th·ª±c t·∫ø
*   **Apache Kafka:** Khi Spark b·ªã l·ªói, n√≥ ghi l·∫°i offset cu·ªëi c√πng ƒë√£ x·ª≠ l√Ω. Khi kh·ªüi ƒë·ªông l·∫°i, n√≥ y√™u c·∫ßu Kafka g·ª≠i l·∫°i d·ªØ li·ªáu t·ª´ offset ƒë√≥.
*   **File Source (HDFS/S3):** Spark ghi nh·ªõ t√™n file v√† v·ªã tr√≠ d√≤ng d·ªØ li·ªáu (offset trong file) ƒë√£ ƒë·ªçc. N·∫øu l·ªói, n√≥ ti·∫øp t·ª•c ƒë·ªçc file t·ª´ v·ªã tr√≠ ƒë√≥.

### Code M·∫´u (PySpark)
ƒê√¢y l√† v√≠ d·ª• c·∫•u h√¨nh m·ªôt ngu·ªìn Kafka trong Structured Streaming, n∆°i t√≠nh nƒÉng replayable l√† m·∫∑c ƒë·ªãnh.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("FaultTolerantSourceExample") \
    .getOrCreate()

# C·∫•u h√¨nh ngu·ªìn Kafka (Replayable Source)
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "orders_topic") \
    .option("startingOffsets", "earliest") \ # B·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu n·∫øu ch∆∞a c√≥ checkpoint
    .load()

# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu binary sang string
lines = kafka_df.selectExpr("CAST(value AS STRING)")
```

---

## 2. Tr·∫°ng Th√°i Ch·ªëng L·ªói (Fault-tolerant State)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Khi x·ª≠ l√Ω c√°c t√°c v·ª• ph·ª©c t·∫°p (v√≠ d·ª•: t√≠nh t·ªïng doanh thu theo phi√™n ng∆∞·ªùi d√πng trong 1 gi·ªù), Spark c·∫ßn l∆∞u tr·ªØ d·ªØ li·ªáu trung gian g·ªçi l√† **State** (Tr·∫°ng th√°i).

ƒê·ªÉ ch·ªëng l·ªói, Spark kh√¥ng l∆∞u state ƒë∆°n gi·∫£n m√† s·ª≠ d·ª•ng c∆° ch·∫ø **Versioned Key-Value Maps** (B·∫£n ƒë·ªì kh√≥a-gi√° tr·ªã c√≥ phi√™n b·∫£n). D·ªØ li·ªáu state ƒë∆∞·ª£c l∆∞u tr·ªØ b·ªÅn v·ªØng (Backed by HDFS/S3) ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t khi worker ch·∫øt.

### C√°c thu·∫≠t ng·ªØ ch√≠nh:
*   **Intermediate "state data":** D·ªØ li·ªáu trung gian ƒë∆∞·ª£c t√≠nh to√°n v√† gi·ªØ l·∫°i ƒë·ªÉ s·ª≠ d·ª•ng cho c√°c batch x·ª≠ l√Ω ti·∫øp theo (v√≠ d·ª•: t·ªïng s·ªë l∆∞·ª£ng, c·ª≠a s·ªï th·ªùi gian).
*   **Versioned Maps:** M·ªói batch x·ª≠ l√Ω s·∫Ω t·∫°o ra m·ªôt phi√™n b·∫£n state m·ªõi. Spark gi·ªØ l·∫°i c√°c phi√™n b·∫£n c≈© ƒë·ªÉ ph√≤ng tr∆∞·ªùng h·ª£p c·∫ßn rollback ho·∫∑c x·ª≠ l√Ω l·∫°i.
*   **Backed by HDFS:** State kh√¥ng ch·ªâ n·∫±m trong RAM c·ªßa Worker m√† c√≤n ƒë∆∞·ª£c ghi xu·ªëng ƒëƒ©a (HDFS ho·∫∑c S3) ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô b·ªÅn (Durability).

### Code M·∫´u (Stateful Processing)
V√≠ d·ª• s·ª≠ d·ª•ng `groupBy` v·ªõi c·ª≠a s·ªï th·ªùi gian (Window), ƒë√¢y l√† n∆°i State ƒë∆∞·ª£c t·∫°o ra.

```python
from pyspark.sql.functions import window, sum

# Gi·∫£ s·ª≠ 'df' l√† stream ƒë√£ parse d·ªØ li·ªáu t·ª´ Kafka
# T√≠nh t·ªïng doanh thu theo c·ª≠a s·ªï 5 ph√∫t, theo t·ª´ng product
windowedSum = df.groupBy(
    window(df.timestamp, "5 minutes"),
    df.product
).agg(sum(df.amount).alias("total_revenue"))

# Ghi ra console
query = windowedSum.writeStream \
    .outputMode("update") \
    .format("console") \
    .option("checkpointLocation", "/tmp/checkpoints/state_example") \
    .start()
```

---

## 3. ƒêi·ªÉm T·ªõi (Sink) Ch·ªëng L·ªói

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒêi·ªÉm t·ªõi (Sink) l√† n∆°i d·ªØ li·ªáu k·∫øt qu·∫£ ƒë∆∞·ª£c ghi ra (Database, File, Kafka...). V·∫•n ƒë·ªÅ l·ªõn nh·∫•t trong stream processing l√† **ghi tr√πng (Double committing)**.

N·∫øu Spark x·ª≠ l√Ω m·ªôt batch d·ªØ li·ªáu th√†nh c√¥ng v√† chu·∫©n b·ªã ghi v√†o Database th√¨ b·ªã l·ªói m·∫°ng. Khi kh·ªüi ƒë·ªông l·∫°i, n√≥ s·∫Ω x·ª≠ l√Ω l·∫°i batch ƒë√≥. N·∫øu Sink kh√¥ng c√≥ c∆° ch·∫ø ch·ªëng l·ªói, d·ªØ li·ªáu s·∫Ω b·ªã ghi 2 l·∫ßn.

ƒê·ªÉ gi·∫£i quy·∫øt, Sink ph·∫£i l√† **Idempotent (B·∫•t bi·∫øn)**.

### C√°c thu·∫≠t ng·ªØ ch√≠nh:
*   **Idempotent:** M·ªôt thao t√°c c√≥ th·ªÉ ƒë∆∞·ª£c th·ª±c hi·ªán nhi·ªÅu l·∫ßn nh∆∞ng k·∫øt qu·∫£ v·∫´n gi·ªëng nh∆∞ ch·ªâ th·ª±c hi·ªán m·ªôt l·∫ßn. V√≠ d·ª•: `INSERT` l√† kh√¥ng idempotent (g·ªçi 2 l·∫ßn l·ªói), `INSERT ... ON DUPLICATE KEY UPDATE` l√† idempotent.
*   **Deterministic (X√°c ƒë·ªãnh):** K·∫øt qu·∫£ ƒë·∫ßu ra lu√¥n gi·ªëng nhau n·∫øu ƒë·∫ßu v√†o gi·ªëng nhau.
*   **Re-executions:** Vi·ªác Spark th·ª±c thi l·∫°i m·ªôt batch ƒë√£ l·ªói.

### V√≠ d·ª• th·ª±c t·∫ø
*   **Ghi v√†o HDFS/S3:** Spark ghi ra file t·∫°m r·ªìi d√πng thao t√°c "rename" (ƒë·ªïi t√™n) ƒë·ªÉ chuy·ªÉn file v√†o v·ªã tr√≠ ch√≠nh th·ª©c. N·∫øu b·ªã l·ªói v√† l√†m l·∫°i, file ghi tr√πng s·∫Ω b·ªã ghi ƒë√® ho·∫∑c b·ªè qua t√πy c·∫•u h√¨nh.
*   **Ghi v√†o Database (JDBC):** S·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t Upsert (Update ho·∫∑c Insert) d·ª±a tr√™n kh√≥a ch√≠nh (Primary Key).

### Code M·∫´u (Idempotent Sink)
V√≠ d·ª• ghi d·ªØ li·ªáu v√†o m·ªôt b·∫£ng SQL v·ªõi ch·∫ø ƒë·ªô Upsert.

```python
# Gi·∫£ s·ª≠ output_df l√† DataFrame k·∫øt qu·∫£
def write_to_db(batch_df, batch_id):
    # S·ª≠ d·ª•ng Pandas ho·∫∑c JDBC ƒë·ªÉ ghi v·ªõi logic Upsert
    # V√≠ d·ª• SQL: INSERT INTO results ... ON DUPLICATE KEY UPDATE ...
    batch_df.write \
        .format("jdbc") \
        .option("url", "jdbc:mysql://localhost:3306/mydb") \
        .option("dbtable", "aggregated_results") \
        .option("user", "root") \
        .option("password", "password") \
        .mode("append") \ # Ch√∫ √Ω: C·∫ßn x·ª≠ l√Ω logic idempotent ·ªü DB ho·∫∑c d√πng trigger
        .save()

query = df.writeStream \
    .foreachBatch(write_to_db) \
    .option("checkpointLocation", "/tmp/checkpoints/db_sink") \
    .start()
```

---

## 4. T·ªïng H·ª£p: ƒê·∫£m b·∫£o Exactly-Once (End-to-End)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê√¢y l√† ph·∫ßn t·ªïng k·∫øt c√¥ng th·ª©c ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c Exactly-Once Guarantee (ƒê·∫£m b·∫£o x·ª≠ l√Ω ch√≠nh x√°c m·ªôt l·∫ßn). ƒê√¢y l√† m·ª•c ti√™u cao nh·∫•t c·ªßa c√°c h·ªá th·ªëng stream processing.

C√¥ng th·ª©c bao g·ªìm 3 th√†nh ph·∫ßn:
1.  **WAL (Write-Ahead Log) + Offset Tracking:** Ghi l·∫°i log c√°c offset ƒë√£ x·ª≠ l√Ω tr∆∞·ªõc khi x·ª≠ l√Ω.
2.  **State Management:** Qu·∫£n l√Ω state tr·∫°ng th√°i an to√†n.
3.  **Fault-tolerant Sources & Sinks:** Ngu·ªìn ph√°t l·∫°i ƒë∆∞·ª£c, ƒë√≠ch ƒë·∫øn ghi kh√¥ng tr√πng l·∫∑p.

### B·∫£ng t√≥m t·∫Øt c∆° ch·∫ø Fault-Tolerance

| Th√†nh ph·∫ßn | Vai tr√≤ trong Exactly-Once | C∆° ch·∫ø ho·∫°t ƒë·ªông |
| :--- | :--- | :--- |
| **Source** | ƒê·∫£m b·∫£o d·ªØ li·ªáu kh√¥ng b·ªã m·∫•t | Replay d·ªØ li·ªáu d·ª±a tr√™n Offset ƒë√£ l∆∞u trong Checkpoint. |
| **Engine (Spark)** | ƒê·∫£m b·∫£o t√≠nh to√°n kh√¥ng b·ªã l·∫∑p | D·ª±a v√†o Checkpoint (WAL) v√† State ƒë·ªÉ bi·∫øt ƒë√£ x·ª≠ l√Ω ƒë·∫øn ƒë√¢u. |
| **Sink** | ƒê·∫£m b·∫£o output kh√¥ng b·ªã tr√πng | Ghi d·ªØ li·ªáu m·ªôt c√°ch Idempotent (kh√¥ng t·∫°o b·∫£n ghi tr√πng l·∫∑p). |

### Code M·∫´u (C·∫•u h√¨nh Checkpoint)
ƒê·ªÉ b·∫≠t Exactly-Once, b·∫Øt bu·ªôc ph·∫£i c·∫•u h√¨nh `checkpointLocation`.

```python
# ƒê√¢y l√† c·∫•u h√¨nh b·∫Øt bu·ªôc ƒë·ªÉ Spark Track Offset v√† State
query = df.writeStream \
    .format("parquet") \
    .option("path", "/output/path") \
    .option("checkpointLocation", "/tmp/checkpoints/exactly_once_demo") \
    .start()
```

---

## 5. K·∫øt Lu·∫≠n: L·ª£i √≠ch c·ªßa Structured Streaming

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Slide cu·ªëi c√πng nh·∫•n m·∫°nh r·∫±ng, v·ªõi c√°c c∆° ch·∫ø tr√™n, ng∆∞·ªùi d√πng kh√¥ng c·∫ßn ph·∫£i t·ª± m√¨nh vi·∫øt c√°c logic ph·ª©c t·∫°p v·ªÅ x·ª≠ l√Ω l·ªói.

### Khi n√†o s·ª≠ d·ª•ng?
*   Khi b·∫°n c·∫ßn x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c v·ªõi y√™u c·∫ßu **kh√¥ng ƒë∆∞·ª£c m·∫•t d·ªØ li·ªáu** v√† **kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω tr√πng l·∫∑p**.
*   Khi b·∫°n c·∫ßn x·ª≠ l√Ω c√°c t√°c v·ª• c√≥ tr·∫°ng th√°i (Stateful) nh∆∞: Session tracking, T√≠nh to√°n trung b√¨nh ƒë·ªông, C·ª≠a s·ªï th·ªùi gian.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
| :--- | :--- |
| **T·ª± ƒë·ªông h√≥a (Automatic):** Spark t·ª± ƒë·ªông x·ª≠ l√Ω retry, recovery m√† kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng. | **ƒê·ªô tr·ªÖ (Latency):** C∆° ch·∫ø ghi checkpoint v√† ƒë·∫£m b·∫£o Exactly-Once th∆∞·ªùng t·∫°o ra ƒë·ªô tr·ªÖ cao h∆°n so v·ªõi ch·∫ø ƒë·ªô At-least-once. |
| **M√£ ngu·ªìn ƒë∆°n gi·∫£n:** Developers ch·ªâ t·∫≠p trung v√†o logic nghi·ªáp v·ª• (map, reduce, aggregate), kh√¥ng lo l·∫Øng v·ªÅ l·ªói h·ªá th·ªëng. | **Y√™u c·∫ßu Sink ph·∫£i h·ªó tr·ª£:** Kh√¥ng ph·∫£i Sink n√†o c≈©ng h·ªó tr·ª£ Idempotent d·ªÖ d√†ng (v√≠ d·ª•: g·ª≠i email). |
| **T·ªëc ƒë·ªô cao:** T·ªëi ∆∞u h√≥a b·ªüi Spark SQL Engine (Catalyst, Tungsten). | **B·ªô nh·ªõ:** Qu·∫£n l√Ω State t·ªën b·ªô nh·ªõ v√† t√†i nguy√™n I/O ƒë·ªÉ l∆∞u xu·ªëng ƒëƒ©a. |

### V√≠ d·ª• th·ª±c t·∫ø trong ng√†nh
*   **Fintech (Ng√¢n h√†ng):** Ki·ªÉm tra gian l·∫≠n giao d·ªãch th·∫ª t√≠n d·ª•ng. D·ªØ li·ªáu ph·∫£i ƒë∆∞·ª£c x·ª≠ l√Ω ch√≠nh x√°c 100%, kh√¥ng ƒë∆∞·ª£c b·ªè s√≥t giao d·ªãch v√† kh√¥ng ƒë∆∞·ª£c t√≠nh to√°n sai s·ªë d∆∞.
*   **IoT (Thi·∫øt b·ªã k·∫øt n·ªëi):** Gi√°m s√°t nhi·ªát ƒë·ªô nh√† m√°y. N·∫øu sensor g·ª≠i d·ªØ li·ªáu 1000 l·∫ßn/ gi√¢y, h·ªá th·ªëng ph·∫£i aggregate (t·ªïng h·ª£p) d·ªØ li·ªáu n√†y ch√≠nh x√°c m·ªôt l·∫ßn ƒë·ªÉ t√≠nh to√°n m·ª©c trung b√¨nh, kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi vi·ªác sensor g·ª≠i l·∫°i g√≥i tin do l·ªói m·∫°ng.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide b·∫°n cung c·∫•p, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp v√† chi ti·∫øt theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch & H∆∞·ªõng d·∫´n Th·ª±c h√†nh Spark Streaming (D·ª±a tr√™n Slide 8)

T√†i li·ªáu slide n√†y d∆∞·ªùng nh∆∞ ƒëang ƒëi s√¢u v√†o m·ªôt b√†i to√°n th·ª±c t·∫ø v·ªÅ **Real-time Sentiment Analysis (Ph√¢n t√≠ch c·∫£m x√∫c th·ªùi gian th·ª±c)** tr√™n d·ªØ li·ªáu Twitter, s·ª≠ d·ª•ng b·ªô c√¥ng c·ª• c·ªßa h·ªá sinh th√°i Big Data. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt.

## 1. Ph√¢n t√≠ch Use Case & Problem Statement

N·ªôi dung slide tham chi·∫øu hai b√†i to√°n l·ªõn: Ph√¢n t√≠ch v·ªã tr√≠ Uber v√† Ph√¢n t√≠ch c·∫£m x√∫c Twitter. Ch√∫ng ta s·∫Ω t·∫≠p trung v√†o b√†i to√°n ch√≠nh ƒë∆∞·ª£c li·ªát k√™ chi ti·∫øt h∆°n: **Twitter Sentiment Analysis**.

### Problem Statement (B√†i to√°n ƒë·∫∑t ra)
*   **M·ª•c ti√™u:** X√¢y d·ª±ng h·ªá th·ªëng ph√¢n t√≠ch c·∫£m x√∫c (Sentiment) c·ªßa ng∆∞·ªùi d√πng Twitter theo th·ªùi gian th·ª±c.
*   **D·ªØ li·ªáu ƒë·∫ßu v√†o (Input):** D√≤ng tweet li√™n t·ª•c t·ª´ API Twitter.
*   **X·ª≠ l√Ω:** Ph√¢n t√≠ch n·ªôi dung vƒÉn b·∫£n ƒë·ªÉ x√°c ƒë·ªãnh c·∫£m x√∫c (Positive, Negative, Neutral).
*   **ƒê·∫ßu ra (Output):** Th·ªëng k√™ c·∫£m x√∫c theo th·ªùi gian th·ª±c ho·∫∑c l∆∞u v√†o c∆° s·ªü d·ªØ li·ªáu.

### Use Cases (Khi n√†o s·ª≠ d·ª•ng?)
*   **Social Listening:** Theo d√µi ph·∫£n ·ª©ng c·ªßa c√¥ng ch√∫ng v·ªÅ m·ªôt s·ª± ki·ªán, s·∫£n ph·∫©m ho·∫∑c th∆∞∆°ng hi·ªáu ngay khi n√≥ di·ªÖn ra.
*   **Brand Monitoring:** ƒê√°nh gi√° m·ª©c ƒë·ªô h√†i l√≤ng c·ªßa kh√°ch h√†ng ƒë·ªëi v·ªõi d·ªãch v·ª•.
*   **Trading Algorithm:** S·ª≠ d·ª•ng c·∫£m x√∫c th·ªã tr∆∞·ªùng (Market Sentiment) l√†m t√≠n hi·ªáu giao d·ªãch t·ª± ƒë·ªông (v√≠ d·ª•: Uber locations analysis mentioned in slide 61 c√≥ th·ªÉ d√πng ƒë·ªÉ ƒë·ªãnh gi√° xe ho·∫∑c d·ª± ƒëo√°n nhu c·∫ßu).

---

## 2. C√†i ƒë·∫∑t & C·∫•u h√¨nh (Importing Packages & Authorization)

ƒê·ªÉ b·∫Øt ƒë·∫ßu, ch√∫ng ta c·∫ßn c·∫•u h√¨nh m√¥i tr∆∞·ªùng v√† quy·ªÅn truy c·∫≠p API.

### A. Importing Packages (Th∆∞ vi·ªán c·∫ßn thi·∫øt)

Trong m√¥i tr∆∞·ªùng **PySpark**, c√°c th∆∞ vi·ªán sau l√† b·∫Øt bu·ªôc ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c v√† k·∫øt n·ªëi Twitter.

**Sample Code (Python/PySpark):**

```python
# Th∆∞ vi·ªán x·ª≠ l√Ω Spark Core & SQL
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Th∆∞ vi·ªán Spark Streaming (Structured Streaming)
from pyspark.streaming import StreamingContext

# Th∆∞ vi·ªán k·∫øt n·ªëi Twitter (Th∆∞·ªùng d√πng tweepy ho·∫∑c pyspark wrapper)
import tweepy
import json
```

### B. Twitter Token Authorization (C·∫•p quy·ªÅn truy c·∫≠p API)

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
Twitter API v2 y√™u c·∫ßu x√°c th·ª±c OAuth 2.0 (ho·∫∑c OAuth 1.0a) ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n v√† ki·ªÉm so√°t l∆∞u l∆∞·ª£ng truy c·∫≠p. B·∫°n c·∫ßn ƒëƒÉng k√Ω ·ª©ng d·ª•ng tr√™n Twitter Developer Portal ƒë·ªÉ l·∫•y c√°c keys sau:
*   **API Key & Secret:** X√°c ƒë·ªãnh danh t√≠nh ·ª©ng d·ª•ng.
*   **Access Token & Secret:** X√°c ƒë·ªãnh danh t√≠nh ng∆∞·ªùi d√πng (ho·∫∑c app user).

**H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**
1.  Truy c·∫≠p [Twitter Developer Portal](https://developer.twitter.com/).
2.  T·∫°o Project v√† App ƒë·ªÉ l·∫•y Credentials.
3.  S·ª≠ d·ª•ng credentials trong code Python.

**Sample Code (Python - C·∫•u h√¨nh Auth):**

```python
# Khai b√°o th√¥ng tin x√°c th·ª±c (L∆∞u √Ω: Kh√¥ng ƒë∆∞·ª£c commit keys l√™n public repo)
# N√™n l∆∞u v√†o bi·∫øn m√¥i tr∆∞·ªùng (environment variables) cho b·∫£o m·∫≠t
API_KEY = "your_api_key"
API_SECRET_KEY = "your_api_secret_key"
ACCESS_TOKEN = "your_access_token"
ACCESS_TOKEN_SECRET = "your_access_token_secret"

# C·∫•u h√¨nh OAuth v·ªõi Tweepy (Th∆∞ vi·ªán ph·ªï bi·∫øn ƒë·ªÉ stream Twitter)
def get_twitter_auth():
    auth = tweepy.OAuthHandler(API_KEY, API_SECRET_KEY)
    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
    return auth

# Ho·∫∑c c·∫•u h√¨nh tr·ª±c ti·∫øp trong Spark (n·∫øu d√πng Kafka ƒë·ªÉ ingest data)
# Spark s·∫Ω ƒë·ªçc t·ª´ Kafka topic ƒë√£ ƒë∆∞·ª£c connector stream t·ª´ Twitter
```

---

## 3. Gi·∫£i ph√°p K·ªπ thu·∫≠t & V√≠ d·ª• Chi ti·∫øt

D·ª±a tr√™n c√°c slide tham chi·∫øu, ƒë√¢y l√† c√°ch tri·ªÉn khai h·ªá th·ªëng **Spark Structured Streaming** ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu Twitter.

### Concept: Spark Structured Streaming
L√† m·ªôt engine x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n Spark SQL. N√≥ cung c·∫•p m·ªôt API gi·ªëng nh∆∞ batch processing (x·ª≠ l√Ω h√†ng lo·∫°t), nh∆∞ng ch·∫°y li√™n t·ª•c tr√™n d·ªØ li·ªáu stream.

### V√≠ d·ª• th·ª±c t·∫ø: Pipeline Ph√¢n t√≠ch C·∫£m x√∫c (Sentiment Analysis)

H·ªá th·ªëng th∆∞·ªùng bao g·ªìm 3 l·ªõp ch√≠nh:
1.  **Ingestion:** Twitter API -> Kafka (ho·∫∑c Kinesis).
2.  **Processing:** Spark Streaming ƒë·ªçc t·ª´ Kafka, x·ª≠ l√Ω NLP (Natural Language Processing).
3.  **Storage/Action:** L∆∞u k·∫øt qu·∫£ v√†o DB (MapR-DB, Cassandra) ho·∫∑c Dashboard.

#### Sample Code: ƒê·ªçc Stream v√† T√≠nh Sentiment (Pseudo-code)

```python
from pyspark.sql import SparkSession

# 1. Kh·ªüi t·∫°o Spark Session
spark = SparkSession.builder \
    .appName("TwitterSentimentAnalysis") \
    .getOrCreate()

# 2. ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka (Topic 'twitter-stream')
# ƒê√¢y l√† c√°ch ph·ªï bi·∫øn thay v√¨ ƒë·ªçc tr·ª±c ti·∫øp t·ª´ API Twitter trong Spark
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "twitter_raw") \
    .load()

# 3. Parse JSON v√† l·∫•y n·ªôi dung tweet
# Chuy·ªÉn ƒë·ªïi binary value sang string
tweets_df = kafka_df.selectExpr("CAST(value AS STRING) as json_value") \
    .select(from_json("json_value", schema).alias("data")) \
    .select("data.text", "data.created_at")

# 4. H√†m t√≠nh Sentiment (ƒê∆°n gi·∫£n h√≥a cho minh h·ªça)
# Trong th·ª±c t·∫ø, b·∫°n s·∫Ω d√πng ML model (BERT, LSTM) ho·∫∑c t·ª´ ƒëi·ªÉn (VADER)
def calculate_sentiment(text):
    positive_words = ['good', 'great', 'happy', 'love', 'awesome']
    negative_words = ['bad', 'sad', 'hate', 'terrible']
    
    score = 0
    words = text.lower().split()
    for word in words:
        if word in positive_words:
            score += 1
        elif word in negative_words:
            score -= 1
            
    if score > 0: return "Positive"
    elif score < 0: return "Negative"
    else: return "Neutral"

# ƒêƒÉng k√Ω UDF (User Defined Function)
from pyspark.sql.functions import udf
sentiment_udf = udf(calculate_sentiment, StringType())

# 5. √Åp d·ª•ng UDF v√† Xu·∫•t ra Console
result_df = tweets_df.withColumn("sentiment", sentiment_udf(tweets_df["text"]))

# Write stream to console
query = result_df.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
```

---

## 4. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng & ƒê√°nh gi√°

### Khi n√†o s·ª≠ d·ª•ng Spark Streaming cho Twitter Data?
*   **High Volume:** Khi b·∫°n c·∫ßn x·ª≠ l√Ω h√†ng tri·ªáu tweet m·ªói ng√†y (Big Data).
*   **Complex Processing:** Khi b·∫°n c·∫ßn k·∫øt h·ª£p d·ªØ li·ªáu tweet v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu kh√°c (v√≠ d·ª•: d·ªØ li·ªáu Uber ƒë·ªÉ xem c·∫£m x√∫c ng∆∞·ªùi d√πng c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn nhu c·∫ßu ƒëi xe kh√¥ng).
*   **Low Latency Requirement:** C·∫ßn k·∫øt qu·∫£ ph√¢n t√≠ch trong v√†i gi√¢y ho·∫∑c v√†i ph√∫t.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| Ti√™u ch√≠ | ∆Øu ƒëi·ªÉm (Pros) | Nh∆∞·ª£c ƒëi·ªÉm (Cons) |
| :--- | :--- | :--- |
| **T·ªëc ƒë·ªô** | X·ª≠ l√Ω song song (Parallelism) t·ªët, scale out d·ªÖ d√†ng tr√™n cluster. | ƒê·ªô tr·ªÖ (Latency) cao h∆°n c√°c engine nh·ªè nh∆∞ Flink ho·∫∑c Storm (trong m·ªôt s·ªë tr∆∞·ªùng h·ª£p). |
| **T√≠ch h·ª£p** | T√≠ch h·ª£p t·ªët v·ªõi h·ªá sinh th√°i Hadoop/Big Data (Kafka, HDFS, HBase). | Kh√≥ c·∫•u h√¨nh (Configuration heavy). |
| **API** | API Structured Streaming ƒë∆°n gi·∫£n, gi·ªëng batch processing. | Vi·ªác x·ª≠ l√Ω "Event Time" (th·ªùi gian s·ª± ki·ªán) ƒë√¥i khi ph·ª©c t·∫°p do d·ªØ li·ªáu ƒë·∫øn mu·ªôn (Late arrival). |

### V√≠ d·ª• th·ª±c t·∫ø trong ng√†nh c√¥ng nghi·ªáp
*   **Uber (Tham chi·∫øu slide 61):** Uber s·ª≠ d·ª•ng Spark ƒë·ªÉ ph√¢n t√≠ch v·ªã tr√≠ xe (Real-time analysis of popular Uber locations). H·ªç k·∫øt h·ª£p d·ªØ li·ªáu GPS th·ªùi gian th·ª±c v·ªõi c√°c y·∫øu t·ªë b√™n ngo√†i (th·ªùi ti·∫øt, s·ª± ki·ªán l·ªõn) ƒë·ªÉ ƒëi·ªÅu ch·ªânh gi√° c∆∞·ªõc (Surge Pricing).
*   **C√°c c√¥ng ty E-commerce:** Ph√¢n t√≠ch tweet v·ªÅ Black Friday ƒë·ªÉ ƒëi·ªÅu ch·ªânh qu·∫£ng c√°o ho·∫∑c inventory (kho h√†ng) t·ª©c th√¨.
*   **D·ªãch v·ª• t√†i ch√≠nh:** Ph√¢n t√≠ch tin t·ª©c Twitter ƒë·ªÉ d·ª± ƒëo√°n bi·∫øn ƒë·ªông gi√° c·ªï phi·∫øu (Sentiment Trading).

---

## 5. T√≥m t·∫Øt quy tr√¨nh th·ª±c hi·ªán (Checklist)

N·∫øu b·∫°n ƒëang l√†m theo slide n√†y, h√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ l√†m c√°c b∆∞·ªõc sau:

1.  **L·∫•y API Key:** ƒêƒÉng k√Ω Twitter Developer v√† l·∫•y Token.
2.  **C√†i ƒë·∫∑t th∆∞ vi·ªán:** `pip install pyspark tweepy kafka-python`.
3.  **Chu·∫©n b·ªã Schema:** ƒê·ªãnh nghƒ©a c·∫•u tr√∫c JSON c·ªßa tweet ƒë·ªÉ Spark parse ƒë√∫ng.
4.  **Vi·∫øt Logic UDF:** Vi·∫øt h√†m Python ƒë·ªÉ x·ª≠ l√Ω text v√† g√°n nh√£n c·∫£m x√∫c.
5.  **Ch·∫°y Streaming Job:** Kh·ªüi ƒë·ªông Spark Structured Streaming v√† theo d√µi k·∫øt qu·∫£ tr√™n Console ho·∫∑c Dashboard.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D·ª±a tr√™n c√°c slide b·∫°n cung c·∫•p (t·ª´ slide 66 ƒë·∫øn 70), t√¥i c√≥ th·ªÉ th·∫•y ƒë√¢y l√† m·ªôt quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c (Real-time) ƒëi·ªÉn h√¨nh s·ª≠ d·ª•ng **Apache Spark Streaming**.

Quy tr√¨nh n√†y bao g·ªìm: Nh·∫≠n d·ªØ li·ªáu tweet -> Tr√≠ch xu·∫•t c·∫£m x√∫c (Sentiment Analysis) -> Hi·ªÉn th·ªã k·∫øt qu·∫£/L∆∞u tr·ªØ.

D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v√† tr√¨nh b√†y l·∫°i d∆∞·ªõi d·∫°ng Markdown chuy√™n nghi·ªáp.

---

# Apache Spark Streaming: Ph√¢n t√≠ch D·ªØ li·ªáu Twitter Theo th·ªùi gian th·ª±c

T√†i li·ªáu n√†y ph√¢n t√≠ch quy tr√¨nh x·ª≠ l√Ω lu·ªìng d·ªØ li·ªáu (Streaming Pipeline) th√¥ng qua c√°c giai ƒëo·∫°n: T·∫°o d·ªØ li·ªáu, Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Sentiment), v√† L∆∞u tr·ªØ/Xu·∫•t k·∫øt qu·∫£.

## 1. Dstream Transformation (Slide 66)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
**DStream (Discretized Stream)** l√† l·ªõp d·ªØ li·ªáu c∆° b·∫£n trong Spark Streaming. N√≥ ƒë·∫°i di·ªán cho m·ªôt lu·ªìng d·ªØ li·ªáu li√™n t·ª•c (continuous flow of data). DStream c√≥ th·ªÉ ƒë∆∞·ª£c t·∫°o ra t·ª´ c√°c ngu·ªìn d·ªØ li·ªáu an to√†n (Kafka, Flume, Kinesis...) ho·∫∑c t·ª´ c√°c DStream kh√°c th√¥ng qua c√°c ph√©p bi·∫øn ƒë·ªïi (Transformations).

C√°c ph√©p bi·∫øn ƒë·ªïi n√†y chia l√†m hai lo·∫°i:
1.  **Stateless Transformations:** M·ªói batch ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªôc l·∫≠p (v√≠ d·ª•: `map`, `filter`, `reduceByKey`).
2.  **Stateful Transformations:** C·∫ßn l∆∞u tr·ªØ tr·∫°ng th√°i c·ªßa c√°c batch tr∆∞·ªõc ƒë√≥ (v√≠ d·ª•: `window`, `updateStateByKey`).

### Khi n√†o s·ª≠ d·ª•ng?
- Khi b·∫°n c·∫ßn x·ª≠ l√Ω d·ªØ li·ªáu li√™n t·ª•c t·ª´ c√°c ngu·ªìn nh∆∞ IoT, Logs server, Social Media.
- Khi c·∫ßn √°p d·ª•ng c√°c logic x·ª≠ l√Ω (ETL) ƒë∆°n gi·∫£n ho·∫∑c ph·ª©c t·∫°p l√™n t·ª´ng d√≤ng d·ªØ li·ªáu ho·∫∑c theo c·ª≠a s·ªï th·ªùi gian (Window).

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
B·∫°n ƒë·ªãnh nghƒ©a c√°c ph√©p to√°n tr√™n DStream, v√† Spark Streaming s·∫Ω t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi ch√∫ng th√†nh c√°c t√°c v·ª• (Jobs) ch·∫°y l·∫∑p l·∫°i tr√™n m·ªói batch interval.

### V√≠ d·ª• Code M·∫´u (Python - PySpark)
Gi·∫£ s·ª≠ ch√∫ng ta c√≥ m·ªôt DStream `lines` ch·ª©a vƒÉn b·∫£n tweet.

```python
from pyspark.streaming import DStream

# Gi·∫£ l·∫≠p DStream ƒë·∫ßu v√†o (trong th·ª±c t·∫ø ƒë·∫øn t·ª´ Kafka ho·∫∑c Socket)
lines = ssc.socketTextStream("localhost", 9999) # type: DStream

# 1. Ph√©p bi·∫øn ƒë·ªïi Stateless: T√°ch t·ª´ng t·ª´ (Word Count Logic)
words = lines.flatMap(lambda line: line.split(" "))

# 2. Ph√©p bi·∫øn ƒë·ªïi Stateful (Window): ƒê·∫øm t·ª´ trong c·ª≠a s·ªï 10 gi√¢y
# Window duration: 10s, Slide interval: 5s
word_counts = words.map(lambda x: (x, 1)) \
                   .reduceByKeyAndWindow(lambda a, b: a + b, lambda a, b: a - b, 10, 5)

# In k·∫øt qu·∫£
word_counts.pprint()
```

---

## 2. Generating Tweet Data (Slide 67)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê√¢y l√† giai ƒëo·∫°n **Data Ingestion (Thu th·∫≠p d·ªØ li·ªáu)**. Trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø, Spark Streaming th∆∞·ªùng k·∫øt n·ªëi tr·ª±c ti·∫øp v·ªõi Twitter API (ho·∫∑c qua Kafka) ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu real-time. Trong slide n√†y, "Generating" c√≥ th·ªÉ √°m ch·ªâ vi·ªác t·∫°o lu·ªìng d·ªØ li·ªáu gi·∫£ l·∫≠p (Mocking) ho·∫∑c thu th·∫≠p th·ª±c t·∫ø ƒë·ªÉ l√†m ƒë·∫ßu v√†o cho h·ªá th·ªëng.

### Khi n√†o s·ª≠ d·ª•ng?
- Khi x√¢y d·ª±ng Proof of Concept (PoC) ho·∫∑c test h·ªá th·ªëng m√† kh√¥ng c·∫ßn k·∫øt n·ªëi API th·∫≠t ngay.
- Khi c·∫ßn feed d·ªØ li·ªáu Twitter th·ª±c v√†o h·ªá th·ªëng ph√¢n t√≠ch c·∫£m x√∫c.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
- **C√°ch 1 (Th·ª±c t·∫ø):** S·ª≠ d·ª•ng `TwitterUtils` (c≈©) ho·∫∑c k·∫øt n·ªëi qua Kafka Producer.
- **C√°ch 2 (Th·ª≠ nghi·ªám):** T·∫°o m·ªôt Socket Server ho·∫∑c Read t·ª´ file ƒë·ªÉ ƒë·∫©y d·ªØ li·ªáu v√†o Spark.

### V√≠ d·ª• Code M·∫´u (T·∫°o d·ªØ li·ªáu gi·∫£ - Mock Data)
ƒê·ªÉ m√¥ ph·ªèng vi·ªác "Generating tweet data", ch√∫ng ta c√≥ th·ªÉ d√πng m·ªôt script Python ƒë∆°n gi·∫£n g·ª≠i d·ªØ li·ªáu qua Socket ƒë·ªÉ Spark nh·∫≠n.

**Script Python (Sender):**
```python
import socket
import time
import random

# C·∫•u h√¨nh Socket
HOST = 'localhost'
PORT = 9999

# Danh s√°ch c√°c tweet m·∫´u
tweets = [
    "I love Spark Streaming! It's amazing.",
    "Big Data is hard to learn.",
    "The weather is so bad today.",
    "Just finished my project, feeling great!",
    "I hate waiting for the code to compile."
]

s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.bind((HOST, PORT))
s.listen(1)
print(f"Listening for connection on {PORT}...")

conn, addr = s.accept()
print(f"Connected by {addr}")

try:
    while True:
        # Ch·ªçn ng·∫´u nhi√™n m·ªôt tweet v√† g·ª≠i ƒëi
        msg = random.choice(tweets)
        print(f"Sending: {msg}")
        conn.sendall((msg + "\n").encode('utf-8'))
        time.sleep(2) # G·ª≠i m·ªói 2 gi√¢y
except KeyboardInterrupt:
    conn.close()
```

---

## 3. Extracting Sentiments (Slide 68)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê√¢y l√† giai ƒëo·∫°n **Data Processing & Analytics**. M·ª•c ti√™u l√† ph√¢n t√≠ch vƒÉn b·∫£n (NLP - Natural Language Processing) ƒë·ªÉ x√°c ƒë·ªãnh c·∫£m x√∫c (Sentiment) c·ªßa tweet: **Positive (T√≠ch c·ª±c)**, **Negative (Ti√™u c·ª±c)**, ho·∫∑c **Neutral (Trung l·∫≠p)**.

Trong Spark, ch√∫ng ta s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c `map` ƒë·ªÉ √°p d·ª•ng m·ªôt h√†m x·ª≠ l√Ω l√™n t·ª´ng d√≤ng d·ªØ li·ªáu trong DStream.

### Khi n√†o s·ª≠ d·ª•ng?
- Khi c·∫ßn ph√¢n lo·∫°i √Ω ki·∫øn ng∆∞·ªùi d√πng theo th·ªùi gian th·ª±c (v√≠ d·ª•: theo d√µi ph·∫£n ·ª©ng c·ªßa ng∆∞·ªùi d√πng v·ªÅ m·ªôt s·∫£n ph·∫©m m·ªõi ra m·∫Øt).
- Khi c·∫ßn l·ªçc c√°c tweet quan tr·ªçng (ch·ªâ l·∫•y tweet ti√™u c·ª±c ƒë·ªÉ x·ª≠ l√Ω khi·∫øu n·∫°i).

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
1.  ƒê·ªãnh nghƒ©a m·ªôt h√†m (Function) th·ª±c hi·ªán logic ph√¢n t√≠ch c·∫£m x√∫c (c√≥ th·ªÉ d√πng t·ª´ ƒëi·ªÉn t·ª´ kh√≥a, ho·∫∑c model Machine Learning).
2.  √Åp d·ª•ng h√†m ƒë√≥ v√†o DStream b·∫±ng `map`.

### V√≠ d·ª• Code M·∫´u (Python)
H√†m n√†y s·∫Ω nh·∫≠n v√†o m·ªôt d√≤ng text v√† tr·∫£ v·ªÅ c·∫£m x√∫c t∆∞∆°ng ·ª©ng.

```python
# H√†m ph√¢n t√≠ch c·∫£m x√∫c ƒë∆°n gi·∫£n (Rule-based)
def get_sentiment(text):
    text = text.lower()
    positive_words = ['love', 'amazing', 'great', 'good', 'happy', 'best']
    negative_words = ['bad', 'hate', 'terrible', 'worst', 'sad', 'hard']
    
    score = 0
    for word in text.split():
        if word in positive_words:
            score += 1
        elif word in negative_words:
            score -= 1
            
    if score > 0:
        return "POSITIVE"
    elif score < 0:
        return "NEGATIVE"
    else:
        return "NEUTRAL"

# √Åp d·ª•ng v√†o DStream (trong Spark Streaming)
# lines l√† DStream t·ª´ slide 67
sentiments = lines.map(lambda tweet: (tweet, get_sentiment(tweet)))

# In k·∫øt qu·∫£ ph√¢n t√≠ch
sentiments.pprint()
```

---

## 4. Results & Output Directory (Slide 69 & 70)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê√¢y l√† giai ƒëo·∫°n **Data Sink (Xu·∫•t d·ªØ li·ªáu)**. Sau khi d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω (c√≥ th√™m c·ªôt Sentiment), ch√∫ng ta c·∫ßn l∆∞u tr·ªØ k·∫øt qu·∫£.
- **Slide 69 (Results):** Hi·ªÉn th·ªã k·∫øt qu·∫£ ra console ho·∫∑c Dashboard.
- **Slide 70 (Output Directory):** L∆∞u k·∫øt qu·∫£ v√†o file system (HDFS, S3, Local) ho·∫∑c Database (Cassandra, HBase).

### Khi n√†o s·ª≠ d·ª•ng?
- Khi c·∫ßn l∆∞u tr·ªØ d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ b√°o c√°o sau n√†y.
- Khi c·∫ßn hi·ªÉn th·ªã k·∫øt qu·∫£ live l√™n Dashboard (v√≠ d·ª•: d√πng Elasticsearch + Kibana).

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
S·ª≠ d·ª•ng c√°c h√†m `foreachRDD` ho·∫∑c `saveAsTextFiles` c·ªßa DStream.

### V√≠ d·ª• Code M·∫´u (Python)
L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch c·∫£m x√∫c v√†o th∆∞ m·ª•c output.

```python
# Gi·∫£ ƒë·ªãnh 'sentiments' l√† DStream ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü slide 68
# C·∫•u tr√∫c: (Tweet, Sentiment)

# 1. In ra Console (Slide 69 - Results)
sentiments.pprint()

# 2. L∆∞u v√†o File (Slide 70 - Output Directory)
# L∆∞u theo ƒë·ªãnh d·∫°ng time-based, t·∫°o file m·ªõi m·ªói batch interval
def save_to_file(rdd):
    # Chuy·ªÉn ƒë·ªïi RDD v·ªÅ string
    data = rdd.map(lambda x: f"Tweet: {x[0]} | Sentiment: {x[1]}")
    # L∆∞u v√†o file
    data.saveAsTextFile("hdfs://output/twitter_sentiments/" + str(time.time()))

# √Åp d·ª•ng foreachRDD ƒë·ªÉ l∆∞u t·ª´ng batch
sentiments.foreachRDD(save_to_file)
```

---

## 5. V√≠ d·ª• Th·ª±c t·∫ø trong Industry

| Use Case | M√¥ t·∫£ | C√¥ng ngh·ªá s·ª≠ d·ª•ng |
| :--- | :--- | :--- |
| **Social Listening** | C√°c c√¥ng ty l·ªõn (VinGroup, Viettel, Unilever) l·∫Øng nghe √Ω ki·∫øn kh√°ch h√†ng tr√™n Twitter/Facebook ƒë·ªÉ ƒëi·ªÅu ch·ªânh chi·∫øn d·ªãch Marketing. | Spark Streaming, Kafka, NLP Models. |
| **Fraud Detection** | Ng√¢n h√†ng ph√¢n t√≠ch log giao d·ªãch th·ªùi gian th·ª±c ƒë·ªÉ ph√°t hi·ªán h√†nh vi b·∫•t th∆∞·ªùng (spam, hack). | Spark Streaming, Machine Learning (MLlib). |
| **Smart City** | Ph√¢n t√≠ch lu·ªìng d·ªØ li·ªáu giao th√¥ng t·ª´ camera/sensor ƒë·ªÉ ƒëi·ªÅu ch·ªânh ƒë√®n t√≠n hi·ªáu. | Spark Streaming, IoT Gateway. |

## 6. T√≥m t·∫Øt ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm c·ªßa Spark Streaming

| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm (Pros)** | - **T√≠ch h·ª£p t·ªët v·ªõi Spark Ecosystem:** D·ªÖ d√†ng k·∫øt h·ª£p v·ªõi Spark SQL, MLlib, GraphX.<br>- **Fault Tolerance:** Kh·∫£ nƒÉng ph·ª•c h·ªìi l·ªói t·ªët (RDD lineage).<br>- **Micro-batch:** X·ª≠ l√Ω theo t·ª´ng ƒë·ª£t nh·ªè, d·ªÖ t√πy ch·ªânh t·ªëc ƒë·ªô. |
| **Nh∆∞·ª£c ƒëi·ªÉm (Cons)** | - **Latency:** Do c∆° ch·∫ø Micro-batch, ƒë·ªô tr·ªÖ th∆∞·ªùng cao h∆°n so v·ªõi c√°c h·ªá th·ªëng x·ª≠ l√Ω s·ª± ki·ªán th·ª±c s·ª± (True Real-time) nh∆∞ Flink (t√≠nh b·∫±ng mili-gi√¢y).<br>- **Complexity:** C√†i ƒë·∫∑t v√† b·∫£o tr√¨ cluster kh√° ph·ª©c t·∫°p. |

---

*L∆∞u √Ω: C√°c slide b·∫°n cung c·∫•p (66-70) l√† m·ªôt ph·∫ßn c·ªßa quy tr√¨nh "Twitter Sentiment Analysis" (Ph√¢n t√≠ch c·∫£m x√∫c Twitter) - m·ªôt b√†i to√°n kinh ƒëi·ªÉn trong h·ªçc Big Data.*

---

D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu ph√¢n t√≠ch chi ti·∫øt v√† tr√¨nh b√†y l·∫°i n·ªôi dung d·ª±a tr√™n c√°c slide ƒë∆∞·ª£c cung c·∫•p, d∆∞·ªõi g√≥c ƒë·ªô chuy√™n gia Big Data v√† H·ªá th·ªëng Ph√¢n t√°n.

---

# Ph√¢n t√≠ch & T·ªïng h·ª£p: X·ª≠ l√Ω D·ªØ li·ªáu Th·ªùi gian th·ª±c (Streaming) & Ph√¢n t√≠ch C·∫£m x√∫c (Sentiment Analysis)

T√†i li·ªáu n√†y d∆∞·ªùng nh∆∞ l√† ph·∫ßn k·∫øt th√∫c c·ªßa m·ªôt quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn (Big Data pipeline), t·∫≠p trung v√†o vi·ªác ph√¢n t√≠ch c·∫£m x√∫c t·ª´ c√°c tweet th·ªùi gian th·ª±c (Real-time Tweets) v√† l·∫•y c·∫£m h·ª©ng t·ª´ c√°c ph√¢n t√≠ch li√™n quan ƒë·∫øn c c·ª±u T·ªïng th·ªëng M·ªπ Donald Trump.

## 1. T·ªïng quan v·ªÅ Pipeline

D·ª±a tr√™n c√°c slide, quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu c√≥ th·ªÉ ƒë∆∞·ª£c m√¥ t·∫£ nh∆∞ sau:

1.  **Input:** D·ªØ li·ªáu tweet th√¥ (Raw Tweets) ƒë∆∞·ª£c stream t·ª´ c√°c ngu·ªìn nh∆∞ Twitter API.
2.  **Processing (Spark Streaming):**
    *   **B∆∞·ªõc 1:** L·ªçc v√† tr√≠ch xu·∫•t th√¥ng tin ng∆∞·ªùi d√πng (`Output usernames`).
    *   **B∆∞·ªõc 2:** Ph√¢n t√≠ch n·ªôi dung vƒÉn b·∫£n ƒë·ªÉ x√°c ƒë·ªãnh c·∫£m x√∫c (`Output tweets and sentiments`).
3.  **Output:** Dashboard hi·ªÉn th·ªã k·∫øt qu·∫£ ph√¢n t√≠ch theo th·ªùi gian th·ª±c.

---

## 2. Ph√¢n t√≠ch Chi ti·∫øt c√°c Giai ƒëo·∫°n

### Giai ƒëo·∫°n 1: Output Usernames (Slide 71)

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
ƒê√¢y l√† b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω (Pre-processing). Trong lu·ªìng d·ªØ li·ªáu Twitter, m·ªói b·∫£n ghi (record) ch·ª©a r·∫•t nhi·ªÅu metadata. Vi·ªác tr√≠ch xu·∫•t `username` gi√∫p ch√∫ng ta c√° th·ªÉ h√≥a d·ªØ li·ªáu, x√°c ƒë·ªãnh ngu·ªìn g·ªëc c·ªßa c·∫£m x√∫c v√† theo d√µi c√°c influencers (ng∆∞·ªùi c√≥ ·∫£nh h∆∞·ªüng).

**H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng:**
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi c·∫ßn ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng ho·∫∑c ph√¢n lo·∫°i c·∫£m x√∫c theo nh√¢n kh·∫©u h·ªçc (demographics).
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** S·ª≠ d·ª•ng c√°c thao t√°c `map` ho·∫∑c `select` trong Spark ƒë·ªÉ tr√≠ch xu·∫•t tr∆∞·ªùng `user.screen_name` t·ª´ ƒë·ªëi t∆∞·ª£ng JSON.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   *∆Øu:* Gi√∫p d·ªØ li·ªáu c√≥ c·∫•u tr√∫c h∆°n, d·ªÖ d√†ng join v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu kh√°c.
    *   *Nh∆∞·ª£c:* C√≥ th·ªÉ g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ b·∫£o m·∫≠t (PII - Th√¥ng tin nh·∫≠n d·∫°ng c√° nh√¢n) n·∫øu kh√¥ng x·ª≠ l√Ω ·∫©n danh.

**Code M·∫´u (PySpark):**
```python
from pyspark.sql.functions import col

# Gi·∫£ s·ª≠ df l√† DataFrame ch·ª©a d·ªØ li·ªáu tweet th√¥
# C·∫•u tr√∫c JSON: {"user": {"screen_name": "elonmusk"}, "text": "...", ...}

usernames_df = df.select(
    col("user.screen_name").alias("username"),
    col("timestamp")
)

usernames_df.show()
```

---

### Giai ƒëo·∫°n 2: Output Tweets and Sentiments (Slide 72)

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
ƒê√¢y l√† tr√°i tim c·ªßa h·ªá th·ªëng (Core Logic). Ch√∫ng ta √°p d·ª•ng **Sentiment Analysis** (Ph√¢n t√≠ch C·∫£m x√∫c) l√™n n·ªôi dung tweet (`text`). M·ª•c ti√™u l√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu vƒÉn b·∫£n phi c·∫•u tr√∫c th√†nh d·ªØ li·ªáu c√≥ c·∫•u tr√∫c (Structured Data) d∆∞·ªõi d·∫°ng ƒëi·ªÉm s·ªë c·∫£m x√∫c (V√≠ d·ª•: Positive, Neutral, Negative).

**H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng:**
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi c·∫ßn theo d√µi √Ω ki·∫øn c√¥ng ch√∫ng (Brand Monitoring), d·ª± b√°o th·ªã tr∆∞·ªùng ch·ª©ng kho√°n, ho·∫∑c ph√¢n t√≠ch b·∫ßu c·ª≠.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    1.  T·∫£i m·ªôt model NLP (Natural Language Processing) ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫µn (nh∆∞ VADER, TextBlob, ho·∫∑c BERT).
    2.  √Åp d·ª•ng model n√†y l√™n c·ªôt `text` c·ªßa DataFrame.
    3.  G·∫Øn nh√£n (Label) k·∫øt qu·∫£.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   *∆Øu:* Cung c·∫•p insights t·ª©c th√¨, t·ª± ƒë·ªông h√≥a quy tr√¨nh ph√¢n t√≠ch quy m√¥ l·ªõn.
    *   *Nh∆∞·ª£c:* ƒê·ªô ch√≠nh x√°c ph·ª• thu·ªôc v√†o model v√† ng√¥n ng·ªØ (ti·∫øng Anh th∆∞·ªùng hi·ªáu qu·∫£ nh·∫•t); kh√≥ x·ª≠ l√Ω sarcasm (m·ªâa mai).

**Code M·∫´u (PySpark v·ªõi UDF - User Defined Function):**
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Gi·∫£ s·ª≠ ch√∫ng ta c√≥ m·ªôt h√†m ph√¢n t√≠ch c·∫£m x√∫c ƒë∆°n gi·∫£n
def get_sentiment(text):
    # Logic ph√¢n t√≠ch c·∫£m x√∫c (V√≠ d·ª•: ƒë∆°n gi·∫£n h√≥a)
    positive_words = ['good', 'great', 'happy', 'love']
    negative_words = ['bad', 'sad', 'hate', 'worst']
    
    text_lower = text.lower()
    if any(word in text_lower for word in positive_words):
        return "Positive"
    elif any(word in text_lower for word in negative_words):
        return "Negative"
    else:
        return "Neutral"

# ƒêƒÉng k√Ω UDF
sentiment_udf = udf(get_sentiment, StringType())

# √Åp d·ª•ng v√†o DataFrame
sentiment_df = df.withColumn("sentiment", sentiment_udf(col("text")))

sentiment_df.select("text", "sentiment").show()
```

---

### Giai ƒëo·∫°n 3: Sentiments for Trump (Slide 73)

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
ƒê√¢y l√† m·ªôt v√≠ d·ª• c c·ª• th·ªÉ v·ªÅ **Filtering & Aggregation** (L·ªçc v√† T·ªïng h·ª£p). Sau khi ƒë√£ c√≥ c·ªôt `sentiment`, h·ªá th·ªëng s·∫Ω l·ªçc c√°c tweet c√≥ li√™n quan ƒë·∫øn t·ª´ kh√≥a "Trump" v√† t√≠nh to√°n ph√¢n ph·ªëi c·∫£m x√∫c (T·ª∑ l·ªá % Positive/Negative).

**H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng:**
*   **Khi n√†o s·ª≠ d·ª•ng?** Ph√¢n t√≠ch s·ª± ki·ªán c c·ª• th·ªÉ, chi·∫øn d·ªãch ch√≠nh tr·ªã, ho·∫∑c kh·ªßng ho·∫£ng truy·ªÅn th√¥ng.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** S·ª≠ d·ª•ng `filter` (ho·∫∑c `where`) ƒë·ªÉ ch·ªçn c√°c b·∫£n ghi c√≥ `text` ch·ª©a t·ª´ kh√≥a, sau ƒë√≥ `groupBy` v√† `count`.
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   *∆Øu:* Gi√∫p thu h·∫πp ph·∫°m vi ph√¢n t√≠ch (Scope), t·∫≠p trung v√†o ch·ªß ƒë·ªÅ quan t√¢m.
    *   *Nh∆∞·ª£c:* D·ªÖ b·ªã nhi·ªÖu (noise) n·∫øu t√™n ng∆∞·ªùi d√πng ho·∫∑c t·ª´ kh√≥a b·ªã tr√πng l·∫∑p v·ªõi c√°c ng·ªØ c·∫£nh kh√°c.

**Code M·∫´u (Aggregation):**
```python
# L·ªçc tweet v·ªÅ Trump v√† t√≠nh to√°n c·∫£m x√∫c
trump_sentiments = sentiment_df.filter(
    col("text").contains("Trump") | col("text").contains("Donald")
)

# T√≠nh to√°n s·ªë l∆∞·ª£ng theo t·ª´ng lo·∫°i c·∫£m x√∫c
result = trump_sentiments.groupBy("sentiment").count()

result.show()
```

---

### Giai ƒëo·∫°n 4: Applying Sentiment Analysis (Slide 74)

**Gi·∫£i th√≠ch kh√°i ni·ªám:**
Slide n√†y ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác **Tri·ªÉn khai (Deployment)** ho·∫∑c m·ªü r·ªông quy tr√¨nh. "Applying" ·ªü ƒë√¢y c√≥ th·ªÉ √°m ch·ªâ vi·ªác √°p d·ª•ng model l√™n m·ªôt t·∫≠p d·ªØ li·ªáu m·ªõi, ho·∫∑c t√≠ch h·ª£p v√†o m·ªôt pipeline ph·ª©c t·∫°p h∆°n (v√≠ d·ª•: k·∫øt h·ª£p v·ªõi Kafka ƒë·ªÉ stream data v√†o database).

**H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng:**
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi b·∫°n ƒë√£ ho√†n thi·ªán model th·ª≠ nghi·ªám v√† mu·ªën ch·∫°y tr√™n d·ªØ li·ªáu th·ª±c t·∫ø ho·∫∑c d·ªØ li·ªáu l·ªãch s·ª≠.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** Ch·∫°y batch job (n·∫øu x·ª≠ l√ΩÂéÜÂè≤Êï∞ÊçÆ) ho·∫∑c streaming job (n·∫øu x·ª≠ l√Ω data li√™n t·ª•c).
*   **∆Øu & Nh∆∞·ª£c ƒëi·ªÉm:**
    *   *∆Øu:* T·ª± ƒë·ªông h√≥a ho√†n to√†n, kh·∫£ nƒÉng m·ªü r·ªông (Scalability) cao.
    *   *Nh∆∞·ª£c:* Chi ph√≠ t√≠nh to√°n (Compute Cost) cao n·∫øu ch·∫°y 24/7.

---

## 3. V√≠ d·ª• Th·ª±c t·∫ø trong C√¥ng nghi·ªáp

**K·ªãch b·∫£n: C√¥ng ty H√†ng kh√¥ng (Airline Company)**

1.  **Input:** Stream tweets c√≥ hashtag #AirlineDelay.
2.  **Username Extraction (Slide 71):** Tr√≠ch xu·∫•t `@username` ƒë·ªÉ tr·∫£ l·ªùi kh√°ch h√†ng.
3.  **Sentiment Analysis (Slide 72):**
    *   N·∫øu tweet c√≥ t·ª´ "delay", "cancel", "lost luggage" -> G·∫Øn nh√£n **Negative**.
    *   N·∫øu tweet c√≥ t·ª´ "comfort", "on time", "great service" -> G·∫Øn nh√£n **Positive**.
4.  **Specific Analysis (Slide 73 - T∆∞∆°ng t·ª±):** L·ªçc c√°c tweet Negative v·ªÅ "Flight 123" ƒë·ªÉ b·ªô ph·∫≠n CSKH can thi·ªáp ngay l·∫≠p t·ª©c.
5.  **Action (Slide 74):** G·ª≠i ticket t·ª± ƒë·ªông ƒë·∫øn h·ªá th·ªëng CRM n·∫øu ph√°t hi·ªán Negative score > ng∆∞·ª°ng cho ph√©p.

## 4. K·∫øt lu·∫≠n (Slide 75)

C√°c slide cu·ªëi c√πng (75) l√† ph·∫ßn k·∫øt th√∫c b√†i thuy·∫øt tr√¨nh, c·∫£m ∆°n ng∆∞·ªùi nghe v√† m·ªü ƒë·∫ßu phi√™n H·ªèi & ƒê√°p (Q&A).

**T√≥m t·∫Øt c√¥ng ngh·ªá:**
*   **Big Data Framework:** Apache Spark (Spark Streaming).
*   **K·ªπ thu·∫≠t:** ETL (Extract, Transform, Load), NLP (Natural Language Processing).
*   **Ki·∫øn tr√∫c:** Stream Processing Architecture.

---

