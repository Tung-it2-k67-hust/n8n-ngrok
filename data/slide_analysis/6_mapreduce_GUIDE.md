# Ph√¢n t√≠ch chi ti·∫øt: 6_mapreduce.pdf

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D·ª±a tr√™n n·ªôi dung slide "6_mapreduce.pdf" b·∫°n cung c·∫•p, t√¥i s·∫Ω ph√¢n t√≠ch v√† tr√¨nh b√†y l·∫°i m·ªôt c√°ch chi ti·∫øt, chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát d∆∞·ªõi d·∫°ng Markdown.

---

# Ch∆∞∆°ng 6: K·ªπ thu·∫≠t X·ª≠ l√Ω D·ªØ li·ªáu L·ªõn theo Kh·ªëi (Ph·∫ßn 1) - MapReduce

B√†i vi·∫øt n√†y ph√¢n t√≠ch s√¢u v·ªÅ **MapReduce**, m·ªôt m√¥ th·ª©c x·ª≠ l√Ω d·ªØ li·ªáu n·ªÅn t·∫£ng trong h·ªá sinh th√°i Hadoop, ƒë∆∞·ª£c Google ƒë·ªÅ xu·∫•t v√† ph√°t tri·ªÉn.

## 1. T·ªïng quan v·ªÅ MapReduce (M√¥ th·ª©c X·ª≠ l√Ω D·ªØ li·ªáu)

### Kh√°i ni·ªám v√† ƒê·∫∑c ƒëi·ªÉm

**MapReduce** l√† m·ªôt m√¥ th·ª©c l·∫≠p tr√¨nh (programming model) v√† c≈©ng l√† m·ªôt framework cho ph√©p x·ª≠ l√Ω l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì song song tr√™n c√°c c·ª•m m√°y t√≠nh ph√¢n t√°n. Trong m√¥i tr∆∞·ªùng Hadoop, MapReduce l√† m√¥ th·ª©c x·ª≠ l√Ω d·ªØ li·ªáu m·∫∑c ƒë·ªãnh.

**C√°c ƒë·∫∑c ƒëi·ªÉm c·ªët l√µi:**

*   **ƒê∆°n gi·∫£n (Simplicity):** Ng∆∞·ªùi l·∫≠p tr√¨nh ch·ªâ c·∫ßn t·∫≠p trung v√†o logic x·ª≠ l√Ω d·ªØ li·ªáu (th√¥ng qua 2 h√†m Map v√† Reduce), kh√¥ng c·∫ßn quan t√¢m ƒë·∫øn vi·ªác qu·∫£n l√Ω t√†i nguy√™n ph·∫ßn c·ª©ng, ph√¢n t√°n t√°c v·ª• hay x·ª≠ l√Ω l·ªói ·ªü m·ª©c h·ªá th·ªëng.
*   **Linh ho·∫°t (Flexibility):** C√≥ th·ªÉ x·ª≠ l√Ω h·∫ßu h·∫øt c√°c b√†i to√°n d·ªØ li·ªáu th√¥ (d·ªØ li·ªáu phi c·∫•u tr√∫c, b√°n c·∫•u tr√∫c) ch·ªâ b·∫±ng c√°ch thay ƒë·ªïi logic trong h√†m Map v√† Reduce.
*   **Kh·∫£ m·ªü (Scalability):** D·ªÖ d√†ng m·ªü r·ªông quy m√¥ x·ª≠ l√Ω b·∫±ng c√°ch th√™m c√°c node (m√°y t√≠nh) v√†o c·ª•m (cluster) m√† kh√¥ng c·∫ßn thay ƒë·ªïi m√£ ngu·ªìn.

### C·∫•u tr√∫c c·ªßa m·ªôt MapReduce Job

M·ªôt ch∆∞∆°ng tr√¨nh MapReduce ƒë∆∞·ª£c g·ªçi l√† m·ªôt **Job**. Job n√†y ƒë∆∞·ª£c chia nh·ªè th√†nh nhi·ªÅu **Task** ƒë·ªôc l·∫≠p v√† ƒë∆∞·ª£c ph√¢n t√°n th·ª±c thi tr√™n c√°c node kh√°c nhau trong c·ª•m.

*   **T√≠nh ƒë·ªôc l·∫≠p:** M·ªói t√°c v·ª• (task) ch·∫°y ƒë·ªôc l·∫≠p v·ªõi c√°c t√°c v·ª• kh√°c, gi√∫p tƒÉng c∆∞·ªùng ƒë·ªô tin c·∫≠y v√† kh·∫£ nƒÉng m·ªü r·ªông.
*   **Gi·∫£m truy·ªÅn th√¥ng:** H·ªá th·ªëng t·ªëi ∆∞u h√≥a vi·ªác truy·ªÅn d·ªØ li·ªáu gi·ªØa c√°c node, ∆∞u ti√™n x·ª≠ l√Ω d·ªØ li·ªáu t·∫°i ch·ªó (data locality) ƒë·ªÉ gi·∫£m bƒÉng th√¥ng m·∫°ng.
*   **Tr√°nh ƒë·ªìng b·ªô:** C∆° ch·∫ø n√†y tr√°nh c√°c v·∫•n ƒë·ªÅ v·ªÅ deadlock hay race condition b·∫±ng c√°ch gi·ªØ c√°c t√°c v·ª• t√°ch bi·ªát nhau.

## 2. Ki·∫øn tr√∫c v√† Lu·ªìng X·ª≠ l√Ω D·ªØ li·ªáu

### M√¥ h√¨nh D·ªØ li·ªáu Input - Output

MapReduce ho·∫°t ƒë·ªông ch·ªß y·∫øu v·ªõi d·ªØ li·ªáu d·∫°ng file l·ªõn v√† th∆∞·ªùng l·∫•y d·ªØ li·ªáu t·ª´ **HDFS (Hadoop Distributed File System)**.

**Lu·ªìng d·ªØ li·ªáu:**

1.  **Input Data:** D·ªØ li·ªáu ƒë·∫ßu v√†o th∆∞·ªùng l√† m·ªôt file l·ªõn.
2.  **Input Splits (Ph√¢n chia ƒë·∫ßu v√†o):** File l·ªõn ƒë∆∞·ª£c chia th√†nh c√°c block nh·ªè h∆°n (v√≠ d·ª•: 128MB m·ªói block). C√°c block n√†y ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n c√°c node kh√°c nhau.
3.  **Data Locality:** Khi th·ª±c thi, m√£ ch∆∞∆°ng tr√¨nh MapReduce s·∫Ω ƒë∆∞·ª£c g·ª≠i ƒë·∫øn c√°c node ch·ª©a d·ªØ li·ªáu t∆∞∆°ng ·ª©ng ƒë·ªÉ x·ª≠ l√Ω t·∫°i ch·ªó, gi·∫£m thi·ªÉu vi·ªác di chuy·ªÉn d·ªØ li·ªáu qua l·∫°i gi·ªØa c√°c node.

```mermaid
graph LR
    A[Input Data: Large File] --> B{HDFS}
    B --> C[Node 1: Chunk 1]
    B --> D[Node 2: Chunk 2]
    B --> E[Node 3: Chunk 3]
    
    subgraph Map Phase
        C --> F[Mapper 1]
        D --> G[Mapper 2]
        E --> H[Mapper 3]
    end
```

### M√¥ h√¨nh L·∫≠p tr√¨nh: Map v√† Reduce

Ng∆∞·ªùi l·∫≠p tr√¨nh c·∫ßn c√†i ƒë·∫∑t ch√≠nh x√°c 2 h√†m:
1.  **Map Function**
2.  **Reduce Function**

D·ªØ li·ªáu trong MapReduce lu√¥n ƒë∆∞·ª£c x·ª≠ l√Ω d∆∞·ªõi d·∫°ng c√°c c·∫∑p **(Key - Value)**.

#### Quy tr√¨nh x·ª≠ l√Ω:

*   **Input Splits:** D·ªØ li·ªáu ƒë·∫ßu v√†o ƒë∆∞·ª£c chia nh·ªè.
*   **Map Function:** Nh·∫≠n ƒë·∫ßu v√†o l√† m·ªôt c·∫∑p `(Key, Value)` v√† sinh ra m·ªôt danh s√°ch c√°c c·∫∑p `(Key', Value')` trung gian.
*   **Reduce Function:** Nh·∫≠n ƒë·∫ßu v√†o l√† m·ªôt c·∫∑p `(Key', (list of Values))` v√† sinh ra k·∫øt qu·∫£ cu·ªëi c√πng.

| Phase | Input | Output | M√¥ t·∫£ |
| :--- | :--- | :--- | :--- |
| **Map** | `(K, V)` | `List(K', V')` | H√†m √°nh x·∫°, x·ª≠ l√Ω d·ªØ li·ªáu th√¥ v√† t·∫°o c·∫∑p kh√≥a-gi√° tr·ªã trung gian. |
| **Reduce** | `(K', List(V'))` | `List(K'', V'')` | H√†m t t·ªïng h·ª£p, gom nh√≥m d·ªØ li·ªáu theo kh√≥a v√† x·ª≠ l√Ω c√°c gi√° tr·ªã. |

## 3. Code M·∫´u v√† Minh H·ªça

ƒê·ªÉ hi·ªÉu r√µ h∆°n, ch√∫ng ta s·∫Ω xem x√©t m·ªôt b√†i to√°n kinh ƒëi·ªÉn: **ƒê·∫øm t·∫ßn su·∫•t t·ª´ (Word Count)**.

### 3.1. Pseudo-code Logic

D∆∞·ªõi ƒë√¢y l√† logic l·∫≠p tr√¨nh cho b√†i to√°n Word Count:

```text
// H√†m Map
Function Map(Key, Value):
    // Key: V·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa ƒëo·∫°n vƒÉn b·∫£n
    // Value: N·ªôi dung ƒëo·∫°n vƒÉn b·∫£n
    words = split(Value, " ") // T√°ch ƒëo·∫°n vƒÉn th√†nh c√°c t·ª´
    For each word in words:
        Emit(word, 1) // Ph√°t ra (t·ª´, 1)

// H√†m Reduce
Function Reduce(Key, Values):
    // Key: M·ªôt t·ª´ duy nh·∫•t (v√≠ d·ª•: "BigData")
    // Values: Danh s√°ch c√°c s·ªë 1 [1, 1, 1, ...]
    sum = 0
    For each value in Values:
        sum = sum + value // C·ªông d·ªìn
    Emit(Key, sum) // Ph√°t ra (t·ª´, t t·ªïng s·ªë l·∫ßn xu·∫•t hi·ªán)
```

### 3.2. Code M·∫´u Th·ª±c t·∫ø (Python)

ƒê√¢y l√† c√°ch c√†i ƒë·∫∑t MapReduce b·∫±ng Python s·ª≠ d·ª•ng th∆∞ vi·ªán `mrjob` ho·∫∑c logic t∆∞∆°ng t·ª±:

```python
from mrjob.job import MRJob

class MRWordCount(MRJob):
    
    # H√†m Map: Nh·∫≠n t·ª´ng d√≤ng v√† t√°ch t·ª´
    def mapper(self, _, line):
        words = line.split()
        for word in words:
            yield word.lower(), 1  # Emit (word, 1)

    # H√†m Reduce: T√≠nh t·ªïng
    def reducer(self, word, counts):
        yield word, sum(counts) # Emit (word, total_count)

if __name__ == '__main__':
    MRWordCount.run()
```

### 3.3. Code M·∫´u (Java - Native Hadoop API)

ƒê√¢y l√† c·∫•u tr√∫c chu·∫©n trong Java, th∆∞·ªùng d√πng cho c√°c h·ªá th·ªëng production l·ªõn:

```java
public class WordCount {

  public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one); // Emit (word, 1)
      }
    }
  }

  public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get(); // C·ªông d·ªìn c√°c gi√° tr·ªã 1
      }
      result.set(sum);
      context.write(key, result); // Emit (word, total)
    }
  }
}
```

## 4. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng v√† Ph√¢n t√≠ch

### Khi n√†o s·ª≠ d·ª•ng MapReduce? (Use Cases)

MapReduce ph√π h·ª£p v·ªõi c√°c b√†i to√°n x·ª≠ l√Ω **Batch Processing** (X·ª≠ l√Ω h√†ng lo·∫°t) tr√™n d·ªØ li·ªáu l·ªõn:

1.  **Ph√¢n t√≠ch Log System:** ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa c√°c m√£ l·ªói (Error Code) trong h√†ng tri·ªáu d√≤ng log server.
2.  **X·ª≠ l√Ω d·ªØ li·ªáu Web:** L·ªçc v√† tr√≠ch xu·∫•t th√¥ng tin t·ª´ c√°c file HTML l·ªõn (Web Crawling).
3.  **Build Search Index:** T·∫°o ch·ªâ m·ª•c t·ª´ kh√≥a cho c√°c c√¥ng c·ª• t√¨m ki·∫øm.
4.  **Ph√¢n t√≠ch d·ªØ li·ªáu khoa h·ªçc:** X·ª≠ l√Ω d·ªØ li·ªáu quan s√°t, d·ªØ li·ªáu gen, m√¥ ph·ªèng v·∫≠t l√Ω.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o? (How to use)

1.  **Chu·∫©n b·ªã Input:** ƒê∆∞a d·ªØ li·ªáu l√™n HDFS.
2.  **Vi·∫øt Code:** C√†i ƒë·∫∑t 2 h√†m `Mapper` v√† `Reducer` (ho·∫∑c s·ª≠ d·ª•ng c√°c th∆∞ vi·ªán c·∫•p cao nh∆∞ Hive/Pig n·∫øu kh√¥ng c·∫ßn t·ªëi ∆∞u h√≥a th·ªß c√¥ng).
3.  **Submit Job:** G·ª≠i job l√™n Resource Manager (trong YARN) c·ªßa Hadoop.
4.  **Theo d√µi:** Gi√°m s√°t qu√° tr√¨nh th·ª±c thi qua UI c·ªßa Hadoop (JobTracker/ResourceManager UI).

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| ∆Øu ƒëi·ªÉm (Pros) | Nh∆∞·ª£c ƒëi·ªÉm (Cons) |
| :--- | :--- |
| **Kh·∫£ nƒÉng m·ªü r·ªông (Scalability):** X·ª≠ l√Ω petabyte d·ªØ li·ªáu d·ªÖ d√†ng. | **ƒê·ªô tr·ªÖ (Latency):** Kh√¥ng ph√π h·ª£p cho th·ªùi gian th·ª±c (Real-time). MapReduce ch·∫°y ch·∫≠m tr√™n c√°c job nh·ªè. |
| **Tolerance (Ch·ªãu l·ªói):** N·∫øu m·ªôt node b·ªã l·ªói, task s·∫Ω ƒë∆∞·ª£c chuy·ªÉn sang node kh√°c t·ª± ƒë·ªông. | **Complexity (Ph·ª©c t·∫°p):** Vi·∫øt code MapReduce "thu·∫ßn" (Java) kh√° c·ªìng k·ªÅnh v√† kh√≥ debug. |
| **Data Locality:** Gi·∫£m t·∫£i network b·∫±ng c√°ch x·ª≠ l√Ω d·ªØ li·ªáu t·∫°i node l∆∞u tr·ªØ. | **Disk I/O Heavy:** Kh√° nhi·ªÅu thao t√°c ghi/ƒë·ªçc t·∫°m th·ªùi ra ƒëƒ©a c·ª©ng, g√¢y ch·∫≠m so v·ªõi x·ª≠ l√Ω trong b·ªô nh·ªõ (In-memory nh∆∞ Spark). |

## 5. V√≠ d·ª• Th·ª±c t·∫ø trong Industry

### Case Study: Facebook (Th·ªùi k·ª≥ ƒë·∫ßu)

Facebook s·ª≠ d·ª•ng MapReduce r·ªông r√£i ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu ng∆∞·ªùi d√πng v√† log.
*   **B√†i to√°n:** T√≠nh to√°n "People You May Know" (PYMK) ho·∫∑c th·ªëng k√™ l∆∞·ª£ng truy c·∫≠p theo khu v·ª±c.
*   **C√°ch l√†m:** H·ªç vi·∫øt c√°c MapReduce Job ƒë·ªÉ qu√©t qua d·ªØ li·ªáu b·∫°n b√® (Social Graph), lo·∫°i b·ªè c√°c k·∫øt n·ªëi ƒë√£ c√≥, v√† t√≠nh to√°n s·ªë l∆∞·ª£ng b·∫°n chung. Job n√†y ch·∫°y h√†ng ƒë√™m (Overnight batch jobs).

### Case Study: Yahoo!

Yahoo! l√† m·ªôt trong nh·ªØng ng∆∞·ªùi d√πng l·ªõn nh·∫•t c·ªßa Hadoop v√† MapReduce.
*   **B√†i to√°n:** X·ª≠ l√Ω d·ªØ li·ªáu t√¨m ki·∫øm v√† qu·∫£ng c√°o.
*   **C√°ch l√†m:** S·ª≠ d·ª•ng MapReduce ƒë·ªÉ ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng (User Behavior), gom nh√≥m c√°c phi√™n t√¨m ki·∫øm (Sessions) ƒë·ªÉ t·ªëi ∆∞u h√≥a v·ªã tr√≠ qu·∫£ng c√°o (Ad ranking).

---

**K·∫øt lu·∫≠n:** M·∫∑c d√π hi·ªán nay Apache Spark (x·ª≠ l√Ω in-memory) ƒë√£ tr·ªü n√™n ph·ªï bi·∫øn h∆°n do t·ªëc ƒë·ªô, nh∆∞ng **MapReduce** v·∫´n l√† n·ªÅn t·∫£ng c∆° b·∫£n, l√† kim ch·ªâ nam cho c√°c h·ªá th·ªëng ph√¢n t√°n x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn theo kh·ªëi (Batch Processing). Hi·ªÉu r√µ MapReduce l√† ch√¨a kh√≥a ƒë·ªÉ n·∫Øm b·∫Øt c√°c kh√°i ni·ªám n√¢ng cao h∆°n nh∆∞ YARN, HDFS v√† Spark.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "6_mapreduce.pdf" c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp v√† chi ti·∫øt theo y√™u c·∫ßu.

---

# üìö Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Chi ti·∫øt v·ªÅ MapReduce

B√†i vi·∫øt n√†y ph√¢n t√≠ch quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu b·∫±ng **MapReduce**, m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh c·ªët l√µi trong h·ªá th·ªëng Big Data (nh∆∞ Hadoop). Ch√∫ng ta s·∫Ω ƒëi s√¢u v√†o t·ª´ng giai ƒëo·∫°n c·ªßa m·ªôt t√°c v·ª• MapReduce ƒëi·ªÉn h√¨nh: t√≠nh t·ªïng doanh s·ªë b√°n h√†ng theo t·ª´ng nh√¢n vi√™n.

## 1. B√†i to√°n V√≠ d·ª• (Problem Statement)

Tr∆∞·ªõc khi ƒëi v√†o chi ti·∫øt k·ªπ thu·∫≠t, ch√∫ng ta c·∫ßn hi·ªÉu r√µ b√†i to√°n m√† slide ƒë∆∞a ra.

*   **ƒê·∫ßu v√†o (Input):** M·ªôt ho·∫∑c nhi·ªÅu t·ªáp vƒÉn b·∫£n l·ªõn ch·ª©a d·ªØ li·ªáu giao d·ªãch. M·ªói d√≤ng (record) bao g·ªìm:
    *   `Order ID`: M√£ ƒë∆°n h√†ng.
    *   `Employee Name`: T√™n nh√¢n vi√™n th·ª±c hi·ªán giao d·ªãch.
    *   `Sale Amount`: S·ªë ti·ªÅn giao d·ªãch.
*   **ƒê·∫ßu ra (Output):** Danh s√°ch c√°c c·∫∑p `(Key, Value)`:
    *   `Key`: T√™n nh√¢n vi√™n (`Employee Name`).
    *   `Value`: T·ªïng doanh s·ªë (`Total Sales Amount`).

### V√≠ d·ª• d·ªØ li·ªáu ƒë·∫ßu v√†o (Input Data Sample)

Gi·∫£ s·ª≠ ch√∫ng ta c√≥ file `sales_data.txt`:

```text
1001, Alice, 200
1002, Bob, 150
1003, Alice, 300
1004, Charlie, 100
1005, Bob, 250
```

---

## 2. Giai ƒëo·∫°n Map (Map Phase)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Giai ƒëo·∫°n **Map** l√† b∆∞·ªõc x·ª≠ l√Ω song song ƒë·∫ßu ti√™n. D·ªØ li·ªáu ƒë·∫ßu v√†o ƒë∆∞·ª£c chia th√†nh c√°c ph·∫ßn nh·ªè (g·ªçi l√† **Input Splits** ho·∫∑c **Chunks**). M·ªói Split ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi m·ªôt t√°c v·ª• Map ƒë·ªôc l·∫≠p (Mapper).

*   **Input:** D√≤ng d·ªØ li·ªáu th√¥ (d·∫°ng text).
*   **Output:** C√°c c·∫∑p `(Key, Value)` trung gian.
*   **Logic:** Trong v√≠ d·ª• n√†y, Mapper ƒë·ªçc t·ª´ng d√≤ng, t√°ch c√°c tr∆∞·ªùng, v√† ph√°t ra `(T√™n nh√¢n vi√™n, Doanh s·ªë)`.

### Code M·∫´u (Python Pseudo-code)

```python
# H√†m Map: Nh·∫≠n v√†o m·ªôt d√≤ng vƒÉn b·∫£n v√† ph√°t ra (key, value)
def mapper(line):
    # Gi·∫£ ƒë·ªãnh ƒë·ªãnh d·∫°ng: OrderID, EmployeeName, SaleAmount
    parts = line.split(',')
    if len(parts) == 3:
        employee_name = parts[1].strip()  # L·∫•y t√™n nh√¢n vi√™n
        sale_amount = int(parts[2].strip()) # L·∫•y s·ªë ti·ªÅn
        
        # Ph√°t ra (Key, Value)
        # Key: T√™n nh√¢n vi√™n, Value: Doanh s·ªë c·ªßa giao d·ªãch ƒë√≥
        print(f"{employee_name}\t{sale_amount}")

# V√≠ d·ª• ch·∫°y Map tr√™n m·ªôt d√≤ng:
# Input: "1001, Alice, 200"
# Output: Alice   200
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi d·ªØ li·ªáu ƒë·∫ßu v√†o l·ªõn, c·∫ßn x·ª≠ l√Ω song song ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin (Extract), chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng (Transform).
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** M·ªói Mapper x·ª≠ l√Ω m·ªôt ph·∫ßn d·ªØ li·ªáu. Mapper kh√¥ng c·∫ßn bi·∫øt c√°c Mapper kh√°c ƒëang l√†m g√¨.
*   **∆Øu ƒëi·ªÉm:** T·ªëc ƒë·ªô x·ª≠ l√Ω cao do song song h√≥a.
*   **Nh∆∞·ª£c ƒëi·ªÉm:** Mapper ch·ªâ x·ª≠ l√Ω ƒë·ªôc l·∫≠p, kh√¥ng th·ªÉ giao ti·∫øp v·ªõi nhau.

---

## 3. Giai ƒëo·∫°n Shuffle & Sort (Tr·ªôn v√† S·∫Øp x·∫øp)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
ƒê√¢y l√† giai ƒëo·∫°n "b√≠ m·∫≠t" nh∆∞ng c·ª±c k·ª≥ quan tr·ªçng, th∆∞·ªùng do h·ªá th·ªëng (nh∆∞ Hadoop YARN) t·ª± ƒë·ªông th·ª±c hi·ªán.

*   **Shuffle:** D·ªØ li·ªáu ƒë·∫ßu ra t·ª´ c√°c Mapper ƒë∆∞·ª£c t·∫≠p h·ª£p l·∫°i v√† g·ª≠i ƒë·∫øn c√°c Reducer t∆∞∆°ng ·ª©ng.
*   **Sort:** D·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp theo **Key**.
*   **Partitioning:** C√°c b·∫£n ghi c√≥ c√πng Key s·∫Ω ƒë∆∞·ª£c g·ª≠i ƒë·∫øn c√πng m·ªôt Reducer.

**Quy tr√¨nh:**
1.  H·ªá th·ªëng ƒë·ªçc ƒë·∫ßu ra c·ªßa t·∫•t c·∫£ Mapper.
2.  L·ªçc theo Key (v√≠ d·ª•: "Alice").
3.  Gom t·∫•t c·∫£ c√°c Value c·ªßa "Alice" l·∫°i th√†nh m·ªôt danh s√°ch.
4.  S·∫Øp x·∫øp danh s√°ch n√†y.

### Minh h·ªça lu·ªìng d·ªØ li·ªáu
*   Mapper 1 ph√°t ra: `(Alice, 200)`
*   Mapper 2 ph√°t ra: `(Alice, 300)`
*   **Shuffle & Sort** s·∫Ω gom l·∫°i v√† t·∫°o ƒë·∫ßu v√†o cho Reducer:
    *   Key: `Alice`
    *   Values: `[200, 300]`

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** Lu√¥n lu√¥n x·∫£y ra trong m·ªçi t√°c v·ª• MapReduce c·∫ßn gom nh√≥m (Aggregation).
*   **∆Øu ƒëi·ªÉm:** ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c chu·∫©n h√≥a tr∆∞·ªõc khi v√†o Reduce, cho ph√©p x·ª≠ l√Ω theo nh√≥m.
*   **Nh∆∞·ª£c ƒëi·ªÉm:** ƒê√¢y l√† giai ƒëo·∫°n g√¢y ra nhi·ªÅu I/O (ƒë·ªçc/ghi) nh·∫•t, c√≥ th·ªÉ l√† "n√∫t th·∫Øt c·ªï chai" (bottleneck) n·∫øu d·ªØ li·ªáu qu√° l·ªõn ho·∫∑c Key b·ªã m·∫•t c√¢n b·∫±ng (skew).

---

## 4. Giai ƒëo·∫°n Reduce (Reduce Phase)

### Gi·∫£i th√≠ch Kh√°i ni·ªám
Giai ƒëo·∫°n **Reduce** nh·∫≠n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp v√† gom nh√≥m t·ª´ b∆∞·ªõc Shuffle & Sort.

*   **Input:** M·ªôt Key v√† m·ªôt danh s√°ch c√°c Value t∆∞∆°ng ·ª©ng (v√≠ d·ª•: `Alice, [200, 300]`).
*   **Output:** K·∫øt qu·∫£ cu·ªëi c√πng c·ªßa qu√° tr√¨nh gom nh√≥m (v√≠ d·ª•: `Alice, 500`).
*   **Logic:** Trong v√≠ d·ª• n√†y, Reduce th·ª±c hi·ªán ph√©p t√≠nh c·ªông (Sum) tr√™n danh s√°ch c√°c gi√° tr·ªã.

### Code M·∫´u (Python Pseudo-code)

```python
# H√†m Reduce: Nh·∫≠n m·ªôt key v√† danh s√°ch c√°c value
def reducer(key, values):
    # values l√† m·ªôt danh s√°ch c√°c s·ªë: [200, 300, ...]
    total_sales = sum(values)
    
    # Ph√°t ra k·∫øt qu·∫£ cu·ªëi c√πng
    print(f"{key}\t{total_sales}")

# V√≠ d·ª• ch·∫°y Reduce:
# Input: Key="Alice", Values=[200, 300]
# Output: Alice   500
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi c·∫ßn t√≠nh to√°n t·ªïng h·ª£p (Aggregation) nh∆∞: T√≠nh t·ªïng (Sum), ƒë·∫øm (Count), t√¨m max/min, ho·∫∑c gom nh√≥m d·ªØ li·ªáu ph·ª©c t·∫°p.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** Reducer ch·ªâ x·ª≠ l√Ω khi nh·∫≠n ƒë∆∞·ª£c to√†n b·ªô d·ªØ li·ªáu cho m·ªôt Key nh·∫•t ƒë·ªãnh.
*   **∆Øu ƒëi·ªÉm:** X·ª≠ l√Ω logic t·∫≠p trung cho t·ª´ng nh√≥m d·ªØ li·ªáu.
*   **Nh∆∞·ª£c ƒëi·ªÉm:** N·∫øu m·ªôt Key c√≥ qu√° nhi·ªÅu Value (v√≠ d·ª•: nh√¢n vi√™n "Alice" c√≥ 1 tri·ªáu giao d·ªãch), Reducer ƒë√≥ s·∫Ω ch·∫°y r·∫•t l√¢u v√† c√≥ th·ªÉ g√¢y l·ªói h·∫øt b·ªô nh·ªõ (OOM - Out of Memory).

---

## 5. V√≠ d·ª• Th·ª±c t·∫ø trong Ng√†nh C√¥ng Nghi·ªáp

D∆∞·ªõi ƒë√¢y l√† c√°c tr∆∞·ªùng h·ª£p √°p d·ª•ng m√¥ h√¨nh MapReduce t∆∞∆°ng t·ª± nh∆∞ v√≠ d·ª• tr√™n:

| Ng√†nh | B√†i to√°n | Minh h·ªça MapReduce |
| :--- | :--- | :--- |
| **B√°n l·∫ª (Retail)** | **Ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng** | **Map:** ƒê·ªçc log mua h√†ng, tr√≠ch xu·∫•t `(CustomerID, ProductID)`. <br> **Reduce:** ƒê·∫øm s·ªë l∆∞·ª£ng s·∫£n ph·∫©m m·ªói kh√°ch mua ƒë·ªÉ ƒë·ªÅ xu·∫•t (Recommendation). |
| **Y t·∫ø (Healthcare)** | **Th·ªëng k√™ b·ªánh √°n** | **Map:** ƒê·ªçc d·ªØ li·ªáu b·ªánh nh√¢n, tr√≠ch xu·∫•t `(DiseaseName, PatientCount)`. <br> **Reduce:** T√≠nh t·ªïng s·ªë ca b·ªánh theo t·ª´ng lo·∫°i b·ªánh. |
| **M·∫°ng x√£ h·ªôi (Social Media)** | **T√≠nh to√°n Degree of Friendship** | **Map:** ƒê·ªçc danh s√°ch b·∫°n b√®, ph√°t ra `(UserA, UserB)`. <br> **Reduce:** Gom nh√≥m ƒë·ªÉ t√¨m ra ng∆∞·ªùi d√πng n√†o c√≥ nhi·ªÅu b·∫°n chung nh·∫•t. |
| **Log Analysis** | **T√≠nh to√°n truy c·∫≠p theo URL** | **Map:** ƒê·ªçc log server, tr√≠ch xu·∫•t `(URL, 1)`. <br> **Reduce:** T√≠nh t·ªïng s·ªë truy c·∫≠p (Traffic) cho m·ªói URL. |

---

## 6. T√≥m t·∫Øt Lu·ªìng D·ªØ li·ªáu (Data Flow Summary)

D·ª±a tr√™n slide, lu·ªìng x·ª≠ l√Ω ho√†n ch·ªânh nh∆∞ sau:

1.  **Input Split:** File ƒë·∫ßu v√†o ƒë∆∞·ª£c chia nh·ªè.
2.  **Map:** M·ªói Mapper x·ª≠ l√Ω m·ªôt Split, sinh ra `(Key, Value)` t·∫°m th·ªùi.
3.  **Partition & Shuffle:** D·ªØ li·ªáu ƒë∆∞·ª£c g·ª≠i ƒëi d·ª±a tr√™n h√†m bƒÉm (Hash) c·ªßa Key.
4.  **Sort:** D·ªØ li·ªáu ƒë∆∞·ª£c s·∫Øp x·∫øp theo Key ngay t·∫°i Mapper tr∆∞·ªõc khi g·ª≠i ƒëi.
5.  **Grouping:** C√°c Value c·ªßa c√πng m·ªôt Key ƒë∆∞·ª£c gom th√†nh m·ªôt danh s√°ch.
6.  **Reduce:** Reducer t√≠nh to√°n v√† sinh ra k·∫øt qu·∫£ cu·ªëi c√πng.
7.  **Output:** File k·∫øt qu·∫£ ch·ª©a `(Key, Total Value)`.

### K·∫øt lu·∫≠n
MapReduce l√† m·ªôt m√¥ h√¨nh m·∫°nh m·∫Ω ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu quy m√¥ l·ªõn (Batch Processing). Tuy nhi√™n, ƒë·ªÉ t·ªëi ∆∞u h√≥a, ng∆∞·ªùi l·∫≠p tr√¨nh c·∫ßn t·∫≠p trung v√†o vi·ªác vi·∫øt h√†m Map v√† Reduce hi·ªáu qu·∫£, ƒë·ªìng th·ªùi ch√∫ √Ω ƒë·∫øn vi·ªác c√¢n b·∫±ng d·ªØ li·ªáu (Data Skew) ƒë·ªÉ tr√°nh t·∫Øc ngh·∫Ωn t·∫°i giai ƒëo·∫°n Reduce.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide b·∫°n cung c·∫•p, tr√¨nh b√†y m·ªôt c√°ch chuy√™n nghi·ªáp v√† chi ti·∫øt theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Th·ª±c h√†nh MapReduce v·ªõi B√†i to√°n Word Count

B√†i vi·∫øt n√†y ƒëi s√¢u v√†o ph√¢n t√≠ch lu·ªìng x·ª≠ l√Ω d·ªØ li·ªáu c·ªßa MapReduce th√¥ng qua b√†i to√°n kinh ƒëi·ªÉn "Word Count" (ƒê·∫øm t·ª´). Ch√∫ng ta s·∫Ω kh√°m ph√° c√°ch h·ªá th·ªëng ph√¢n t√°n n√†y ho·∫°t ƒë·ªông, vai tr√≤ c·ªßa c√°c th√†nh ph·∫ßn ch√≠nh v√† c√°ch tri·ªÉn khai th·ª±c t·∫ø.

## 1. Lu·ªìng x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi b√†i to√°n Word Count

B√†i to√°n Word Count l√† "Hello World" c·ªßa th·∫ø gi·ªõi Big Data. M·ª•c ti√™u l√† ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói t·ª´ trong m·ªôt ho·∫∑c nhi·ªÅu t√†i li·ªáu vƒÉn b·∫£n l·ªõn.

### Kh√°i ni·ªám ch√≠nh: Lu·ªìng x·ª≠ l√Ω (Data Flow)

Lu·ªìng x·ª≠ l√Ω trong MapReduce bao g·ªìm 3 giai ƒëo·∫°n ch√≠nh:

1.  **Input (ƒê·∫ßu v√†o):** D·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh c√°c **Input Splits** (ƒëo·∫°n d·ªØ li·ªáu ph√¢n v√πng), m·ªói split ƒë∆∞·ª£c giao cho m·ªôt **Mapper**.
2.  **Map:** Mapper x·ª≠ l√Ω t·ª´ng d√≤ng d·ªØ li·ªáu, sinh ra c√°c c·∫∑p `(Key, Value)` trung gian. V·ªõi Word Count, Key l√† t·ª´ (word), Value l√† s·ªë ƒë·∫øm (th∆∞·ªùng l√† 1).
3.  **Reduce:** C√°c gi√° tr·ªã (Values) c√≥ c√πng Key ƒë∆∞·ª£c gom nh√≥m v√† truy·ªÅn ƒë·∫øn **Reducer**. Reducer th·ª±c hi·ªán ph√©p t√≠nh t t·ªïng h·ª£p (v√≠ d·ª•: c·ªông c√°c gi√° tr·ªã 1 l·∫°i) ƒë·ªÉ cho ra k·∫øt qu·∫£ cu·ªëi c√πng.

### V√≠ d·ª• Minh h·ªça Lu·ªìng x·ª≠ l√Ω (Pseudo-code)

D∆∞·ªõi ƒë√¢y l√† lu·ªìng logic c·ªßa Word Count ƒë∆∞·ª£c th·ªÉ hi·ªán b·∫±ng m√£ gi·∫£:

```python
# D·ªØ li·ªáu ƒë·∫ßu v√†o (Input Splits)
Input = [
    "hello world",
    "hello hadoop"
]

# --- Giai ƒëo·∫°n MAP ---
def map_function(document):
    for word in document.split():
        emit(word, 1)

# K·∫øt qu·∫£ sau Map (Intermediate Key-Value Pairs):
# ("hello", 1), ("world", 1), ("hello", 1), ("hadoop", 1)

# --- Giai ƒëo·∫°n SHUFFLE & SORT (T·ª± ƒë·ªông c·ªßa h·ªá th·ªëng) ---
# H·ªá th·ªëng t·ª± ƒë·ªông gom nh√≥m c√°c Key gi·ªëng nhau:
# ("hello", [1, 1])
# ("world", [1])
# ("hadoop", [1])

# --- Giai ƒëo·∫°n REDUCE ---
def reduce_function(key, values):
    sum = 0
    for value in values:
        sum += value
    emit(key, sum)

# K·∫øt qu·∫£ cu·ªëi (Output):
# ("hello", 2)
# ("world", 1)
# ("hadoop", 1)
```

---

## 2. Ch∆∞∆°ng tr√¨nh Word Count th·ª±c t·∫ø

ƒê·ªÉ tri·ªÉn khai tr√™n h·ªá th·ªëng th·ª±c t·∫ø (nh∆∞ Hadoop MapReduce), ch√∫ng ta c·∫ßn vi·∫øt code Java (ho·∫∑c Python streaming). D∆∞·ªõi ƒë√¢y l√† c√°c v√≠ d·ª• chi ti·∫øt.

### V√≠ d·ª• 1: M√£ ngu·ªìn Java (Hadoop MapReduce)

ƒê√¢y l√† c√°ch ti·∫øp c·∫≠n chu·∫©n trong h·ªá sinh th√°i Hadoop.

#### Mapper Class
```java
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    public void map(LongWritable key, Text value, Context context) 
            throws IOException, InterruptedException {
        
        // T√°ch d√≤ng vƒÉn b·∫£n th√†nh c√°c t·ª´
        StringTokenizer itr = new StringTokenizer(value.toString());
        
        while (itr.hasMoreTokens()) {
            word.set(itr.nextToken());
            // Ph√°t ra (Key, Value) = (T·ª´, 1)
            context.write(word, one);
        }
    }
}
```

#### Reducer Class
```java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    
    private IntWritable result = new IntWritable();

    @Override
    public void reduce(Text key, Iterable<IntWritable> values, Context context) 
            throws IOException, InterruptedException {
        
        int sum = 0;
        // C·ªông t·∫•t c·∫£ c√°c gi√° tr·ªã (1, 1, ...) l·∫°i
        for (IntWritable val : values) {
            sum += val.get();
        }
        result.set(sum);
        // Ph√°t ra (Key, Value) cu·ªëi c√πng = (T·ª´, T·ªïng s·ªë l∆∞·ª£ng)
        context.write(key, result);
    }
}
```

### V√≠ d·ª• 2: S·ª≠ d·ª•ng Python (Hadoop Streaming)
N·∫øu b·∫°n mu·ªën ph√°t tri·ªÉn nhanh ho·∫∑c d√πng Python, Hadoop cung c·∫•p c√¥ng c·ª• **Hadoop Streaming**.

**File `mapper.py`:**
```python
#!/usr/bin/env python3
import sys

def main():
    for line in sys.stdin:
        line = line.strip()
        words = line.split()
        for word in words:
            print(f"{word}\t1")

if __name__ == "__main__":
    main()
```

**File `reducer.py`:**
```python
#!/usr/bin/env python3
import sys

def main():
    current_word = None
    current_count = 0
    word = None

    for line in sys.stdin:
        line = line.strip()
        word, count = line.split('\t', 1)
        
        try:
            count = int(count)
        except ValueError:
            continue

        if current_word == word:
            current_count += count
        else:
            if current_word:
                print(f"{current_word}\t{current_count}")
            current_count = count
            current_word = word

    if current_word == word:
        print(f"{current_word}\t{current_count}")

if __name__ == "__main__":
    main()
```

**C√°ch ch·∫°y l·ªánh:**
```bash
hadoop jar hadoop-streaming.jar \
    -input /input \
    -output /output \
    -mapper mapper.py \
    -reducer reducer.py \
    -file mapper.py \
    -file reducer.py
```

---

## 3. MapReduce tr√™n m√¥i tr∆∞·ªùng ph√¢n t√°n

Khi d·ªØ li·ªáu qu√° l·ªõn ƒë·ªÉ x·ª≠ l√Ω tr√™n m·ªôt m√°y, MapReduce s·∫Ω ph√¢n chia c√¥ng vi·ªác tr√™n m·ªôt c·ª•m (Cluster) c√°c m√°y t√≠nh.

### Ki·∫øn tr√∫c ph√¢n t√°n
H·ªá th·ªëng bao g·ªìm hai lo·∫°i node ch√≠nh:
1.  **Master Node (N√∫t ch√≠nh):** Qu·∫£n l√Ω v√† ƒëi·ªÅu ph·ªëi c√¥ng vi·ªác.
2.  **Slave Node (N√∫t ph·ª•):** Th·ª±c thi c√°c t√°c v·ª• ƒë∆∞·ª£c giao.

### Vai tr√≤ c·ªßa c√°c th√†nh ph·∫ßn (Job Tracker & Task Tracker)
*(D·ª±a tr√™n ki·∫øn tr√∫c Hadoop MapReduce 1.x)*

| Th√†nh ph·∫ßn | Vai tr√≤ | V·ªã tr√≠ |
| :--- | :--- | :--- |
| **JobTracker** | - **Qu·∫£n l√Ω t√†i nguy√™n:** Nh·∫≠n c√°c y√™u c·∫ßu job t·ª´ client.<br>- **L·∫≠p l·ªãch:** Ph√¢n b·ªï c√°c t√°c v·ª• (Map/Reduce) cho TaskTracker.<br>- **Theo d√µi ti·∫øn ƒë·ªô:** Gi√°m s√°t tr·∫°ng th√°i c·ªßa c√°c t√°c v·ª•, x·ª≠ l√Ω l·ªói n·∫øu TaskTracker b·ªã ch·∫øt. | Master Node |
| **TaskTracker** | - **Th·ª±c thi:** Ch·∫°y c√°c t√°c v·ª• (Map Task ho·∫∑c Reduce Task) ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.<br>- **B√°o c√°o:** G·ª≠i t√≠n hi·ªáu "heartbeat" ƒë·ªãnh k·ª≥ ƒë·∫øn JobTracker ƒë·ªÉ th√¥ng b√°o tr·∫°ng th√°i v√† kh·∫£ nƒÉng ho·∫°t ƒë·ªông. | Slave Node |

### Lu·ªìng ho·∫°t ƒë·ªông trong m√¥i tr∆∞·ªùng ph√¢n t√°n:
1.  **Client** g·ª≠i job (bao g·ªìm code Map, Reduce v√† d·ªØ li·ªáu input) ƒë·∫øn **JobTracker**.
2.  **JobTracker** chia d·ªØ li·ªáu input th√†nh c√°c **Input Splits** (v√≠ d·ª•: 128MB m·ªói split).
3.  **JobTracker** ch·ªçn c√°c **TaskTracker** c√≥ dung l∆∞·ª£ng tr·ªëng v√† ƒë·∫∑t c√°c **Map Task** v√†o ƒë√≥.
4.  Sau khi Map Task ho√†n th√†nh, d·ªØ li·ªáu trung gian ƒë∆∞·ª£c l∆∞u t·∫°m th·ªùi v√† **JobTracker** s·∫Ω ch·ªâ ƒë·ªãnh c√°c **Reduce Task** tr√™n c√°c TaskTracker kh√°c ƒë·ªÉ x·ª≠ l√Ω.
5.  K·∫øt qu·∫£ ƒë∆∞·ª£c ghi v√†o h·ªá th·ªëng file ph√¢n t√°n (HDFS).

---

## 4. Ph√¢n t√≠ch & H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng (Use Cases & Best Practices)

### Khi n√†o s·ª≠ d·ª•ng MapReduce? (Use Cases)
*   **X·ª≠ l√Ω L√¥ (Batch Processing):** Khi b·∫°n kh√¥ng c·∫ßn k·∫øt qu·∫£ t·ª©c th√¨ (Real-time) m√† c√≥ th·ªÉ ch·ªù v√†i gi·ªù ho·∫∑c v√†i ng√†y.
*   **Ph√¢n t√≠ch Log Server:** ƒê·∫øm s·ªë l·∫ßn l·ªói (Error code) xu·∫•t hi·ªán trong h√†ng tri·ªáu d√≤ng log.
*   **Tr√≠ch xu·∫•t d·ªØ li·ªáu th√¥:** L·ªçc d·ªØ li·ªáu t·ª´ c√°c file CSV/Text kh·ªïng l·ªì ƒë·ªÉ chuy·ªÉn ƒë·ªïi sang ƒë·ªãnh d·∫°ng kh√°c.
*   **X√¢y d·ª±ng ch·ªâ m·ª•c t√¨m ki·∫øm:** X·ª≠ l√Ω t√†i li·ªáu ƒë·ªÉ t·∫°o t·ª´ ƒëi·ªÉn (Inverted Index).

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o? (How to use)
1.  **Chu·∫©n b·ªã d·ªØ li·ªáu:** ƒê∆∞a d·ªØ li·ªáu l√™n HDFS (Hadoop Distributed File System).
2.  **Vi·∫øt Logic:** X√°c ƒë·ªãnh h√†m `map()` (t√°ch bi·ªát d·ªØ li·ªáu) v√† `reduce()` (t·ªïng h·ª£p).
3.  **C·∫•u h√¨nh:** Thi·∫øt l·∫≠p c√°c tham s·ªë nh∆∞ s·ªë l∆∞·ª£ng Reduce task, b·ªô nh·ªõ (Memory).
4.  **Tri·ªÉn khai:** Submit Job l√™n Cluster th√¥ng qua Command Line Interface (CLI) ho·∫∑c API.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| ∆Øu ƒëi·ªÉm (Pros) | Nh∆∞·ª£c ƒëi·ªÉm (Cons) |
| :--- | :--- |
| **T√≠nh m·ªü r·ªông (Scalability):** X·ª≠ l√Ω petabyte d·ªØ li·ªáu d·ªÖ d√†ng b·∫±ng c√°ch th√™m node. | **ƒê·ªô tr·ªÖ (Latency):** R·∫•t ch·∫≠m ƒë·ªëi v·ªõi c√°c t√°c v·ª• nh·ªè do overhead (kh·ªüi t·∫°o JVM, ghi ƒëƒ©a). |
| **Tolerance (Ch·ªãu l·ªói):** N·∫øu m·ªôt node ch·∫øt, JobTracker s·∫Ω giao t√°c v·ª• cho node kh√°c. | **Kh√¥ng ph√π h·ª£p cho Real-time:** Ph√π h·ª£p cho Batch, kh√¥ng d√πng cho Streaming data. |
| **ƒê∆°n gi·∫£n h√≥a:** Che gi·∫•u s·ª± ph·ª©c t·∫°p c·ªßa vi·ªác song song h√≥a v√† m·∫°ng l∆∞·ªõi. | **T·ªën t√†i nguy√™n:** Kh·ªüi ƒë·ªông m·ªôt Job m·ªõi t·ªën nhi·ªÅu th·ªùi gian v√† CPU. |

---

## 5. V√≠ d·ª• Th·ª±c t·∫ø trong Industry

D∆∞·ªõi ƒë√¢y l√† c√°c tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng MapReduce th·ª±c t·∫ø c·ªßa c√°c c√¥ng ty l·ªõn:

1.  **Yahoo!:**
    *   **B√†i to√°n:** X√¢y d·ª±ng ch·ªâ m·ª•c cho c√¥ng c·ª• t√¨m ki·∫øm.
    *   **C√°ch l√†m:** H·ªç d√πng MapReduce ƒë·ªÉ qu√©t h√†ng t·ª∑ trang web, tr√≠ch xu·∫•t t·ª´ kh√≥a (Map), v√† gom nh√≥m URL ch·ª©a t·ª´ kh√≥a ƒë√≥ (Reduce) ƒë·ªÉ t·∫°o ra ch·ªâ m·ª•c t√¨m ki·∫øm kh·ªïng l·ªì.

2.  **Facebook (Log Analysis):**
    *   **B√†i to√°n:** Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng v√† th·ªëng k√™.
    *   **C√°ch l√†m:** M·ªói ng√†y, Facebook x·ª≠ l√Ω h√†ng petabyte d·ªØ li·ªáu log (ai ƒë√£ click v√†o ƒë√¢u, ai ƒë√£ like c√°i g√¨). MapReduce ƒë∆∞·ª£c d√πng ƒë·ªÉ ch·∫°y c√°c job t·ªïng h·ª£p th·ªëng k√™ h√†ng ng√†y (v√≠ d·ª•: "Top 10 b√†i vi·∫øt hot nh·∫•t ng√†y h√¥m qua").

3.  **LinkedIn:**
    *   **B√†i to√°n:** "People You May Know" (Nh·ªØng ng∆∞·ªùi b·∫°n c√≥ th·ªÉ bi·∫øt).
    *   **C√°ch l√†m:** S·ª≠ d·ª•ng MapReduce ƒë·ªÉ ph√¢n t√≠ch ƒë·ªì th·ªã x√£ h·ªôi (Social Graph). MapReduce t√≠nh to√°n s·ªë l∆∞·ª£ng k·∫øt n·ªëi chung gi·ªØa ng∆∞·ªùi d√πng A v√† ng∆∞·ªùi d√πng B ƒë·ªÉ ƒë·ªÅ xu·∫•t k·∫øt n·ªëi.

---

## T√≥m t·∫Øt

B√†i to√°n **Word Count** kh√¥ng ch·ªâ l√† v√≠ d·ª• ƒë∆°n gi·∫£n m√† n√≥ ch·ª©a ƒë·ª±ng to√†n b·ªô b·∫£n ch·∫•t c·ªßa **MapReduce**: vi·ªác chia nh·ªè (Map), truy·ªÅn d·ªØ li·ªáu qua m·∫°ng (Shuffle), v√† gom nh√≥m x·ª≠ l√Ω (Reduce). Hi·ªÉu r√µ lu·ªìng x·ª≠ l√Ω n√†y v√† vai tr√≤ c·ªßa **JobTracker** v√† **TaskTracker** l√† b∆∞·ªõc n·ªÅn t·∫£ng quan tr·ªçng ƒë·ªÉ l√†m vi·ªác v·ªõi c√°c h·ªá th·ªëng Big Data ph√¢n t√°n.

---

Ch√†o b·∫°n, v·ªõi vai tr√≤ l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n, t√¥i s·∫Ω ph√¢n t√≠ch v√† tr√¨nh b√†y l·∫°i n·ªôi dung t·ª´ c√°c slide v·ªÅ MapReduce m·ªôt c√°ch chi ti·∫øt, chuy√™n nghi·ªáp v√† d·ªÖ hi·ªÉu theo y√™u c·∫ßu c·ªßa b·∫°n.

---

# üìö Ph√¢n T√≠ch & H∆∞·ªõng D·∫´n Chi Ti·∫øt V·ªÅ MapReduce Algorithms

T√†i li·ªáu n√†y tr√¨nh b√†y c√°c thu·∫≠t to√°n c∆° b·∫£n v√† ti√™n ti·∫øn c√≥ th·ªÉ th·ª±c thi tr√™n khung kh·ªï **MapReduce**. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt cho t·ª´ng ph·∫ßn.

---

## 1. T·ªïng Quan V·ªÅ C√°c Thu·∫≠t To√°n MapReduce

**MapReduce** l√† m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh song song ph√¢n t√°n (distributed programming model) ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì (Big Data) tr√™n c√°c c·ª•m m√°y t√≠nh (clusters). Slide n√†y li·ªát k√™ c√°c thu·∫≠t to√°n ph·ªï bi·∫øn c√≥ th·ªÉ tri·ªÉn khai b·∫±ng MapReduce:

*   **Sorting (S·∫Øp x·∫øp):** S·∫Øp x·∫øp d·ªØ li·ªáu l·ªõn.
*   **Searching (T√¨m ki·∫øm):** T√¨m ki·∫øm th√¥ng tin trong t·∫≠p d·ªØ li·ªáu.
*   **TF-IDF (Term Frequency-Inverse Document Frequency):** Thu·∫≠t to√°n x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) ƒë·ªÉ ƒë√°nh gi√° t·∫ßm quan tr·ªçng c·ªßa m·ªôt t·ª´ trong m·ªôt t√†i li·ªáu so v·ªõi m·ªôt t·∫≠p h·ª£p c√°c t√†i li·ªáu.
*   **BFS (Breadth-First Search):** T√¨m ki·∫øm theo chi·ªÅu r·ªông, th∆∞·ªùng d√πng cho c√°c b√†i to√°n ƒë·ªì th·ªã.
*   **PageRank:** Thu·∫≠t to√°n c·ªßa Google ƒë·ªÉ x·∫øp h·∫°ng t·∫ßm quan tr·ªçng c·ªßa c√°c trang web (nodes) trong ƒë·ªì th·ªã.
*   **C√°c thu·∫≠t to√°n n√¢ng cao kh√°c.**

---

## 2. Thu·∫≠t To√°n S·∫Øp X·∫øp (Sort Algorithm)

Thu·∫≠t to√°n s·∫Øp x·∫øp trong MapReduce ƒë∆∞·ª£c xem nh∆∞ m·ªôt b√†i test "ƒë·ªô m·∫°nh" (raw speed) c·ªßa h·ªá th·ªëng Hadoop, th∆∞·ªùng ƒë∆∞·ª£c v√≠ von l√† m·ªôt cu·ªôc "ƒëua t·ªëc ƒë·ªô IO" (**IO drag race**).

### A. Gi·∫£i th√≠ch Kh√°i ni·ªám

*   **Input:** M·ªôt t·∫≠p h·ª£p c√°c file, m·ªói d√≤ng ch·ª©a m·ªôt gi√° tr·ªã (value).
    *   *Mapper Key:* T√™n file v√† s·ªë th·ª© t·ª± d√≤ng.
    *   *Mapper Value:* N·ªôi dung c·ªßa d√≤ng ƒë√≥.
*   **√ù t∆∞·ªüng c·ªët l√µi (Idea):**
    *   T·∫≠n d·ª•ng ƒë·∫∑c t√≠nh c·ªßa **Reducer**: C√°c c·∫∑p (key, value) ƒë∆∞·ª£c x·ª≠ l√Ω theo th·ª© t·ª± tƒÉng d·∫ßn c·ªßa `key`.
    *   **Mapper:** Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ƒë·∫ßu v√†o th√†nh c·∫∑p `(value, _)`. M·ª•c ƒë√≠ch l√† ƒë∆∞a d·ªØ li·ªáu c·∫ßn s·∫Øp x·∫øp l√™n l√†m `key`.
    *   **Reducer:** Ch·ªâ ƒë∆°n gi·∫£n l√† in ra `key` (v√¨ `key` ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp) v√† m·ªôt gi√° tr·ªã r·ªóng ho·∫∑c gi√° tr·ªã ban ƒë·∫ßu.

### B. M√£ gi·∫£ (Pseudo-code) & Code M·∫´u

D·ª±a tr√™n √Ω t∆∞·ªüng "Identity function" (h√†m nh·∫≠n d·∫°ng), ta c√≥ th·ªÉ vi·∫øt l·∫°i logic nh∆∞ sau:

#### 1. Mapper Phase
M·ª•c ti√™u: L·∫•y gi√° tr·ªã t·ª´ d√≤ng input v√† d√πng n√≥ l√†m Key.

```python
# Python Pseudo-code cho Mapper
def mapper(key, value):
    # key: filename, line_number
    # value: content of the line (e.g., "Apple", "Banana", "100")
    
    # Emit (value, _) ƒë·ªÉ gi√° tr·ªã tr·ªü th√†nh key m·ªõi
    # D·∫•u '_' bi·ªÉu th·ªã gi√° tr·ªã ph·ª• (kh√¥ng quan tr·ªçng cho vi·ªác sort)
    emit(value, "_")
```

#### 2. Reducer Phase
M·ª•c ti√™u: Nh·∫≠n d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c MapReduce Framework t·ª± ƒë·ªông s·∫Øp x·∫øp theo Key v√† in ra.

```python
# Python Pseudo-code cho Reducer
def reducer(key, values):
    # key: gi√° tr·ªã ƒë√£ ƒë∆∞·ª£c sort (v√≠ d·ª•: "100", "Apple", "Banana")
    # values: iterable c√°c gi√° tr·ªã ph·ª• (v√≠ d·ª•: ["_", "_", "_"])
    
    # Ch·ªâ c·∫ßn emit key l√† ƒë·ªß, v√¨ key ƒë√£ c√≥ th·ª© t·ª±
    emit(key, "")
```

### C. Ph√¢n t√≠ch c∆° ch·∫ø Shuffle & Partition

ƒê√¢y l√† ph·∫ßn quan tr·ªçng nh·∫•t ƒë·ªÉ thu·∫≠t to√°n Sort ho·∫°t ƒë·ªông ƒë√∫ng.

*   **Partition:** D·ªØ li·ªáu t·ª´ Mapper ƒë∆∞·ª£c chia nh·ªè v√† g·ª≠i ƒë·∫øn c√°c Reducer kh√°c nhau.
*   **Hash Function:** C·∫ßn ch·ªçn h√†m bƒÉm (hash function) sao cho:
    $$k_1 < k_2 \Rightarrow \text{hash}(k_1) < \text{hash}(k_2)$$
    *   *Gi·∫£i th√≠ch:* N·∫øu d·ªØ li·ªáu ƒë∆∞·ª£c chia ƒë·ªÅu nh∆∞ng kh√¥ng ƒë√∫ng th·ª© t·ª± (v√≠ d·ª•: Reducer 1 nh·∫≠n d·ªØ li·ªáu t·ª´ 'A' ƒë·∫øn 'M', Reducer 2 nh·∫≠n t·ª´ 'N' ƒë·∫øn 'Z'), nh∆∞ng Reducer 1 ch·∫°y xong tr∆∞·ªõc Reducer 2, k·∫øt qu·∫£ ƒë·∫ßu ra s·∫Ω b·ªã sai th·ª© t·ª± to√†n c·ª•c. Do ƒë√≥, h√†m bƒÉm ph·∫£i ƒë·∫£m b·∫£o t√≠nh **to√†n c·ª•c (total order)**.

### D. ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| Ti√™u ch√≠ | Chi ti·∫øt |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | ‚Ä¢ Kh·∫£ nƒÉng m·ªü r·ªông (Scalability) c·ª±c t·ªët, s·∫Øp x·∫øp h√†ng Petabyte d·ªØ li·ªáu.<br>‚Ä¢ T·ª± ƒë·ªông c√¢n b·∫±ng t·∫£i (Load balancing) gi·ªØa c√°c Reducer. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | ‚Ä¢ T·ªën nhi·ªÅu t√†i nguy√™n IO (ƒê·ªçc/Ghi) v√† Network (Shuffle).<br>‚Ä¢ ƒê·ªô tr·ªÖ (Latency) cao so v·ªõi c√°c thu·∫≠t to√°n s·∫Øp x·∫øp trong b·ªô nh·ªõ (In-memory sort). |

---

## 3. Thu·∫≠t To√°n T√¨m Ki·∫øm (Search Algorithm)

M·∫∑c d√π slide kh√¥ng ƒëi s√¢u v√†o code c·ª• th·ªÉ cho Search, nh∆∞ng d·ª±a tr√™n c·∫•u tr√∫c MapReduce, ch√∫ng ta c√≥ th·ªÉ hi·ªÉu thu·∫≠t to√°n n√†y nh∆∞ sau:

### A. Gi·∫£i th√≠ch Kh√°i ni·ªám
*   **Input:** C√°c file vƒÉn b·∫£n.
*   **Mapper:** ƒê·ªçc t·ª´ng d√≤ng, ki·ªÉm tra xem d√≤ng ƒë√≥ c√≥ ch·ª©a t·ª´ kh√≥a (keyword) c·∫ßn t√¨m hay kh√¥ng.
*   **Reducer:** Thu th·∫≠p c√°c k·∫øt qu·∫£ v√† tr·∫£ v·ªÅ danh s√°ch c√°c file/d√≤ng ch·ª©a t·ª´ kh√≥a.

### B. Code M·∫´u (Minh h·ªça)

```python
# Mapper: T√¨m t·ª´ kh√≥a "Hadoop"
def mapper(key, value):
    # value l√† n·ªôi dung d√≤ng vƒÉn b·∫£n
    if "Hadoop" in value:
        # Emit (key, value) ho·∫∑c ch·ªâ emit (key, 1) ƒë·ªÉ ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán
        emit("Found", value)

# Reducer: ƒê∆°n gi·∫£n l√† gom k·∫øt qu·∫£
def reducer(key, values):
    for v in values:
        emit(v, "Matched")
```

### C. S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi c·∫ßn t√¨m ki·∫øm song song tr√™n h√†ng tri·ªáu file vƒÉn b·∫£n m√† kh√¥ng c·∫ßn c∆° s·ªü d·ªØ li·ªáu quan h·ªá.
*   **∆Øu ƒëi·ªÉm:** Ph√¢n t√≠ch ƒë·ªìng th·ªùi (Parallel processing) gi√∫p t·ªëc ƒë·ªô t√¨m ki·∫øm nhanh h∆°n ƒë√°ng k·ªÉ so v·ªõi t√¨m ki·∫øm tu·∫ßn t·ª±.

---

## 4. V√≠ d·ª• Th·ª±c T·∫ø & B√†i T·∫≠p Th·ª±c Ti·ªÖn

### V√≠ d·ª• 1: Lƒ©nh v·ª±c Ph√¢n t√≠ch T√†i ch√≠nh (Financial Analysis)
*   **B√†i to√°n:** S·∫Øp x·∫øp h√†ng t·ª∑ giao d·ªãch ch·ª©ng kho√°n theo m√£ ticker (v√≠ d·ª•: AAPL, GOOG, MSFT) ƒë·ªÉ ph√¢n t√≠ch xu h∆∞·ªõng.
*   **√Åp d·ª•ng MapReduce:**
    *   **Mapper:** ƒê·ªçc log giao d·ªãch, emit `(Ticker, Price)`.
    *   **Shuffle & Sort:** Framework t·ª± ƒë·ªông gom c√°c giao d·ªãch c·ªßa c√πng m·ªôt m√£ ticker l·∫°i v·ªõi nhau.
    *   **Reducer:** T√≠nh to√°n gi√° trung b√¨nh, t·ªïng kh·ªëi l∆∞·ª£ng giao d·ªãch cho t·ª´ng m√£.

### V√≠ d·ª• 2: Lƒ©nh v·ª±c Y t·∫ø (Healthcare)
*   **B√†i to√°n:** T√¨m ki·∫øm b·ªánh nh√¢n c√≥ tri·ªáu ch·ª©ng c·ª• th·ªÉ trong h√†ng tri·ªáu h·ªì s∆° b·ªánh √°n.
*   **√Åp d·ª•ng MapReduce:**
    *   **Mapper:** Qu√©t t·ª´ng h·ªì s∆°, emit `(PatientID, Symptom)` n·∫øu tri·ªáu ch·ª©ng kh·ªõp v·ªõi t·ª´ kh√≥a t√¨m ki·∫øm.
    *   **Reducer:** Li·ªát k√™ danh s√°ch `PatientID`.

---

## 5. T√≥m t·∫Øt & K·∫øt lu·∫≠n

*   **MapReduce** l√† n·ªÅn t·∫£ng m·∫°nh m·∫Ω ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu th√¥ (Raw Data).
*   **Sort Algorithm** l√† b√†i test quan tr·ªçng, minh ch·ª©ng cho kh·∫£ nƒÉng x·ª≠ l√Ω IO v√† th·ª© t·ª± d·ªØ li·ªáu c·ªßa h·ªá th·ªëng.
*   **Shuffle & Partition** l√† c√°c b∆∞·ªõc quy·∫øt ƒë·ªãnh s·ª± th√†nh b·∫°i c·ªßa thu·∫≠t to√°n, ƒë·∫∑c bi·ªát l√† y√™u c·∫ßu v·ªÅ h√†m bƒÉm (Hash function) ƒë·ªÉ gi·ªØ ƒë√∫ng th·ª© t·ª±.
*   ƒê·ªÉ s·ª≠ d·ª•ng hi·ªáu qu·∫£, c·∫ßn hi·ªÉu r√µ **Trade-off**: T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô x·ª≠ l√Ω song song nh∆∞ng ph·∫£i ƒë√°nh ƒë·ªïi b·∫±ng bƒÉng th√¥ng m·∫°ng v√† IO disk.

---

Ch√†o b·∫°n, v·ªõi vai tr√≤ l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n, t√¥i s·∫Ω ph√¢n t√≠ch n·ªôi dung slide v·ªÅ MapReduce, c·∫£i thi·ªán c√°c ƒëo·∫°n m√£ gi·∫£ v√† tr√¨nh b√†y chi ti·∫øt theo y√™u c·∫ßu c·ªßa b·∫°n.

D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu ƒë∆∞·ª£c h·ªá th·ªëng l·∫°i d∆∞·ªõi d·∫°ng Markdown chuy√™n nghi·ªáp.

---

# Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n th·ª±c h√†nh MapReduce: T·ª´ Search ƒë·∫øn TF-IDF

T√†i li·ªáu n√†y ph√¢n t√≠ch c√°c thu·∫≠t to√°n MapReduce c∆° b·∫£n bao g·ªìm **Search Algorithm** (Thu·∫≠t to√°n T√¨m ki·∫øm) v√† **TF-IDF** (T·∫ßn su·∫•t Thu·∫≠t ng·ªØ - T·∫ßn su·∫•t Ng∆∞·ª£c c·ªßa T√†i li·ªáu), d·ª±a tr√™n c√°c slide h·ªçc t·∫≠p.

---

## 1. Thu·∫≠t to√°n T√¨m ki·∫øm (Search Algorithm)

ƒê√¢y l√† v√≠ d·ª• kinh ƒëi·ªÉn v·ªÅ MapReduce ƒë·ªÉ t√¨m ki·∫øm m·ªôt pattern (m·∫´u) c·ª• th·ªÉ trong m·ªôt kho d·ªØ li·ªáu vƒÉn b·∫£n l·ªõn (v√≠ d·ª•: logs, t√†i li·ªáu).

### Kh√°i ni·ªám v√† Gi·∫£i th√≠ch

*   **Input (ƒê·∫ßu v√†o):**
    *   M·ªôt t·∫≠p h·ª£p c√°c file ch·ª©a vƒÉn b·∫£n.
    *   M·ªôt m·∫´u t√¨m ki·∫øm (search pattern).
*   **Mapper:**
    *   **Key:** T√™n file v√† s·ªë d√≤ng (`filename, line_number`).
    *   **Value:** N·ªôi dung c·ªßa d√≤ng ƒë√≥ (`line_content`).
    *   **Logic:** Mapper nh·∫≠n input, so s√°nh n·ªôi dung d√≤ng v·ªõi pattern. N·∫øu kh·ªõp, n√≥ output m·ªôt c·∫∑p `(filename, "_")`. D·∫•u g·∫°ch d∆∞·ªõi "_" ·ªü ƒë√¢y l√† m·ªôt gi√° tr·ªã ƒë√°nh d·∫•u ƒë∆°n gi·∫£n ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng.
*   **Reducer:**
    *   S·ª≠ d·ª•ng h√†m **Identity Function** (h√†m nh·∫≠n d·∫°ng). Nghƒ©a l√† Reducer kh√¥ng thay ƒë·ªïi gi√° tr·ªã ƒë·∫ßu v√†o, ch·ªâ ƒë∆°n gi·∫£n l√† chuy·ªÉn ti·∫øp (pass-through) k·∫øt qu·∫£ t·ª´ Mapper. N·∫øu m·ªôt file c√≥ nhi·ªÅu d√≤ng kh·ªõp pattern, Reducer s·∫Ω nh·∫≠n nhi·ªÅu c·∫∑p `(filename, "_")` v√† output ch√∫ng.

### M√£ gi·∫£ v√† Code M·∫´u

D∆∞·ªõi ƒë√¢y l√† m√£ gi·∫£ g·ªëc t·ª´ slide v√† m√£ Python minh h·ªça th·ª±c t·∫ø s·ª≠ d·ª•ng th∆∞ vi·ªán `mrjob` (m·ªôt framework MapReduce ph·ªï bi·∫øn).

#### A. M√£ gi·∫£ g·ªëc (Pseudo-code)

```python
# Mapper
def mapper(filename, line_number, line_content, pattern):
    if pattern in line_content:
        emit(filename, "_")

# Reducer (Identity)
def reducer(filename, values):
    for value in values:
        emit(filename, value)
```

#### B. Code M·∫´u th·ª±c t·∫ø (Python - Hadoop Streaming)

```python
#!/usr/bin/env python3
import sys

def main():
    # Pattern c·∫ßn t√¨m (l·∫•y t·ª´ tham s·ªë d√≤ng l·ªánh ho·∫∑c file c·∫•u h√¨nh)
    search_pattern = "error" # V√≠ d·ª•: t√¨m l·ªói "error"
    
    # Mapper
    for line in sys.stdin:
        # Gi·∫£ s·ª≠ input l√†: filename:line_content
        # Trong th·ª±c t·∫ø Hadoop, key l√† offset, value l√† line
        try:
            # ƒê·ªçc d·ªØ li·ªáu (filename, content)
            filename, content = line.strip().split(':', 1)
            
            if search_pattern in content:
                # Output: (filename, 1) - D√πng 1 ƒë·ªÉ ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán
                print(f"{filename}\t1")
        except ValueError:
            continue

if __name__ == "__main__":
    main()
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng & Ph√¢n t√≠ch

| Ti√™u ch√≠ | N·ªôi dung chi ti·∫øt |
| :--- | :--- |
| **Khi n√†o s·ª≠ d·ª•ng?** | Khi c·∫ßn t√¨m ki·∫øm m·ªôt t·ª´ kh√≥a ho·∫∑c pattern trong h√†ng tri·ªáu/t·ª∑ file vƒÉn b·∫£n (Log analysis, Search Engine, Data Mining). |
| **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** | 1. Map: ƒê·ªçc t·ª´ng d√≤ng, ki·ªÉm tra pattern.<br>2. Reduce: Gom nh√≥m theo filename. |
| **∆Øu ƒëi·ªÉm** | *T√≠nh m·ªü r·ªông (Scalability):* X·ª≠ l√Ω song song tr√™n h√†ng ngh√¨n node.<br>*ƒê∆°n gi·∫£n:* Logic d·ªÖ hi·ªÉu, d·ªÖ implement. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | *Network I/O:* N·∫øu pattern xu·∫•t hi·ªán r·∫•t nhi·ªÅu, Mapper s·∫Ω emit r·∫•t nhi·ªÅu c·∫∑p `(filename, "_")` g√¢y t·∫Øc ngh·∫Ωn m·∫°ng. |

### Optimization (T·ªëi ∆∞u h√≥a)

Slide ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác t·ªëi ∆∞u b·∫±ng **Combiner**.

*   **V·∫•n ƒë·ªÅ:** N·∫øu m·ªôt file c√≥ 1000 d√≤ng ch·ª©a "error", Mapper s·∫Ω emit 1000 c·∫∑p `(filename, "_")`. Reducer nh·∫≠n 1000 d√≤ng n√†y ch·ªâ ƒë·ªÉ output 1 d√≤ng.
*   **Gi·∫£i ph√°p (Combiner):** Combiner ch·∫°y t·∫°i node Mapper (local) ƒë·ªÉ gom c√°c b·∫£n ghi tr√πng l·∫∑p tr∆∞·ªõc khi g·ª≠i l√™n Reducer.
*   **Code t·ªëi ∆∞u:**
    ```python
    # Combiner (t·∫°i node Mapper)
    def combiner(filename, values):
        # Ch·ªâ emit m·ªôt l·∫ßn cho d√π c√≥ bao nhi√™u d√≤ng kh·ªõp
        emit(filename, "_")
    ```

---

## 2. Thu·∫≠t to√°n TF-IDF (Term Frequency - Inverse Document Frequency)

TF-IDF l√† thu·∫≠t to√°n th·ªëng k√™ d√πng ƒë·ªÉ ƒë√°nh gi√° t·∫ßm quan tr·ªçng c·ªßa m·ªôt t·ª´ ƒë·ªëi v·ªõi m·ªôt t√†i li·ªáu trong m·ªôt t·∫≠p h·ª£p ho·∫∑c m·ªôt kho l∆∞u tr·ªØ ng√¥n ng·ªØ.

### Kh√°i ni·ªám v√† Gi·∫£i th√≠ch

*   **Term Frequency (TF):** T·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa m·ªôt t·ª´ trong m·ªôt t√†i li·ªáu. T·ª´ xu·∫•t hi·ªán nhi·ªÅu l·∫ßn trong t√†i li·ªáu A th√¨ TF cao.
*   **Inverse Document Frequency (IDF):** T·∫ßn su·∫•t ng∆∞·ª£c c·ªßa t√†i li·ªáu. ƒêo l∆∞·ªùng m·ª©c ƒë·ªô hi·∫øm c·ªßa t·ª´ ƒë√≥ trong to√†n b·ªô kho d·ªØ li·ªáu (corpus).
    *   C√¥ng th·ª©c: $IDF(t) = \log(\frac{Total\ Documents}{Number\ of\ documents\ containing\ t})$
*   **Observation (Th√¥ng tin c·∫ßn thu th·∫≠p):**
    *   S·ªë l·∫ßn term $X$ xu·∫•t hi·ªán trong t√†i li·ªáu c·ª• th·ªÉ (t√≠nh TF).
    *   T·ªïng s·ªë t·ª´ trong m·ªói t√†i li·ªáu (ƒë·ªÉ chu·∫©n h√≥a TF).
    *   S·ªë t√†i li·ªáu ch·ª©a term $X$ (t√≠nh IDF).

### M√£ gi·∫£ v√† Code M·∫´u

Thu·∫≠t to√°n n√†y th∆∞·ªùng chia l√†m 3 giai ƒëo·∫°n MapReduce ho·∫∑c s·ª≠ d·ª•ng c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω.

#### A. M√£ gi·∫£ g·ªëc (Pseudo-code)

```python
# Giai ƒëo·∫°n 1: T√≠nh TF (Term Frequency)
# Input: (doc_id, text)
def mapper_tf(doc_id, text):
    for word in text.split():
        emit((doc_id, word), 1)

def reducer_tf(doc_id_word, counts):
    total_tf = sum(counts)
    emit(doc_id_word, total_tf)

# Giai ƒëo·∫°n 2: T√≠nh IDF (Inverse Document Frequency)
# Input: (doc_id, text)
def mapper_idf(doc_id, text):
    unique_words = set(text.split())
    for word in unique_words:
        emit(word, doc_id) # G·ª≠i t·ª´ v√† doc_id ch·ª©a n√≥

def reducer_idf(word, doc_ids):
    # doc_ids l√† m·ªôt iterable ch·ª©a c√°c doc_id kh√°c nhau
    doc_count = count_unique(doc_ids)
    emit(word, log(TOTAL_DOCS / doc_count))

# Giai ƒëo·∫°n 3: K·∫øt h·ª£p TF * IDF (th∆∞·ªùng l√†m ngo√†i MapReduce ho·∫∑c Join)
```

#### B. Code M·∫´u th·ª±c t·∫ø (Python)

```python
import math
from collections import defaultdict

# D·ªØ li·ªáu m·∫´u
documents = {
    "doc1": "big data hadoop spark",
    "doc2": "big data analysis",
    "doc3": "machine learning"
}

def calculate_tf_idf(docs):
    # 1. T√≠nh TF v√† thu th·∫≠p th√¥ng tin cho IDF
    tf_counts = defaultdict(lambda: defaultdict(int))
    doc_freq = defaultdict(int) # S·ªë t√†i li·ªáu ch·ª©a t·ª´
    total_docs = len(docs)
    
    for doc_id, text in docs.items():
        words = text.split()
        # ƒê·∫øm t·∫ßn su·∫•t trong doc
        for word in words:
            tf_counts[doc_id][word] += 1
        
        # ƒê·∫øm Document Frequency (ch·ªâ ƒë·∫øm 1 l·∫ßn m·ªói t·ª´ trong 1 doc)
        unique_words = set(words)
        for word in unique_words:
            doc_freq[word] += 1

    # 2. T√≠nh to√°n TF-IDF
    tfidf_scores = defaultdict(lambda: defaultdict(float))
    
    for doc_id, words in tf_counts.items():
        total_words_in_doc = sum(words.values())
        for word, count in words.items():
            # TF: (S·ªë l·∫ßn xu·∫•t hi·ªán / T·ªïng s·ªë t·ª´ trong doc)
            tf = count / total_words_in_doc
            
            # IDF: log(T·ªïng s·ªë doc / S·ªë doc ch·ª©a t·ª´)
            idf = math.log(total_docs / doc_freq[word])
            
            tfidf_scores[doc_id][word] = tf * idf
            
    return tfidf_scores

# Ch·∫°y v√† In k·∫øt qu·∫£
scores = calculate_tf_idf(documents)
for doc, terms in scores.items():
    print(f"Document: {doc}")
    for term, score in terms.items():
        print(f"  {term}: {score:.4f}")
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng & Ph√¢n t√≠ch

| Ti√™u ch√≠ | N·ªôi dung chi ti·∫øt |
| :--- | :--- |
| **Khi n√†o s·ª≠ d·ª•ng?** | *Search Engines:* X·∫øp h·∫°ng t√†i li·ªáu quan tr·ªçng nh·∫•t cho truy v·∫•n.<br>*Text Mining/Clustering:* Ph√¢n lo·∫°i t√†i li·ªáu, t√≥m t·∫Øt vƒÉn b·∫£n.<br>*Recommendation System:* T√¨m similarities gi·ªØa c√°c b√†i vi·∫øt/tin t·ª©c. |
| **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** | 1. **Map Phase 1:** ƒê·ªçc t√†i li·ªáu, emit `(word, doc_id)`.<br>2. **Reduce Phase 1:** T√≠nh IDF cho m·ªói t·ª´ (s·ªë l∆∞·ª£ng doc_id nh·∫≠n ƒë∆∞·ª£c).<br>3. **Map Phase 2:** T√≠nh TF cho m·ªói t·ª´ trong t·ª´ng doc.<br>4. **Join:** K·∫øt h·ª£p TF v√† IDF ƒë·ªÉ ra ƒëi·ªÉm s·ªë cu·ªëi c√πng. |
| **∆Øu ƒëi·ªÉm** | *Ph·ªï bi·∫øn:* Chu·∫©n m·ª±c trong x·ª≠ l√Ω vƒÉn b·∫£n.<br>*C√≥ √Ω nghƒ©a th·ªëng k√™:* Gi·∫£m tr·ªçng s·ªë c·ªßa c√°c t·ª´ ph·ªï bi·∫øn (nh∆∞ "l√†", "c·ªßa") v√† tƒÉng tr·ªçng s·ªë t·ª´ hi·∫øm, quan tr·ªçng. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | *Chi ph√≠ t√≠nh to√°n cao:* C·∫ßn nhi·ªÅu giai ƒëo·∫°n MapReduce.<br>*Kh√¥ng n·∫Øm b·∫Øt ng·ªØ c·∫£nh:* Ch·ªâ quan t√¢m ƒë·∫øn t·∫ßn su·∫•t, kh√¥ng hi·ªÉu nghƒ©a (S·ª≠ d·ª•ng BERT/Transformer hi·ªán ƒë·∫°i h∆°n cho vi·ªác n√†y). |

### V√≠ d·ª• Th·ª±c t·∫ø trong Ng√†nh

*   **Google Search:** Khi b·∫°n g√µ "C√¥ng ngh·ªá Big Data", Google kh√¥ng ch·ªâ t√¨m c√°c trang c√≥ ch·ªØ "Big Data" m√† c√≤n t√≠nh to√°n xem trang n√†o c√≥ ch·ªâ s·ªë TF-IDF cao nh·∫•t (t·ª©c l√† trang ƒë√≥ n√≥i v·ªÅ Big Data m·ªôt c√°ch chuy√™n s√¢u, kh√¥ng ph·∫£i trang tin t·ª©c t·ªïng h·ª£p c√≥ nhi·ªÅu t·ª´ kh√°c).
*   **Ph√¢n t√≠ch c·∫£m x√∫c (Sentiment Analysis):** D√πng TF-IDF ƒë·ªÉ t√¨m c√°c t·ª´ kh√≥a quan tr·ªçng trong ƒë√°nh gi√° s·∫£n ph·∫©m (v√≠ d·ª•: t·ª´ "pin" xu·∫•t hi·ªán nhi·ªÅu trong c√°c ƒë√°nh gi√° ti√™u c·ª±c).

---

Ch√†o b·∫°n, v·ªõi t∆∞ c√°ch l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n, t√¥i s·∫Ω ph√¢n t√≠ch chi ti·∫øt n·ªôi dung slide v·ªÅ quy tr√¨nh t√≠nh to√°n **TF-IDF (Term Frequency - Inverse Document Frequency)** s·ª≠ d·ª•ng **MapReduce**.

D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu ƒë∆∞·ª£c tr√¨nh b√†y l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp, chi ti·∫øt v√† c√≥ k√®m theo m√£ ngu·ªìn minh h·ªça.

---

# TF-IDF Calculation using MapReduce

**TF-IDF** l√† m·ªôt k·ªπ thu·∫≠t th·ªëng k√™ ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô quan tr·ªçng c·ªßa m·ªôt t·ª´ (word) ƒë·ªëi v·ªõi m·ªôt t√†i li·ªáu (document) trong m·ªôt t·∫≠p h·ª£p c√°c t√†i li·ªáu (corpus). N√≥ ƒë∆∞·ª£c t√≠nh b·∫±ng t√≠ch c·ªßa hai ch·ªâ s·ªë:
*   **TF (Term Frequency):** T·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ trong t√†i li·ªáu.
*   **IDF (Inverse Document Frequency):** Logarithm c·ªßa t·ª∑ l·ªá ngh·ªãch gi·ªØa s·ªë l∆∞·ª£ng t√†i li·ªáu ch·ª©a t·ª´ ƒë√≥ v√† t·ªïng s·ªë t√†i li·ªáu.

Quy tr√¨nh n√†y ƒë∆∞·ª£c chia th√†nh **4 Jobs (C√¥ng vi·ªác)** ri√™ng bi·ªát tr√™n Hadoop MapReduce.

---

## Job 1: T√≠nh t·∫ßn su·∫•t t·ª´ trong t·ª´ng t√†i li·ªáu (TF)

M·ª•c ti√™u c·ªßa Job n√†y l√† ƒë·∫øm s·ªë l·∫ßn m·ªói t·ª´ xu·∫•t hi·ªán trong t·ª´ng t√†i li·ªáu c·ª• th·ªÉ.

### 1.1. Gi·∫£i th√≠ch Kh√°i ni·ªám
*   **Input:** D·∫°ng c·∫∑p `(t√™n_t√†i li·ªáu, n·ªôidung_t√†i li·ªáu)`.
*   **Mapper:** Th·ª±c hi·ªán tokenize (t√°ch t·ª´) n·ªôi dung v√† emit (ph√°t ra) c·∫∑p `(word, docname)` k√®m theo gi√° tr·ªã `1`.
*   **Reducer:** Nh·∫≠n d·ªØ li·ªáu t·ª´ Mapper, th·ª±c hi·ªán t·ªïng h·ª£p (sum) c√°c gi√° tr·ªã `1` ƒë·ªÉ t√≠nh t·∫ßn su·∫•t th·ª±c t·∫ø c·ªßa t·ª´ trong t√†i li·ªáu ƒë√≥.

### 1.2. M√£ ngu·ªìn minh h·ªça (Python)

```python
# Mapper cho Job 1
import sys

def mapper():
    for line in sys.stdin:
        # Gi·∫£ ƒë·ªãnh input: doc_name \t content
        doc_name, content = line.strip().split('\t', 1)
        
        # T√°ch t·ª´ (tokenize) - ƒë∆°n gi·∫£n h√≥a
        words = content.split()
        
        for word in words:
            # Output: ((word, docname), 1)
            print(f"({word},{doc_name})\t1")

# Reducer cho Job 1
from itertools import groupby

def reducer():
    for key, group in groupby(sys.stdin, key=lambda x: x.split('\t')[0]):
        # key: (word,docname)
        total_count = sum(int(val.split('\t')[1]) for val in group)
        # Output: ((word, docname), n)
        print(f"{key}\t{total_count}")

if __name__ == "__main__":
    # Trong m√¥i tr∆∞·ªùng Hadoop, mapper/reducer ƒë∆∞·ª£c g·ªçi ri√™ng
    # ·ªû ƒë√¢y ƒë·ªÉ minh h·ªça logic
    mapper() 
    # reducer()
```

### 1.3. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi c·∫ßn ƒë·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa c√°c c·∫∑p `(T·ª´, T√†i li·ªáu)`.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?** Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫ßu v√†o th√†nh `(docname, content)`. Mapper t√°ch t·ª´, Reducer c·ªông d·ªìn.
*   **Combiner:** Trong slide c√≥ ghi "Combiner is same as Reducer". ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† ta c√≥ th·ªÉ t·ªëi ∆∞u h√≥a b·∫±ng c√°ch d√πng Combiner ƒë·ªÉ gi·∫£m l∆∞·ª£ng d·ªØ li·ªáu truy·ªÅn gi·ªØa Mapper v√† Reducer b·∫±ng c√°ch c·ªông d·ªìn c·ª•c b·ªô t·∫°i Mapper tr∆∞·ªõc.
*   **∆Øu ƒëi·ªÉm:** Gi·∫£m t·∫£i cho network I/O.

---

## Job 2: T√≠nh t·∫ßn su·∫•t t√≠ch l≈©y v√† t·ªïng s·ªë t·ª´ trong t√†i li·ªáu

M·ª•c ti√™u c·ªßa Job n√†y l√† chu·∫©n b·ªã d·ªØ li·ªáu cho vi·ªác t√≠nh IDF. N√≥ c·∫ßn t·ªïng h·ª£p l·∫°i theo t√†i li·ªáu ƒë·ªÉ bi·∫øt t√†i li·ªáu ƒë√≥ c√≥ bao nhi√™u t·ª´ t·ªïng c·ªông (`N`).

### 2.1. Gi·∫£i th√≠ch Kh√°i ni·ªám
*   **Input:** K·∫øt qu·∫£ c·ªßa Job 1: `((word, docname), n)`.
*   **Mapper:** ƒê·ªïi key ƒë·ªÉ group theo t√†i li·ªáu: `(docname, (word, n))`.
*   **Reducer:** V·ªõi m·ªói t√†i li·ªáu, n√≥ c·∫ßn:
    1.  T√≠nh t·ªïng s·ªë t·ª´ trong t√†i li·ªáu (`N = sum(n)`).
    2.  Emit k·∫øt qu·∫£ cho t·ª´ng t·ª´ trong t√†i li·ªáu ƒë√≥, k√®m theo c·∫£ `n` (t·∫ßn su·∫•t t·ª´ ƒë√≥) v√† `N` (t·ªïng t·∫ßn su·∫•t t√†i li·ªáu).

### 2.2. M√£ ngu·ªìn minh h·ªça (Python)

```python
# Mapper cho Job 2
import sys

def mapper():
    for line in sys.stdin:
        # Input t·ª´ Job 1: ((word,docname)\t n)
        pair, count = line.strip().split('\t')
        # Parse pair: (word, docname)
        word, docname = pair.strip('()').split(',')
        
        # Output: (docname, (word, n))
        print(f"{docname}\t({word},{count})")

# Reducer cho Job 2
from itertools import groupby

def reducer():
    for docname, group in groupby(sys.stdin, key=lambda x: x.split('\t')[0]):
        doc_data = []
        # ƒê·ªçc d·ªØ li·ªáu t√†i li·ªáu
        for line in group:
            val = line.strip().split('\t')[1]
            word, n = val.strip('()').split(',')
            doc_data.append((word, int(n)))
        
        # T√≠nh N = sum(n) cho t√†i li·ªáu n√†y
        N = sum(n for _, n in doc_data)
        
        # Emit: ((word, docname), (n, N))
        for word, n in doc_data:
            print(f"({word},{docname})\t({n},{N})")

if __name__ == "__main__":
    mapper()
```

### 2.3. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi c·∫ßn chu·∫©n h√≥a d·ªØ li·ªáu theo ng·ªØ c·∫£nh (document-level normalization).
*   **∆Øu ƒëi·ªÉm:** Cho ph√©p t√≠nh to√°n c√°c ch·ªâ s·ªë ph·ª• thu·ªôc v√†o ng·ªØ c·∫£nh l·ªõn h∆°n (c·∫£ t√†i li·ªáu) m√† kh√¥ng c·∫ßn load to√†n b·ªô d·ªØ li·ªáu v√†o memory.

---

## Job 3: T√≠nh s·ªë t√†i li·ªáu ch·ª©a t·ª´ (DF)

M·ª•c ti√™u l√† t√¨m `d` (document frequency) - s·ªë l∆∞·ª£ng t√†i li·ªáu m√† t·ª´ ƒë√≥ xu·∫•t hi·ªán √≠t nh·∫•t m·ªôt l·∫ßn.

### 3.1. Gi·∫£i th√≠ch Kh√°i ni·ªám
*   **Input:** `((word, docname), (n, N))`.
*   **Mapper:** Group l·∫°i theo t·ª´ (word): `(word, (docname, n, N, 1))`. Gi√° tr·ªã `1` ·ªü cu·ªëi d√πng ƒë·ªÉ ƒë·∫øm s·ªë t√†i li·ªáu.
*   **Reducer:** V·ªõi m·ªói t·ª´ (word), n√≥ c·ªông d·ªìn c√°c gi√° tr·ªã `1` ƒë·ªÉ ra `d` (s·ªë t√†i li·ªáu ch·ª©a t·ª´ ƒë√≥).

### 3.2. M√£ ngu·ªìn minh h·ªça (Python)

```python
# Mapper cho Job 3
import sys

def mapper():
    for line in sys.stdin:
        # Input: ((word,docname)\t(n,N))
        pair, counts = line.strip().split('\t')
        word, docname = pair.strip('()').split(',')
        n, N = counts.strip('()').split(',')
        
        # Output: (word, (docname, n, N, 1))
        print(f"{word}\t({docname},{n},{N},1)")

# Reducer cho Job 3
from itertools import groupby

def reducer():
    for word, group in groupby(sys.stdin, key=lambda x: x.split('\t')[0]):
        d = 0 # Document frequency
        # L∆∞u l·∫°i d·ªØ li·ªáu ƒë·ªÉ emit ·ªü b∆∞·ªõc cu·ªëi
        buffer = []
        
        for line in group:
            val = line.strip().split('\t')[1]
            docname, n, N, one = val.strip('()').split(',')
            d += int(one)
            # L∆∞u tr·ªØ ƒë·ªÉ emit ra (word, docname) v√† c√°c gi√° tr·ªã c≈©
            buffer.append((docname, n, N))
        
        # Outputs: ((word, docname), (n, N, d))
        for docname, n, N in buffer:
            print(f"({word},{docname})\t({n},{N},{d})")

if __name__ == "__main__":
    mapper()
```

### 3.3. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi c·∫ßn t√≠nh IDF (Inverse Document Frequency).
*   **L∆∞u √Ω:** Slide ƒë·ªÅ c·∫≠p "Assume D is known" (D l√† t·ªïng s·ªë t√†i li·ªáu). Trong th·ª±c t·∫ø, D c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh b·∫±ng m·ªôt MapReduce job ƒë·∫øm d√≤ng ƒë∆°n gi·∫£n ho·∫∑c hardcode n·∫øu ƒë√£ bi·∫øt tr∆∞·ªõc.

---

## Job 4: T√≠nh to√°n TF-IDF

ƒê√¢y l√† job cu·ªëi c√πng, th·ª±c hi·ªán ph√©p nh√¢n ƒë·ªÉ ra k·∫øt qu·∫£ cu·ªëi c√πng.

### 4.1. Gi·∫£i th√≠ch Kh√°i ni·ªám
*   **Input:** `((word, docname), (n, N, d))`.
*   **Mapper (ho·∫∑c Reducer):** Th·ª±c hi·ªán c√¥ng th·ª©c:
    $$TF = \frac{n}{N}$$
    $$IDF = \log(\frac{D}{d})$$
    $$TF \times IDF = \frac{n}{N} \times \log(\frac{D}{d})$$
*   **Reducer:** Trong slide ghi "Just the identity function", c√≥ nghƒ©a l√† Mapper ƒë√£ x·ª≠ l√Ω xong h·∫øt, Reducer ch·ªâ vi·ªác pass-through (in l·∫°i k·∫øt qu·∫£) ho·∫∑c Mapper/Reducer c√≥ th·ªÉ g·ªôp l√†m m·ªôt n·∫øu logic cho ph√©p.

### 4.2. M√£ ngu·ªìn minh h·ªça (Python)

```python
# Mapper cho Job 4
import sys
import math

# Gi·∫£ s·ª≠ D (Total Documents) ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr∆∞·ªõc ho·∫∑c truy·ªÅn v√†o
# V√≠ d·ª•: D = 1000
D = 1000 

def mapper():
    for line in sys.stdin:
        # Input: ((word,docname)\t(n,N,d))
        pair, values = line.strip().split('\t')
        word, docname = pair.strip('()').split(',')
        n, N, d = map(int, values.strip('()').split(','))
        
        # T√≠nh TF-IDF
        tf = n / N
        idf = math.log(D / d)
        tf_idf = tf * idf
        
        # Output: ((word, docname), TF-IDF)
        print(f"({word},{docname})\t{tf_idf}")

# Reducer: Identity (ch·ªâ pass d·ªØ li·ªáu)
def reducer():
    for line in sys.stdin:
        print(line.strip())

if __name__ == "__main__":
    mapper()
```

### 4.3. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng
*   **Khi n√†o s·ª≠ d·ª•ng?** Khi ƒë√£ c√≥ ƒë·ªß c√°c th√†nh ph·∫ßn (TF, DF, N) ƒë·ªÉ t√≠nh to√°n tr·ªçng s·ªë cu·ªëi c√πng.
*   **∆Øu ƒëi·ªÉm:** Logic ƒë∆°n gi·∫£n, d·ªÖ d√†ng scale.

---

## T·ªïng k·∫øt & T·ªëi ∆∞u h√≥a (Final Thoughts)

### 5.1. Ph√¢n t√≠ch quy tr√¨nh
Quy tr√¨nh TF-IDF tr√™n MapReduce l√† v√≠ d·ª• ƒëi·ªÉn h√¨nh c·ªßa **Data Flow Pipeline**. Thay v√¨ c·ªë g·∫Øng l√†m t·∫•t c·∫£ trong m·ªôt Job (s·∫Ω r·∫•t ph·ª©c t·∫°p v√† d·ªÖ tr√†n b·ªô nh·ªõ), ch√∫ng ta chia th√†nh c√°c b∆∞·ªõc nh·ªè:
1.  Local Count (TF).
2.  Aggregation by Doc (T√≠nh N).
3.  Global Count (T√≠nh d).
4.  Final Calculation.

### 5.2. T·ªëi ∆∞u h√≥a (Optimization)
*   **Code Reuse:** C√°c l·ªõp (class) stock nh∆∞ Aggregation (t√≠nh t·ªïng) v√† Identity (ƒë·∫©y d·ªØ li·ªáu) c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng.
*   **Reduce I/O (Jobs 3 & 4):** Slide ch·ªâ ra r·∫±ng Job 3 v√† Job 4 c√≥ th·ªÉ g·ªôp l·∫°i trong c√πng m·ªôt Reducer.
    *   *T·∫°i sao?* Job 3 emit `((word, docname), (n, N, d))`. Thay v√¨ ghi ra disk r·ªìi Job 4 l·∫°i ƒë·ªçc v√†o, ta c√≥ th·ªÉ thi·∫øt k·∫ø m·ªôt Reducer nh·∫≠n `(word, (docname, n, N))`, t√≠nh `d` (b·∫±ng c√°ch ƒë·∫øm s·ªë l∆∞·ª£ng input), sau ƒë√≥ ngay l·∫≠p t·ª©c t√≠nh TF-IDF v√† emit k·∫øt qu·∫£ cu·ªëi.
    *   *L·ª£i √≠ch:* Ti·∫øt ki·ªám 1 l·∫ßn Read/Write t·ª´ HDFS, tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω ƒë√°ng k·ªÉ.

### 5.3. Th√°ch th·ª©c v·ªÅ Scale
*   **Memory Usage:** Slide c·∫£nh b√°o "must take care to ensure flat memory usage".
*   **Gi·∫£i th√≠ch:** Trong Reducer, n·∫øu m·ªôt t·ª´ (word) xu·∫•t hi·ªán trong qu√° nhi·ªÅu t√†i li·ªáu (v√≠ d·ª• t·ª´ "the", "a"), danh s√°ch `(docname, n, N)` c√≥ th·ªÉ qu√° l·ªõn ƒë·ªÉ fit v√†o RAM c·ªßa m·ªôt Node x·ª≠ l√Ω.
*   **Gi·∫£i ph√°p:** S·ª≠ d·ª•ng k·ªπ thu·∫≠t **Secondary Sort** ho·∫∑c thi·∫øt k·∫ø l·∫°i Key ƒë·ªÉ ph√¢n ph·ªëi d·ªØ li·ªáu ƒë·ªÅu h∆°n (v√≠ d·ª•: Hash Key).

### 5.4. V√≠ d·ª• th·ª±c t·∫ø trong ng√†nh
*   **C√¥ng c·ª• T√¨m ki·∫øm (Search Engines):** Google, Bing s·ª≠ d·ª•ng TF-IDF (ho·∫∑c c√°c bi·∫øn th·ªÉ n√¢ng cao nh∆∞ BM25) ƒë·ªÉ x·∫øp h·∫°ng relevancy c·ªßa k·∫øt qu·∫£ t√¨m ki·∫øm.
*   **Recommendation Systems:** ƒê√°nh gi√° s·ª± quan tr·ªçng c·ªßa m·ªôt t·ª´ trong ƒë√°nh gi√° s·∫£n ph·∫©m ƒë·ªÉ t√¨m ng∆∞·ªùi d√πng c√≥ s·ªü th√≠ch t∆∞∆°ng t·ª±.
*   **Document Clustering:** Ph√¢n lo·∫°i c√°c b√†i b√°o, t√†i li·ªáu theo ch·ªß ƒë·ªÅ d·ª±a tr√™n t·ª´ kh√≥a quan tr·ªçng.

---

D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu ph√¢n t√≠ch v√† tr√¨nh b√†y l·∫°i n·ªôi dung v·ªÅ **Thu·∫≠t to√°n T√¨m ki·∫øm theo Chi·ªÅu r·ªông (Breadth-First Search - BFS)** trong m√¥i tr∆∞·ªùng **MapReduce**, ƒë∆∞·ª£c ƒë√≥ng g√≥i theo y√™u c·∫ßu chuy√™n nghi·ªáp.

---

# Thu·∫≠t to√°n T√¨m ki·∫øm theo Chi·ªÅu r·ªông (BFS) tr√™n MapReduce

## 1. T·ªïng quan v·ªÅ B√†i to√°n ƒê·ªì th·ªã (Graph Data Structure)

### Kh√°i ni·ªám
Vi·ªác th·ª±c hi·ªán t√≠nh to√°n tr√™n c·∫•u tr√∫c d·ªØ li·ªáu ƒë·ªì th·ªã y√™u c·∫ßu x·ª≠ l√Ω t·∫°i m·ªói n√∫t (node). M·ªói n√∫t trong ƒë·ªì th·ªã th∆∞·ªùng ch·ª©a:
1.  **D·ªØ li·ªáu ri√™ng (Node-specific data):** Th√¥ng tin ƒë·ªãnh danh ho·∫∑c thu·ªôc t√≠nh c·ªßa n√∫t ƒë√≥.
2.  **Li√™n k·∫øt (Links/Edges):** C√°c ƒë∆∞·ªùng d·∫´n tr·ªè ƒë·∫øn c√°c n√∫t l√¢n c·∫≠n.

### V·∫•n ƒë·ªÅ c·ªët l√µi
*   **Traversing (Duy·ªát ƒë·ªì th·ªã):** Thu·∫≠t to√°n c·∫ßn di chuy·ªÉn qua l·∫°i gi·ªØa c√°c n√∫t ƒë·ªÉ th·ª±c hi·ªán b∆∞·ªõc t√≠nh to√°n.
*   **B√†i to√°n MapReduce:** L√†m th·∫ø n√†o ƒë·ªÉ duy·ªát m·ªôt ƒë·ªì th·ªã b·∫±ng MapReduce? Ch√∫ng ta n√™n bi·ªÉu di·ªÖn ƒë·ªì th·ªã nh∆∞ th·∫ø n√†o ƒë·ªÉ ph√π h·ª£p v·ªõi m√¥ h√¨nh n√†y?

---

## 2. Thu·∫≠t to√°n BFS (Breadth-First Search)

### Kh√°i ni·ªám
BFS l√† m·ªôt thu·∫≠t to√°n l·∫∑p (iterated algorithm) tr√™n ƒë·ªì th·ªã.
*   **Nguy√™n l√Ω:** B·∫Øt ƒë·∫ßu t·ª´ m·ªôt n√∫t ngu·ªìn (origin), m·ªü r·ªông "ti·ªÅn tuy·∫øn" (frontier) ra xa th√™m m·ªôt c·∫•p ƒë·ªô (level) v·ªõi m·ªói l·∫ßn l·∫∑p.
*   **V√≠ d·ª• minh h·ªça:**
    *   Level 0: N√∫t ngu·ªìn (v√≠ d·ª•: n√∫t 1).
    *   Level 1: C√°c n√∫t h√†ng x√≥m tr·ª±c ti·∫øp c·ªßa n√∫t ngu·ªìn (v√≠ d·ª•: n√∫t 2, 3).
    *   Level 2: C√°c n√∫t h√†ng x√≥m c·ªßa n√∫t 2 v√† 3 (v√≠ d·ª•: n√∫t 4).

### Th√°ch th·ª©c khi √°p d·ª•ng MapReduce
MapReduce ti√™u chu·∫©n l√† m·ªôt m√¥ h√¨nh x·ª≠ l√Ω **m·ªôt l·∫ßn** (batch processing). Tuy nhi√™n, BFS l√† m·ªôt thu·∫≠t to√°n **l·∫∑p** (iterative) v√¨ ta c·∫ßn th√¥ng tin t·ª´ c·∫•p ƒë·ªô tr∆∞·ªõc ƒë·ªÉ t√≠nh c·∫•p ƒë·ªô ti·∫øp theo.

*   **V·∫•n ƒë·ªÅ:** Thu·∫≠t to√°n BFS kh√¥ng "v·ª´a v·∫∑n" (fit) v·ªõi m√¥ h√¨nh MapReduce ƒë·ªôc l·∫≠p.
*   **Gi·∫£i ph√°p:** S·ª≠ d·ª•ng c√°c **ƒë·ª£t ch·∫°y MapReduce l·∫∑p l·∫°i (Iterated passes)**.
    *   ƒê·∫ßu v√†o c·ªßa ƒë·ª£t ch·∫°y th·ª© $N$ l√† k·∫øt qu·∫£ ƒë·∫ßu ra c·ªßa ƒë·ª£t ch·∫°y th·ª© $N-1$.
    *   Qu√° tr√¨nh n√†y l·∫∑p l·∫°i cho ƒë·∫øn khi kh√¥ng c√≤n n√∫t m·ªõi n√†o ƒë∆∞·ª£c kh√°m ph√°.

---

## 3. Bi·ªÉu di·ªÖn ƒê·ªì th·ªã (Graph Representations)

ƒê√¢y l√† ph·∫ßn quan tr·ªçng nh·∫•t ƒë·ªÉ t·ªëi ∆∞u h√≥a BFS tr√™n Big Data.

### Ph∆∞∆°ng ph√°p tr·ª±c quan nh·∫•t (Kh√¥ng khuy·∫øn kh√≠ch)
S·ª≠ d·ª•ng c·∫•u tr√∫c tham chi·∫øu (references) t·ª´ m·ªói n√∫t tr·ªè ƒë·∫øn h√†ng x√≥m c·ªßa n√≥ (gi·ªëng nh∆∞ con tr·ªè trong C/C++ ho·∫∑c object reference trong Java).

*   **H·∫°n ch·∫ø:** G·ª≠i to√†n b·ªô ƒë·ªì th·ªã (ho·∫∑c m·ªôt ph·∫ßn l·ªõn) ƒë·∫øn m·ªôt task Map (ho·∫∑c h√†ng ngh√¨n task Map) ƒë√≤i h·ªèi m·ªôt l∆∞·ª£ng b·ªô nh·ªõ kh·ªïng l·ªì, d·ªÖ g√¢y ra **OutOfMemoryError**.

### Gi·∫£i ph√°p t·ªëi ∆∞u cho MapReduce
Thay v√¨ g·ª≠i c·∫£ ƒë·ªëi t∆∞·ª£ng n√∫t, ch√∫ng ta n√™n bi·ªÉu di·ªÖn ƒë·ªì th·ªã d∆∞·ªõi d·∫°ng c√°c c·∫∑p **(Key, Value)** ƒë∆°n gi·∫£n.

#### V√≠ d·ª• bi·ªÉu di·ªÖn (Adjacency List)
Ch√∫ng ta c√≥ th·ªÉ bi·ªÉu di·ªÖn ƒë·ªì th·ªã d∆∞·ªõi d·∫°ng danh s√°ch k·ªÅ:
*   **Key:** ID c·ªßa n√∫t hi·ªán t·∫°i.
*   **Value:** Danh s√°ch c√°c n√∫t h√†ng x√≥m (ho·∫∑c th√¥ng tin ƒë∆∞·ªùng ƒëi).

**C·∫•u tr√∫c d·ªØ li·ªáu m·∫´u:**
```text
NodeID    Neighbors
1         [2, 3]
2         [1, 4]
3         [1, 4]
4         [2, 3]
```

---

## 4. Code M·∫´u & H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

### A. M√¥ ph·ªèng Logic BFS v·ªõi MapReduce (Pseudo-code)

D∆∞·ªõi ƒë√¢y l√† logic gi·∫£ l·∫≠p cho m·ªôt l∆∞·ª£t ch·∫°y (iteration) c·ªßa BFS tr√™n MapReduce. Gi·∫£ s·ª≠ ch√∫ng ta ƒëang t√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t t·ª´ m·ªôt ngu·ªìn.

#### 1. ƒê·ªãnh nghƒ©a D·ªØ li·ªáu (Node Representation)
M·ªói n√∫t s·∫Ω l∆∞u tr·ªØ:
*   Danh s√°ch h√†ng x√≥m (Adjacency List).
*   Kho·∫£ng c√°ch t·ª´ ngu·ªìn (Distance).
*   Tr·∫°ng th√°i (Visited or not).

#### 2. Mapper Code
Mapper nh·∫≠n v√†o m·ªôt n√∫t v√† th√¥ng tin kho·∫£ng c√°ch hi·ªán t·∫°i, sau ƒë√≥ "ph√°t" (emit) th√¥ng tin ƒë·∫øn c√°c h√†ng x√≥m.

```python
# Python Pseudo-code for BFS Mapper
def mapper(node_id, node_data):
    # node_data = {'neighbors': [list], 'distance': int, 'color': str}
    
    # N·∫øu n√∫t n√†y ƒë√£ ƒë∆∞·ª£c thƒÉm (Black), ta kh√¥ng c·∫ßn x·ª≠ l√Ω l·∫°i
    if node_data['color'] == 'BLACK':
        return
    
    # Ph√°t th√¥ng tin ƒë·∫øn c√°c h√†ng x√≥m (Edge propagation)
    for neighbor in node_data['neighbors']:
        # H√†ng x√≥m nh·∫≠n th√¥ng b√°o: "Ta c√°ch ngu·ªìn bao xa?"
        emit(neighbor, node_data['distance'] + 1)
    
    # Ph√°t l·∫°i th√¥ng tin n√∫t hi·ªán t·∫°i ƒë·ªÉ gi·ªØ c·∫•u tr√∫c ƒë·ªì th·ªã (State preservation)
    emit(node_id, node_data)
```

#### 3. Reducer Code
Reducer nh·∫≠n th√¥ng tin t·ª´ Mapper v√† c·∫≠p nh·∫≠t kho·∫£ng c√°ch m·ªõi nh·∫•t (ng·∫Øn nh·∫•t).

```python
# Python Pseudo-code for BFS Reducer
def reducer(key, values):
    # values l√† danh s√°ch c√°c message: [Node_Data, Distance_Update, ...]
    
    current_distance = INF
    neighbors = []
    
    for value in values:
        if isinstance(value, NodeData):
            # Gi·ªØ l·∫°i danh s√°ch h√†ng x√≥m
            neighbors = value.neighbors
            if value.distance < current_distance:
                current_distance = value.distance
        elif isinstance(value, int):
            # C·∫≠p nh·∫≠t kho·∫£ng c√°ch m·ªõi (t·ª´ Mapper)
            if value < current_distance:
                current_distance = value
    
    # N·∫øu kho·∫£ng c√°ch thay ƒë·ªïi, n√∫t n√†y c√≥ th·ªÉ d√πng cho l∆∞·ª£t sau
    new_color = 'BLACK' if current_distance != INF else 'WHITE'
    
    # Emit k·∫øt qu·∫£ m·ªõi
    emit(key, {'neighbors': neighbors, 'distance': current_distance, 'color': new_color})
```

### B. V√≠ d·ª• th·ª±c t·∫ø: T√≠nh to√°n PageRank (B√†i to√°n t∆∞∆°ng t·ª±)

PageRank c≈©ng l√† m·ªôt thu·∫≠t to√°n l·∫∑p tr√™n ƒë·ªì th·ªã, s·ª≠ d·ª•ng logic t∆∞∆°ng t·ª± BFS (truy·ªÅn th√¥ng tin qua l·∫°i gi·ªØa c√°c n√∫t).

**Khi n√†o s·ª≠ d·ª•ng?**
*   Khi b·∫°n c√≥ ƒë·ªì th·ªã kh·ªïng l·ªì (h√†ng t·ª∑ n√∫t, h√†ng ch·ª•c t·ª∑ c·∫°nh) v√† c·∫ßn t√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t, ho·∫∑c t√≠nh to√°n t·∫ßm quan tr·ªçng c·ªßa c√°c n√∫t.

**S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
1.  **Input Format:** D·∫°ng text file, m·ªói d√≤ng: `NodeID \t Neighbor1,Neighbor2,...`
2.  **Driver Program:** Vi·∫øt m·ªôt script (Java/Python) ƒëi·ªÅu khi·ªÉn v√≤ng l·∫∑p.
    *   Ch·∫°y MapReduce Job 1.
    *   Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng (v√≠ d·ª•: kh√¥ng c√≤n n√∫t m·ªõi c√≥ kho·∫£ng c√°ch thay ƒë·ªïi).
    *   Ch·∫°y MapReduce Job 2...
3.  **L∆∞u tr·ªØ:** S·ª≠ d·ª•ng HDFS ƒë·ªÉ l∆∞u tr·ªØ ƒë·∫ßu ra c·ªßa m·ªói iteration.

### C. ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| Ti√™u ch√≠ | Ph√¢n t√≠ch |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | **T√≠nh m·ªü r·ªông (Scalability):** X·ª≠ l√Ω ƒë∆∞·ª£c ƒë·ªì th·ªã l·ªõn h∆°n b·ªô nh·ªõ c·ªßa m·ªôt m√°y ƒë∆°n.<br>**Tolerance (Ch·ªãu l·ªói):** MapReduce t·ª± ƒë·ªông x·ª≠ l√Ω n·∫øu m·ªôt node t√≠nh to√°n b·ªã l·ªói.<br>**Ph√π h·ª£p Batch:** T·ªëi ∆∞u cho b√†i to√°n duy·ªát to√†n b·ªô ƒë·ªì th·ªã m·ªôt c√°ch ch·∫≠m r√£i. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | **ƒê·ªô tr·ªÖ (Latency):** R·∫•t ch·∫≠m cho c√°c b√†i to√°n c·∫ßn ph·∫£n h·ªìi th·ªùi gian th·ª±c (Real-time).<br>**Overhead I/O:** M·ªói iteration l√† m·ªôt Job MapReduce ri√™ng bi·ªát, t·ªën k√©m I/O ghi xu·ªëng HDFS.<br>**Kh√≥ khƒÉn v·ªõi ƒë·ªì th·ªã c√≥ c·∫•u tr√∫c ph·ª©c t·∫°p:** Vi·ªác x·ª≠ l√Ω c√°c ƒë·ªì th·ªã c√≥ t√≠nh ch·∫•t "ti·∫øn h√≥a" (Dynamic graph) r·∫•t kh√≥ khƒÉn. |

---

## 5. V√≠ d·ª• Th·ª±c t·∫ø trong C√¥ng nghi·ªáp

### 1. M·∫°ng x√£ h·ªôi (Social Networks)
*   **B√†i to√°n:** T√¨m b·∫°n chung (Mutual Friends) ho·∫∑c ƒë·ªÅ xu·∫•t b·∫°n b√® (Friend Recommendation).
*   **·ª®ng d·ª•ng BFS:** Duy·ªát qua c√°c k·∫øt n·ªëi ƒë·ªÉ t√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t gi·ªØa hai ng∆∞·ªùi d√πng (Degrees of Separation). V√≠ d·ª•: Facebook t√¨m xem b·∫°n c√°ch m·ªôt ng∆∞·ªùi l·∫° bao nhi√™u "c√°i b·∫Øt tay".

### 2. H·ªá th·ªëng ƒê·ªãnh tuy·∫øn (Routing Systems)
*   **B√†i to√°n:** T√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t trong m·∫°ng l∆∞·ªõi giao th√¥ng ho·∫∑c m·∫°ng m√°y t√≠nh.
*   **·ª®ng d·ª•ng:** C√°c c√¥ng ty logistics s·ª≠ d·ª•ng ƒë·ªì th·ªã (ƒë∆∞·ªùng ph·ªë l√† c·∫°nh, giao l·ªô l√† n√∫t) ƒë·ªÉ t√≠nh to√°n l·ªô tr√¨nh giao h√†ng t·ªëi ∆∞u nh·∫•t.

### 3. Web Crawling
*   **B√†i to√°n:** L·∫≠p ch·ªâ m·ª•c (Indexing) c√°c trang web.
*   **·ª®ng d·ª•ng:** C√°c c√¥ng c·ª• t√¨m ki·∫øm (Google, Bing) s·ª≠ d·ª•ng c√°c bi·∫øn th·ªÉ c·ªßa BFS ƒë·ªÉ duy·ªát web (Web Crawling), b·∫Øt ƒë·∫ßu t·ª´ c√°c trang seed v√† lan t·ªèa ra c√°c li√™n k·∫øt b√™n ngo√†i.

---

## T√≥m t·∫Øt

ƒê·ªÉ th·ª±c hi·ªán **BFS tr√™n MapReduce**, ch√∫ng ta kh√¥ng th·ªÉ d√πng thu·∫≠t to√°n ƒë·ªá quy hay queue truy·ªÅn th·ªëng. Thay v√†o ƒë√≥, ch√∫ng ta:
1.  **Bi·ªÉu di·ªÖn ƒë·ªì th·ªã** d∆∞·ªõi d·∫°ng danh s√°ch k·ªÅ (Adjacency List) l∆∞u tr√™n HDFS.
2.  **Chia nh·ªè b√†i to√°n** th√†nh c√°c MapReduce Jobs l·∫∑p l·∫°i.
3.  **M·ªói iteration** t∆∞∆°ng ƒë∆∞∆°ng v·ªõi m·ªôt b∆∞·ªõc ti·∫øn c·ªßa "ti·ªÅn tuy·∫øn" (frontier) trong BFS.
4.  **L∆∞u √Ω:** C·∫ßn t·ªëi ∆∞u h√≥a vi·ªác truy·ªÅn d·ªØ li·ªáu gi·ªØa c√°c iteration ƒë·ªÉ tr√°nh t·∫Øc ngh·∫Ωn I/O.

---

Ch√†o b·∫°n, t√¥i l√† chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "6_mapreduce.pdf" m√† b·∫°n ƒë√£ cung c·∫•p, ƒë∆∞·ª£c tr√¨nh b√†y l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch ƒê·∫°i di·ªán ƒê·ªì th·ªã v√† T√¨m ƒê∆∞·ªùng ƒëi Ng·∫Øn nh·∫•t trong MapReduce

T√†i li·ªáu n√†y t·∫≠p trung v√†o c√°c ph∆∞∆°ng ph√°p bi·ªÉu di·ªÖn ƒë·ªì th·ªã (Graph Representation) v√† c√°ch ti·∫øp c·∫≠n ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n t√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t (Shortest Path) trong m√¥i tr∆∞·ªùng t√≠nh to√°n ph√¢n t√°n, c·ª• th·ªÉ l√† v·ªõi m√¥ h√¨nh MapReduce.

## 1. ƒê·∫°i di·ªán ƒê·ªì th·ªã Tr·ª±c ti·∫øp (Direct References)

### Kh√°i ni·ªám
Ph∆∞∆°ng ph√°p n√†y s·ª≠ d·ª•ng c√°c ƒë·ªëi t∆∞·ª£ng (objects) ƒë·ªÉ bi·ªÉu di·ªÖn c√°c n√∫t (nodes) trong ƒë·ªì th·ªã. M·ªói ƒë·ªëi t∆∞·ª£ng `GraphNode` ch·ª©a d·ªØ li·ªáu c·ªßa n√≥ v√† m·ªôt danh s√°ch c√°c tham chi·∫øu tr·ª±c ti·∫øp ƒë·∫øn c√°c n√∫t m√† n√≥ k·∫øt n·ªëi ƒë·∫øn (c√°c c·∫°nh ƒëi ra - `out_edges`).

### Ph√¢n t√≠ch Code & C·∫•u tr√∫c D·ªØ li·ªáu
C·∫•u tr√∫c n√†y th∆∞·ªùng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ m·ªôt l·ªõp ƒë·ªëi t∆∞·ª£ng trong c√°c ng√¥n ng·ªØ h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng (OOP).

```java
// ƒê·ªãnh nghƒ©a l·ªõp GraphNode
class GraphNode {
    Object data;           // D·ªØ li·ªáu c·ªßa n√∫t (v√≠ d·ª•: t√™n, ID)
    Vector<GraphNode> out_edges; // Danh s√°ch c√°c n√∫t k·ªÅ (c·∫°nh ƒëi ra)
    GraphNode iter_next;   // Con tr·ªè ƒë·ªÉ duy·ªát qua danh s√°ch (n·∫øu c·∫ßn)

    // Constructor v√† c√°c ph∆∞∆°ng th·ª©c kh√°c...
}
```

### Gi·∫£i th√≠ch Chi ti·∫øt
*   **Inherent Structure (C·∫•u tr√∫c b·∫©m sinh):** C·∫•u tr√∫c d·ªØ li·ªáu n√†y ph·∫£n √°nh tr·ª±c ti·∫øp m·ªëi quan h·ªá th·ª±c t·∫ø gi·ªØa c√°c n√∫t.
*   **Iteration (L·∫∑p):** ƒê·ªÉ duy·ªát qua c√°c n√∫t k·ªÅ, h·ªá th·ªëng c·∫ßn ph·∫£i ƒëi theo c√°c con tr·ªè t·ª´ n√∫t n√†y sang n√∫t kh√°c.
*   **Common View of Shared Memory (Chung view b·ªô nh·ªõ chia s·∫ª):** ƒêi·ªÅu n√†y gi·∫£ ƒë·ªãnh r·∫±ng t·∫•t c·∫£ c√°c n√∫t ƒë·ªÅu n·∫±m trong c√πng m·ªôt kh√¥ng gian b·ªô nh·ªõ v√† c√≥ th·ªÉ truy c·∫≠p l·∫´n nhau. ƒêi·ªÅu n√†y y√™u c·∫ßu c∆° ch·∫ø **synchronization (ƒë·ªìng b·ªô h√≥a)** ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh to√†n v·∫πn d·ªØ li·ªáu khi nhi·ªÅu lu·ªìng truy c·∫≠p c√πng l√∫c.
*   **Not Easily Serializable (Kh√≥ tu·∫ßn t·ª± h√≥a):** C√°c con tr·ªè b·ªô nh·ªõ ch·ªâ c√≥ √Ω nghƒ©a trong m·ªôt m√°y t√≠nh ƒë∆°n l·∫ª. Khi chuy·ªÉn d·ªØ li·ªáu qua m·∫°ng trong m√¥i tr∆∞·ªùng ph√¢n t√°n (nh∆∞ MapReduce), c√°c con tr·ªè n√†y tr·ªü n√™n v√¥ nghƒ©a.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
| Ti√™u ch√≠ | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
| :--- | :--- | :--- |
| **Truy xu·∫•t** | Nhanh ch√≥ng khi duy·ªát tr·ª±c ti·∫øp tr√™n RAM. | Ph√π h·ª£p cho c√°c h·ªá th·ªëng **Single Machine** (m√°y ƒë∆°n). |
| **Ph√¢n t√°n** | - | **Kh√¥ng kh·∫£ thi** trong m√¥i tr∆∞·ªùng ph√¢n t√°n do ph·ª• thu·ªôc v√†o con tr·ªè b·ªô nh·ªõ ƒë·ªãa ph∆∞∆°ng. |
| **B·ªô nh·ªõ** | - | Chi ph√≠ b·ªô nh·ªõ cao n·∫øu l∆∞u tr·ªØ ƒë·∫ßy ƒë·ªß ƒë·ªëi t∆∞·ª£ng. |

---

## 2. Ma tr·∫≠n K·ªÅ (Adjacency Matrices)

### Kh√°i ni·ªám
ƒê√¢y l√† m·ªôt ph∆∞∆°ng ph√°p ƒë·∫°i di·ªán ƒë·ªì th·ªã kinh ƒëi·ªÉn s·ª≠ d·ª•ng m·ªôt ma tr·∫≠n vu√¥ng. N·∫øu c√≥ m·ªôt li√™n k·∫øt t·ª´ n√∫t `i` ƒë·∫øn n√∫t `j`, th√¨ gi√° tr·ªã t·∫°i v·ªã tr√≠ `M[i][j]` b·∫±ng `1` (ho·∫∑c tr·ªçng s·ªë c·ªßa c·∫°nh).

### Minh h·ªça
Gi·∫£ s·ª≠ ƒë·ªì th·ªã c√≥ 5 n√∫t (0 ƒë·∫øn 4):

```
      0  1  2  3  4
    0 0  1  0  1  1
    1 1  0  1  0  1
    2 0  1  0  0  0
    3 1  0  0  0  0
    4 1  1  0  0  0
```

*   H√†ng 0 c√≥ c√°c gi√° tr·ªã `1` t·∫°i c·ªôt 1, 3, 4 -> N√∫t 0 k·∫øt n·ªëi ƒë·∫øn n√∫t 1, 3, 4.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
*   **∆Øu ƒëi·ªÉm:** Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c·∫°nh `(i, j)` r·∫•t nhanh (truy c·∫≠p tr·ª±c ti·∫øp `M[i][j]`).
*   **Nh∆∞·ª£c ƒëi·ªÉm:** ƒê·ªëi v·ªõi c√°c ƒë·ªì th·ªã l·ªõn (v√≠ d·ª•: Web, M·∫°ng x√£ h·ªôi), ma tr·∫≠n n√†y s·∫Ω r·∫•t th∆∞a (sparse) v√¨ s·ªë l∆∞·ª£ng c·∫°nh th·ª±c t·∫ø nh·ªè h∆°n r·∫•t nhi·ªÅu so v·ªõi s·ªë l∆∞·ª£ng c·∫°nh c√≥ th·ªÉ c√≥ ($N^2$). ƒêi·ªÅu n√†y g√¢y l√£ng ph√≠ b·ªô nh·ªõ kh·ªïng l·ªì.

---

## 3. Ma tr·∫≠n K·ªÅ D·∫°ng Th∆∞a (Sparse Matrices)

### Kh√°i ni·ªám
V√¨ ma tr·∫≠n k·ªÅ th∆∞·ªùng ch·ª©a r·∫•t nhi·ªÅu s·ªë `0`, ta ch·ªâ c·∫ßn l∆∞u tr·ªØ c√°c gi√° tr·ªã kh√°c `0` (c√°c c·∫°nh th·ª±c s·ª± t·ªìn t·∫°i).

### C·∫•u tr√∫c D·ªØ li·ªáu
Thay v√¨ l∆∞u ma tr·∫≠n $N \times N$, ta l∆∞u tr·ªØ:
*   **Key:** N√∫t ngu·ªìn (source node).
*   **Value:** Danh s√°ch c√°c n√∫t ƒë√≠ch (destination nodes).

**V√≠ d·ª•:**
*   D·∫°ng ma tr·∫≠n: N√∫t 1 n·ªëi v·ªõi 3, 18, 200.
*   D·∫°ng th∆∞a (Sparse):
    ```text
    1: [3, 18, 200]
    2: [6, 12, 80, 400]
    3: [1, 14]
    ...
    ```

### ·ª®ng d·ª•ng trong MapReduce
ƒê√¢y l√† ƒë·ªãnh d·∫°ng l√Ω t∆∞·ªüng cho MapReduce. N√∫t ngu·ªìn c√≥ th·ªÉ l√†m **Key**, v√† danh s√°ch n√∫t ƒë√≠ch l√† **Value**. ƒêi·ªÅu n√†y cho ph√©p ph√¢n chia (partition) ƒë·ªì th·ªã m·ªôt c√°ch t·ª± nhi√™n ƒë·ªÉ x·ª≠ l√Ω song song.

---

## 4. T√¨m ƒê∆∞·ªùng ƒëi Ng·∫Øn nh·∫•t (Finding the Shortest Path)

### Kh√°i ni·ªám
*   **B√†i to√°n:** T√¨m ƒë∆∞·ªùng ƒëi c√≥ t·ªïng tr·ªçng s·ªë (ho·∫∑c s·ªë l∆∞·ª£ng c·∫°nh) nh·ªè nh·∫•t t·ª´ m·ªôt n√∫t ngu·ªìn (start node) ƒë·∫øn m·ªôt ho·∫∑c nhi·ªÅu n√∫t ƒë√≠ch (target nodes).
*   **T√™n g·ªçi:** *Single-Source Shortest Path (SSSP)*.
*   **Thu·∫≠t to√°n kinh ƒëi·ªÉn:** Dijkstra (tr√™n m√°y ƒë∆°n).

### C√¢u h·ªèi: C√≥ th·ªÉ d√πng BFS (Breadth-First Search) v·ªõi MapReduce kh√¥ng?
C√¢u tr·∫£ l·ªùi l√† **C√≥**, nh∆∞ng c·∫ßn m·ªôt c√°ch ti·∫øp c·∫≠n ƒë·∫∑c bi·ªát g·ªçi l√† **Iterative MapReduce** (ho·∫∑c c√°c v√≤ng l·∫∑p t√≠nh to√°n).

### Gi·∫£i th√≠ch Intuition (Tr·ª±c gi√°c)
B√†i to√°n c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i quy·∫øt b·∫±ng ph∆∞∆°ng ph√°p quy ho·∫°ch ƒë·ªông (inductive approach). Ta t√≠nh to√°n kho·∫£ng c√°ch ƒë·∫øn c√°c n√∫t m·ªôt c√°ch tu·∫ßn t·ª±.

#### Logic thu·∫≠t to√°n:
1.  **Kh·ªüi t·∫°o:**
    *   `DistanceTo(startNode) = 0`
    *   C√°c n√∫t kh√°c: `DistanceTo(n) = Infinity` (v√¥ c·ª±c)

2.  **B∆∞·ªõc 1 (C√°c n√∫t k·ªÅ tr·ª±c ti·∫øp):**
    *   V·ªõi t·∫•t c·∫£ c√°c n√∫t `n` tr·ª±c ti·∫øp k·∫øt n·ªëi t·ª´ `startNode`:
    *   `DistanceTo(n) = 1`

3.  **B∆∞·ªõc 2 (M·ªü r·ªông):**
    *   V·ªõi m·ªôt n√∫t `n` b·∫•t k·ª≥, kho·∫£ng c√°ch ƒë·∫øn `n` l√† `1 +` kho·∫£ng c√°ch nh·ªè nh·∫•t (min) trong s·ªë c√°c n√∫t `m` c√≥ th·ªÉ ƒëi ƒë·∫øn `n`.
    *   C√¥ng th·ª©c: `DistanceTo(n) = 1 + min(DistanceTo(m))` v·ªõi m·ªçi `m` l√† n√∫t ngu·ªìn c·ªßa c·∫°nh `(m, n)`.

### V√≠ d·ª• Code M·∫´u (Pseudo-code cho MapReduce)

ƒê·ªÉ th·ª±c hi·ªán SSSP tr√™n MapReduce, ta th∆∞·ªùng ch·∫°y m·ªôt chu·ªói c√°c c√¥ng vi·ªác (job) cho ƒë·∫øn khi kho·∫£ng c√°ch kh√¥ng c√≤n thay ƒë·ªïi.

**D·ªØ li·ªáu ƒë·∫ßu v√†o (Input):**
D·∫°ng `Adjacency List`: `NodeID [Neighbors...]`
D·∫°ng `Distance`: `NodeID Distance`

**Mapper:**
```python
def map(node_id, neighbors_and_distance):
    # node_id: ID c·ªßa n√∫t hi·ªán t·∫°i
    # neighbors_and_distance: (danh s√°ch h√†ng x√≥m, kho·∫£ng c√°ch hi·ªán t·∫°i)
    
    neighbors, current_dist = neighbors_and_distance
    
    # Ph√°t t√°n kho·∫£ng c√°ch hi·ªán t·∫°i ƒë·∫øn t·∫•t c·∫£ h√†ng x√≥m
    for neighbor in neighbors:
        # H√†ng x√≥m 'neighbor' c√≥ th·ªÉ ƒë·∫øn ƒë∆∞·ª£c t·ª´ 'node_id' 
        # v·ªõi kho·∫£ng c√°ch l√† current_dist + 1
        emit(neighbor, current_dist + 1)
        
    # Gi·ªØ l·∫°i c·∫•u tr√∫c ƒë·ªì th·ªã ƒë·ªÉ v√≤ng l·∫∑p sau v·∫´n bi·∫øt ƒë∆∞·ªùng ƒëi
    emit(node_id, (neighbors, current_dist)) 
```

**Reducer:**
```python
def reduce(node_id, values):
    # values l√† danh s√°ch c√°c kho·∫£ng c√°ch m·ªõi ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t + (danh s√°ch h√†ng x√≥m c≈©)
    
    min_distance = Infinity
    neighbors = [] # Gi·ªØ l·∫°i danh s√°ch h√†ng x√≥m
    
    for value in values:
        if isinstance(value, tuple): 
            # ƒê√¢y l√† c·∫•u tr√∫c ƒë·ªì th·ªã c≈© (neighbors list)
            neighbors = value[0]
            # C·∫≠p nh·∫≠t l·∫°i kho·∫£ng c√°ch c≈© n·∫øu c·∫ßn (ƒë·ªÉ gi·ªØ c·∫•u tr√∫c)
            old_dist = value[1]
            if old_dist < min_distance:
                min_distance = old_dist
        else:
            # ƒê√¢y l√† kho·∫£ng c√°ch m·ªõi ƒë∆∞·ª£c Mapper g·ª≠i ƒë·∫øn
            if value < min_distance:
                min_distance = value
                
    # Ph√°t ra k·∫øt qu·∫£ cu·ªëi c√πng cho n√∫t n√†y
    emit(node_id, (neighbors, min_distance))
```

### S·ª≠ d·ª•ng khi n√†o v√† nh∆∞ th·∫ø n√†o?
*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Khi ƒë·ªì th·ªã qu√° l·ªõn kh√¥ng th·ªÉ n·∫°p v√†o RAM c·ªßa m·ªôt m√°y ƒë∆°n (v√≠ d·ª•: Map c·ªßa th·∫ø gi·ªõi, Graph c·ªßa Facebook).
    *   Khi c·∫ßn t√≠nh to√°n c√°c thu·ªôc t√≠nh to√†n c·ª•c c·ªßa ƒë·ªì th·ªã (PageRank, Connected Components, Shortest Path).
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   **C√°ch 1 (BFS kinh ƒëi·ªÉn):** Ch·∫°y MapReduce cho m·ªói ƒë·ªô s√¢u (level). V√≤ng l·∫∑p 1 t√¨m t·∫•t c·∫£ n√∫t ·ªü kho·∫£ng c√°ch 1, v√≤ng l·∫∑p 2 t√¨m kho·∫£ng c√°ch 2...
    *   **C√°ch 2 (Distributed Dijkstra):** S·ª≠ d·ª•ng bi·∫øn to√†n c·ª•c ho·∫∑c c∆° ch·∫ø "th√¥ng ƒëi·ªáp" ƒë·ªÉ c·∫≠p nh·∫≠t kho·∫£ng c√°ch t·ªët nh·∫•t. Trong MapReduce, c√°ch n√†y th∆∞·ªùng ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch ch·∫°y l·∫∑p l·∫°i (iterations) cho ƒë·∫øn khi kh√¥ng c√≤n c·∫≠p nh·∫≠t kho·∫£ng c√°ch m·ªõi (gi·∫£m gi√° tr·ªã).

### V√≠ d·ª• Th·ª±c t·∫ø trong C√¥ng nghi·ªáp
1.  **Google Maps:** T√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t gi·ªØa hai ƒë·ªãa ƒëi·ªÉm tr√™n m·ªôt bi·ªÉu ƒë·ªì ƒë∆∞·ªùng b·ªô kh·ªïng l·ªì.
2.  **M·∫°ng x√£ h·ªôi:** ƒê·ªÅ xu·∫•t "B·∫°n c·ªßa b·∫°n b√®" (Friend of Friend) ho·∫∑c t√≠nh to√°n kho·∫£ng c√°ch gi·ªØa hai ng∆∞·ªùi d√πng (degrees of separation).
3.  **Ph√¢n t√≠ch R·ªßi ro T√†i ch√≠nh:** T√≠nh to√°n t√°c ƒë·ªông lan truy·ªÅn (cascading failures) trong m·∫°ng l∆∞·ªõi c√°c ng√¢n h√†ng ho·∫∑c giao d·ªãch.

---

## T√≥m t·∫Øt Ph√¢n t√≠ch

| Kh√°i ni·ªám | B·∫£n ch·∫•t | Ph√π h·ª£p v·ªõi MapReduce? |
| :--- | :--- | :--- |
| **Direct References** | D√πng con tr·ªè ƒë·ªëi t∆∞·ª£ng. | **Kh√¥ng** (Ch·ªâ cho Single Machine). |
| **Adjacency Matrix** | Ma tr·∫≠n vu√¥ng $N \times N$. | **Kh√≥** (Qu√° l·ªõn v√† th∆∞a). |
| **Sparse Matrix** | List c√°c c·∫°nh (Key-Value). | **R·∫•t t·ªët** (Ti√™u chu·∫©n cho Graph Processing). |
| **Shortest Path** | T√≠nh to√°n kho·∫£ng c√°ch theo t·ª´ng b∆∞·ªõc. | **C√≥** (D√πng thu·∫≠t to√°n l·∫∑p - Iterative BFS/Dijkstra). |

---

Ch√†o b·∫°n, t√¥i l√† chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt n·ªôi dung slide v·ªÅ **MapReduce v√† thu·∫≠t to√°n t√¨m ƒë∆∞·ªùng ng·∫Øn nh·∫•t (Shortest Path)**, ƒë∆∞·ª£c tr√¨nh b√†y chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát theo y√™u c·∫ßu c·ªßa b·∫°n.

---

# Ph√¢n t√≠ch thu·∫≠t to√°n MapReduce cho T√¨m ƒê∆∞·ªùng Ng·∫Øn Nh·∫•t (Shortest Path Algorithm)

T√†i li·ªáu n√†y m√¥ t·∫£ c√°ch ti·∫øp c·∫≠n ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n t√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t trong m·ªôt ƒë·ªì th·ªã (Graph) s·ª≠ d·ª•ng m√¥ h√¨nh l·∫≠p tr√¨nh MapReduce. ƒê√¢y l√† m·ªôt v√≠ d·ª• ƒëi·ªÉn h√¨nh c·ªßa vi·ªác x·ª≠ l√Ω d·ªØ li·ªáu ƒë·ªì th·ªã quy m√¥ l·ªõn (Large-scale Graph Processing).

## 1. From Intuition to Algorithm (T·ª´ tr·ª±c quan ƒë·∫øn thu·∫≠t to√°n)

### Kh√°i ni·ªám ch√≠nh
Thu·∫≠t to√°n n√†y m√¥ ph·ªèng qu√° tr√¨nh **Breadth-First Search (BFS)** - T√¨m ki·∫øm theo chi·ªÅu r·ªông. √ù t∆∞·ªüng c∆° b·∫£n l√† lan truy·ªÅn kho·∫£ng c√°ch t·ª´ n√∫t ngu·ªìn (start node) ƒë·∫øn c√°c n√∫t l√°ng gi·ªÅng.

*   **Input:** M·ªôt n√∫t `n` (key) v√† th√¥ng tin li√™n quan `(D, points-to)` (value).
    *   `D`: Kho·∫£ng c√°ch t·ª´ n√∫t b·∫Øt ƒë·∫ßu ƒë·∫øn n√∫t `n`.
    *   `points-to`: Danh s√°ch c√°c n√∫t c√≥ th·ªÉ ƒëi t·ªõi t·ª´ `n`.
*   **Logic Map:** V·ªõi m·ªói n√∫t `p` trong danh s√°ch `points-to`, ta ph√°t ra (emit) m·ªôt c·∫∑p `(p, D + 1)`. ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†: "N√∫t `p` c√≥ th·ªÉ ƒë∆∞·ª£c ƒë·∫øn t·ª´ `n` v·ªõi kho·∫£ng c√°ch l√† `D + 1`".
*   **Logic Reduce:** N√∫t Reduce nh·∫≠n t·∫•t c·∫£ c√°c kho·∫£ng c√°ch ti·ªÅm nƒÉng ƒë·∫øn m·ªôt n√∫t `p` v√† ch·ªçn ra gi√° tr·ªã nh·ªè nh·∫•t (minimum).

### Code m·∫´u (Pseudo-code)

D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ gi·∫£ minh h·ªça cho logic tr√™n, ƒë∆∞·ª£c vi·∫øt b·∫±ng Python ƒë·ªÉ d·ªÖ hi·ªÉu.

```python
# --- MAPPER ---
def mapper(node_id, value):
    """
    Input: node_id (Key), value = (distance, list_of_neighbors)
    """
    distance, neighbors = value
    
    # Ph√°t ra th√¥ng tin v·ªÅ c·∫•u tr√∫c ƒë·ªì th·ªã (gi·ªØ l·∫°i n√∫t g·ªëc)
    # ƒêi·ªÅu n√†y gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ "Where'd the points-to list go?" ·ªü slide 42
    emit(node_id, ('graph_info', neighbors))
    
    # Lan truy·ªÅn kho·∫£ng c√°ch ƒë·∫øn c√°c n√∫t l√°ng gi·ªÅng
    for neighbor in neighbors:
        # Gi·∫£ s·ª≠ neighbors l√† danh s√°ch c√°c n√∫t, weight = 1
        new_distance = distance + 1
        emit(neighbor, ('distance', new_distance))

# --- REDUCER ---
def reducer(node_id, values):
    """
    Input: node_id (Key), values = Danh s√°ch c√°c kho·∫£ng c√°ch ho·∫∑c info
    """
    min_distance = float('inf')
    points_to = []
    
    for value_type, val in values:
        if value_type == 'graph_info':
            # Gi·ªØ l·∫°i c·∫•u tr√∫c ƒë·ªì th·ªã cho v√≤ng l·∫∑p ti·∫øp theo
            points_to = val
        elif value_type == 'distance':
            # T√¨m kho·∫£ng c√°ch nh·ªè nh·∫•t
            if val < min_distance:
                min_distance = val
    
    # Output: (min_distance, points_to)
    # N·∫øu kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng ƒëi ng·∫Øn h∆°n, gi·ªØ nguy√™n kho·∫£ng c√°ch c≈©
    emit(node_id, (min_distance, points_to))
```

---

## 2. Discussion (Th·∫£o lu·∫≠n v·ªÅ v√≤ng l·∫∑p v√† tr·∫°ng th√°i)

### V·∫•n ƒë·ªÅ (Problem)
MapReduce l√† m·ªôt m√¥ h√¨nh Stateless (kh√¥ng tr·∫°ng th√°i) v√† th∆∞·ªùng k·∫øt th√∫c sau m·ªôt Job. Tuy nhi√™n, thu·∫≠t to√°n BFS ƒë√≤i h·ªèi nhi·ªÅu b∆∞·ªõc (hop) ƒë·ªÉ ƒëi h·∫øt ƒë·ªì th·ªã.
*   **V·∫•n ƒë·ªÅ:** Sau khi Mapper ch·∫°y xong, th√¥ng tin v·ªÅ c·∫•u tr√∫c ƒë·ªì th·ªã (`points-to list`) b·ªã m·∫•t. N·∫øu kh√¥ng c√≥ n√≥, Reduce s·∫Ω kh√¥ng bi·∫øt n√∫t n√†o ƒë·ªÉ ti·∫øp t·ª•c lan truy·ªÅn ·ªü b∆∞·ªõc sau.

### Gi·∫£i ph√°p (Solution)
*   **V√≤ng l·∫∑p (Iteration):** M·ªôt th√†nh ph·∫ßn ngo√†i MapReduce (nh∆∞ m·ªôt b·ªô ƒëi·ªÅu ph·ªëi ho·∫∑c script ƒëi·ªÅu khi·ªÉn) s·∫Ω l·∫•y output c·ªßa Reduce v√† feed (cung c·∫•p) l·∫°i l√†m input cho Mapper c·ªßa iteration ti·∫øp theo.
*   **Gi·ªØ l·∫°i c·∫•u tr√∫c:** Mapper c·∫ßn emit th√™m c·∫∑p `(n, points-to)`. ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o r·∫±ng sau m·ªói b∆∞·ªõc,Reduce v·∫´n gi·ªØ ƒë∆∞·ª£c danh s√°ch c√°c n√∫t l√°ng gi·ªÅng ƒë·ªÉ ti·∫øp t·ª•c qu√° tr√¨nh t√≠nh to√°n.

### Code M·∫´u: V√≤ng l·∫∑p ƒëi·ªÅu khi·ªÉn (Driver Script)

```python
# Pseudo-code cho vi·ªác ƒëi·ªÅu khi·ªÉn v√≤ng l·∫∑p
def run_bfs(start_node, graph_data):
    # Kh·ªüi t·∫°o kho·∫£ng c√°ch cho node b·∫Øt ƒë·∫ßu l√† 0, c√°c node kh√°c l√† v√¥ c√πng
    current_distances = initialize_distances(graph_data, start_node)
    
    iteration = 0
    while True:
        print(f"Running MapReduce Iteration {iteration}...")
        
        # Ch·∫°y MapReduce Job
        # Input: current_distances (bao g·ªìm distance v√† points-to)
        output = run_mapreduce_job(mapper, reducer, input_data=current_distances)
        
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng (Termination)
        if output == current_distances:
            print("Kho·∫£ng c√°ch kh√¥ng thay ƒë·ªïi. Thu·∫≠t to√°n h·ªôi t·ª•.")
            break
            
        current_distances = output
        iteration += 1
        
    return current_distances
```

---

## 3. Blow-up and Termination (Ph√°t tri·ªÉn v√† ƒêi·ªÅu ki·ªán d·ª´ng)

### Ph√¢n t√≠ch
*   **Blow-up (Ph√°t tri·ªÉn):** Thu·∫≠t to√°n b·∫Øt ƒë·∫ßu t·ª´ m·ªôt n√∫t. ·ªû c√°c iteration sau, s·ªë l∆∞·ª£ng n√∫t trong frontier (bi√™n gi·ªõi) tƒÉng l√™n. MapReduce s·∫Ω x·ª≠ l√Ω song song t·∫•t c·∫£ c√°c n√∫t n√†y.
*   **Termination (ƒêi·ªÅu ki·ªán d·ª´ng):**
    *   Thu·∫≠t to√°n s·∫Ω d·ª´ng khi kh√¥ng c√≤n t√¨m th·∫•y c√°c l·ªô tr√¨nh m·ªõi ho·∫∑c kh√¥ng c√≤n c·∫£i thi·ªán ƒë∆∞·ª£c kho·∫£ng c√°ch.
    *   **Logic:** N·∫øu kho·∫£ng c√°ch t√≠nh ƒë∆∞·ª£c t·∫°i iteration m·ªõi l·ªõn h∆°n ho·∫∑c b·∫±ng kho·∫£ng c√°ch ƒëang c√≥, ta kh√¥ng c·∫≠p nh·∫≠t.
    *   **L∆∞u √Ω quan tr·ªçng:** Mapper c·∫ßn emit `(n, D)` (kho·∫£ng c√°ch hi·ªán t·∫°i c·ªßa n√∫t `n`) ƒë·ªÉ ƒë·∫£m b·∫£o Reducer c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß so s√°nh v·ªõi kho·∫£ng c√°ch m·ªõi.

### Code M·∫´u: C·∫≠p nh·∫≠t tr·∫°ng th√°i (Reducer v·ªõi Termination Logic)

```python
def reducer_with_termination(node_id, values):
    current_dist = float('inf')
    graph_structure = []
    
    # Ph√¢n lo·∫°i input
    for val_type, val in values:
        if val_type == 'current_dist':
            current_dist = val # Kho·∫£ng c√°ch ƒëang c√≥ t·ª´ iteration tr∆∞·ªõc
        elif val_type == 'new_dist':
            if val < current_dist:
                current_dist = val # C·∫≠p nh·∫≠t n·∫øu t√¨m th·∫•y ƒë∆∞·ªùng ƒëi ng·∫Øn h∆°n
        elif val_type == 'graph_info':
            graph_structure = val
            
    emit(node_id, (current_dist, graph_structure))
```

---

## 4. Adding weights (X·ª≠ l√Ω ƒë·ªì th·ªã c√≥ tr·ªçng s·ªë)

### Kh√°i ni·ªám
B√†i to√°n th·ª±c t·∫ø th∆∞·ªùng l√† **Weighted Shortest Path** (T√¨m ƒë∆∞·ªùng ng·∫Øn nh·∫•t c√≥ tr·ªçng s·ªë) thay v√¨ ch·ªâ ƒë·∫øm s·ªë b∆∞·ªõc (cost=1).

### Thay ƒë·ªïi thu·∫≠t to√°n
*   **Input:** `points-to` kh√¥ng ch·ªâ l√† danh s√°ch n√∫t, m√† l√† danh s√°ch c√°c c·∫∑p `(node, weight)`.
*   **C√¥ng th·ª©c:** Thay v√¨ `D + 1`, ta t√≠nh `D + w` (tr·ªçng s·ªë `w` c·ªßa c·∫°nh).
*   **ƒêi·ªÅu ki·ªán:** Ch·ªâ ho·∫°t ƒë·ªông ch√≠nh x√°c v·ªõi ƒë·ªì th·ªã c√≥ **tr·ªçng s·ªë d∆∞∆°ng** (positive-weighted graph).

### Code M·∫´u: H·ªó tr·ª£ tr·ªçng s·ªë

```python
def mapper_weighted(node_id, value):
    distance, neighbors = value # neighbors l√† list of (neighbor_node, weight)
    
    emit(node_id, ('graph_info', neighbors))
    
    for neighbor, weight in neighbors:
        new_distance = distance + weight # S·ª≠ d·ª•ng tr·ªçng s·ªë
        emit(neighbor, ('distance', new_distance))

def reducer_weighted(node_id, values):
    # Logic t∆∞∆°ng t·ª± nh∆∞ reducer c∆° b·∫£n
    min_distance = float('inf')
    points_to = []
    
    for val_type, val in values:
        if val_type == 'graph_info':
            points_to = val
        elif val_type == 'distance':
            if val < min_distance:
                min_distance = val
                
    emit(node_id, (min_distance, points_to))
```

---

## 5. Comparison to Dijkstra (So s√°nh v·ªõi thu·∫≠t to√°n Dijkstra)

ƒê√¢y l√† ph·∫ßn so s√°nh quan tr·ªçng gi·ªØa c√°ch ti·∫øp c·∫≠n MapReduce v√† thu·∫≠t to√°n chu·∫©n (Dijkstra).

| Ti√™u ch√≠ | Dijkstra (Thu·∫≠t to√°n ƒë∆°n m√°y) | MapReduce (Ph√¢n t√°n) |
| :--- | :--- | :--- |
| **C√°ch ho·∫°t ƒë·ªông** | **Greedy (Tham lam):** Lu√¥n ∆∞u ti√™n m·ªü r·ªông t·ª´ n√∫t c√≥ chi ph√≠ th·∫•p nh·∫•t trong frontier hi·ªán t·∫°i. | **Song song to√†n b·ªô:** Kh√°m ph√° t·∫•t c·∫£ c√°c ƒë∆∞·ªùng ƒëi trong m·ªôt b∆∞·ªõc (hop) song song tr√™n nhi·ªÅu m√°y. |
| **Hi·ªáu qu·∫£ (Efficiency)** | **Cao h∆°n:** Tr√°nh ƒë∆∞·ª£c vi·ªác t√≠nh to√°n c√°c ƒë∆∞·ªùng ƒëi kh√¥ng c·∫ßn thi·∫øt. | **Th·∫•p h∆°n v·ªÅ m·∫∑t t√≠nh to√°n (Overhead):** T√≠nh to√°n nhi·ªÅu ƒë∆∞·ªùng ƒëi "v√¥ √≠ch" nh∆∞ng ƒë·ªïi l·∫°i t·ªëc ƒë·ªô x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn nhanh h∆°n. |
| **Kh·∫£ nƒÉng m·ªü r·ªông (Scalability)** | **H·∫°n ch·∫ø:** Ph·ª• thu·ªôc v√†o b·ªô nh·ªõ c·ªßa m·ªôt m√°y (RAM) ƒë·ªÉ l∆∞u tr·ªØ tr·∫°ng th√°i. | **R·∫•t cao:** C√≥ th·ªÉ x·ª≠ l√Ω ƒë·ªì th·ªã l·ªõn h∆°n nhi·ªÅu so v·ªõi b·ªô nh·ªõ c·ªßa m·ªôt m√°y, ph√¢n b·ªï l√™n h√†ng ngh√¨n node. |
| **Tr∆∞·ªùng h·ª£p Weight=1** | T∆∞∆°ng ƒë∆∞∆°ng v·ªÅ k·∫øt qu·∫£. | T∆∞∆°ng ƒë∆∞∆°ng v·ªÅ k·∫øt qu·∫£. |

### Khi n√†o s·ª≠ d·ª•ng?
*   **S·ª≠ d·ª•ng Dijkstra:** Khi d·ªØ li·ªáu v·ª´a v√† nh·ªè, ho·∫∑c y√™u c·∫ßu ƒë·ªô tr·ªÖ th·∫•p, ch·∫°y tr√™n m·ªôt m√°y m·∫°nh.
*   **S·ª≠ d·ª•ng MapReduce/BFS Song song:** Khi d·ªØ li·ªáu ƒë·ªì th·ªã qu√° l·ªõn (V√≠ d·ª•: M·∫°ng x√£ h·ªôi, Graph web) kh√¥ng th·ªÉ ch·ª©a trong b·ªô nh·ªõ c·ªßa m·ªôt m√°y.

---

## 6. T√≥m t·∫Øt & H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

### Khi n√†o s·ª≠ d·ª•ng thu·∫≠t to√°n MapReduce cho Graph Processing?
*   Khi b·∫°n c·∫ßn x·ª≠ l√Ω c√°c b√†i to√°n **PageRank**, **Connected Components**, ho·∫∑c **Shortest Path** tr√™n c√°c ƒë·ªì th·ªã kh·ªïng l·ªì (h√†ng t·ª∑ c·∫°nh).
*   Khi d·ªØ li·ªáu n·∫±m r·∫£i r√°c tr√™n nhi·ªÅu m√°y (HDFS).

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?
1.  **Chu·∫©n b·ªã d·ªØ li·ªáu:** D·∫°ng h√≥a d·ªØ li·ªáu ƒë·ªì th·ªã th√†nh c·∫∑p `(Node, Danh s√°ch l√°ng gi·ªÅng)`.
2.  **Vi·∫øt Mapper:** Nh·∫≠n Node, emit `(L√°ng gi·ªÅng, Kho·∫£ng c√°ch)`.
3.  **Vi·∫øt Reducer:** Nh·∫≠n t·∫•t c·∫£ kho·∫£ng c√°ch ƒë·∫øn c√πng m·ªôt Node, ch·ªçn Min, gi·ªØ l·∫°i c·∫•u tr√∫c ƒë·ªì th·ªã.
4.  **Vi·∫øt Driver:** V√≤ng l·∫∑p g·ªçi MapReduce Job cho ƒë·∫øn khi d·ªØ li·ªáu ·ªïn ƒë·ªãnh.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm
*   **∆Øu ƒëi·ªÉm:**
    *   **T√≠nh m·ªü r·ªông (Scalability):** X·ª≠ l√Ω ƒë∆∞·ª£c ƒë·ªì th·ªã l·ªõn h∆°n RAM c·ªßa 1 m√°y.
    *   **Tolerance (Ch·ªãu l·ªói):** MapReduce t·ª± ƒë·ªông x·ª≠ l√Ω n·∫øu m·ªôt node t√≠nh to√°n b·ªã l·ªói.
*   **Nh∆∞·ª£c ƒëi·ªÉm:**
    *   **High Latency (ƒê·ªô tr·ªÖ cao):** Ph·∫£i ƒë·ªçc/ghi d·ªØ li·ªáu ra ƒëƒ©a sau m·ªói v√≤ng l·∫∑p (I/O cost).
    *   **Kh√¥ng t·ªëi ∆∞u:** T·ªën t√†i nguy√™n t√≠nh to√°n cho c√°c ƒë∆∞·ªùng ƒëi kh√¥ng shortest (kh√°c v·ªõi Dijkstra).

---

Ch√†o b·∫°n, v·ªõi vai tr√≤ l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n, t√¥i s·∫Ω ph√¢n t√≠ch v√† tr√¨nh b√†y l·∫°i n·ªôi dung v·ªÅ **PageRank** t·ª´ t√†i li·ªáu slide c·ªßa b·∫°n m·ªôt c√°ch chi ti·∫øt, chuy√™n nghi·ªáp v√† d·ªÖ hi·ªÉu d∆∞·ªõi d·∫°ng Markdown.

---

# PageRank: Thu·∫≠t to√°n ƒê√°nh gi√° Quan tr·ªçng c·ªßa Web

PageRank l√† m·ªôt thu·∫≠t to√°n ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Google ƒë·ªÉ x·∫øp h·∫°ng c√°c trang web d·ª±a tr√™n c·∫•u tr√∫c li√™n k·∫øt c·ªßa ch√∫ng. N√≥ m√¥ ph·ªèng h√†nh vi c·ªßa m·ªôt "ng∆∞·ªùi d√πng ng·∫´u nhi√™n" duy·ªát web ƒë·ªÉ x√°c ƒë·ªãnh m·ª©c ƒë·ªô quan tr·ªçng c·ªßa m·ªôt trang web.

## 1. Kh√°i ni·ªám c∆° b·∫£n (The Concept)

### Gi·∫£i th√≠ch
Thu·∫≠t to√°n n√†y d·ª±a tr√™n gi·∫£ ƒë·ªãnh r·∫±ng m·ªôt trang web c√†ng c√≥ nhi·ªÅu li√™n k·∫øt tr·ªè ƒë·∫øn (backlinks) t·ª´ c√°c trang web quan tr·ªçng kh√°c th√¨ n√≥ c√†ng quan tr·ªçng.

*   **Random Walk (Cu·ªôc ƒëi b·ªô ng·∫´u nhi√™n):** M√¥ ph·ªèng vi·ªác ng∆∞·ªùi d√πng b·∫Øt ƒë·∫ßu t·ª´ m·ªôt trang web b·∫•t k·ª≥ v√† li√™n t·ª•c nh·∫•p v√†o c√°c li√™n k·∫øt ng·∫´u nhi√™n tr√™n trang ƒë√≥.
*   **Damping Factor (H·ªá s·ªë ma s√°t):** Ng∆∞·ªùi d√πng c√≥ th·ªÉ c·∫£m th·∫•y m·ªát m·ªèi v√† quy·∫øt ƒë·ªãnh g√µ m·ªôt URL ho√†n to√†n m·ªõi v√†o thanh ƒë·ªãa ch·ªâ thay v√¨ nh·∫•p li√™n k·∫øt. X√°c su·∫•t n√†y ƒë∆∞·ª£c g·ªçi l√† "random jump".

### C√¥ng th·ª©c To√°n h·ªçc
ƒêi·ªÉm PageRank c·ªßa m·ªôt trang A ƒë∆∞·ª£c t√≠nh nh∆∞ sau:

$$PR(A) = (1 - d) + d \left( \frac{PR(T_1)}{C(T_1)} + \dots + \frac{PR(T_n)}{C(T_n)} \right)$$

**Trong ƒë√≥:**
*   $PR(A)$: ƒêi·ªÉm PageRank c·ªßa trang A.
*   $T_1, \dots, T_n$: C√°c trang web li√™n k·∫øt tr·ªè ƒë·∫øn trang A.
*   $PR(T_i)$: ƒêi·ªÉm PageRank c·ªßa trang tr·ªè ngu·ªìn $T_i$.
*   $C(T_i)$: S·ªë l∆∞·ª£ng li√™n k·∫øt outbound (out-degree) c·ªßa trang $T_i$.
*   $d$: **Damping factor** (th∆∞·ªùng thi·∫øt l·∫≠p l√† 0.85), ƒë·∫°i di·ªán cho x√°c su·∫•t ng∆∞·ªùi d√πng ti·∫øp t·ª•c nh·∫•p chu·ªôt.

### Logic v·∫≠n h√†nh (Intuition)
*   **T√≠nh to√°n l·∫∑p l·∫°i (Iterative):** ƒêi·ªÉm PR c·ªßa trang A ph·ª• thu·ªôc v√†o ƒëi·ªÉm PR c·ªßa c√°c trang tr·ªè ƒë·∫øn n√≥. Do ƒë√≥, ta ph·∫£i ch·∫°y thu·∫≠t to√°n qua nhi·ªÅu v√≤ng (iteration) cho ƒë·∫øn khi c√°c gi√° tr·ªã PR ·ªïn ƒë·ªãnh (converged).
*   **Ph√¢n ph·ªëi ƒëi·ªÉm:** ·ªû m·ªói v√≤ng, m·ªôt trang s·∫Ω "chia s·∫ª" ƒëi·ªÉm PR c·ªßa n√≥ cho c√°c trang m√† n√≥ li√™n k·∫øt ƒë·∫øn.
*   **C√¥ng th·ª©c:**
    *   M·ªôt ph·∫ßn ƒëi·ªÉm s·ªë ƒë∆∞·ª£c gi·ªØ l·∫°i (t·ª∑ l·ªá $1-d$).
    *   M·ªôt ph·∫ßn ƒëi·ªÉm s·ªë ƒë∆∞·ª£c chia ƒë·ªÅu cho c√°c li√™n k·∫øt outbound (t·ª∑ l·ªá $d$).

---

## 2. Ph√¢n t√≠ch & C·∫£i thi·ªán Code (Code Extraction & Improvement)

T√†i li·ªáu slide cung c·∫•p pseudo-code cho b∆∞·ªõc ƒë·∫ßu ti√™n c·ªßa thu·∫≠t to√°n. D∆∞·ªõi ƒë√¢y l√† ph·∫ßn tr√≠ch xu·∫•t v√† vi·∫øt l·∫°i chi ti·∫øt.

### Pseudo-code g·ªëc t·ª´ slide
Slide m√¥ t·∫£ logic c∆° b·∫£n:
1.  T·∫°o hai b·∫£ng `current` v√† `next` ch·ª©a gi√° tr·ªã PR.
2.  Kh·ªüi t·∫°o `current` v·ªõi c√°c gi√° tr·ªã ban ƒë·∫ßu.
3.  L·∫∑p qua c√°c trang, ph√¢n ph·ªëi PR t·ª´ `current` sang `next` c·ªßa c√°c trang ƒë∆∞·ª£c li√™n k·∫øt.
4.  G√°n `current = next`, t·∫°o `next` m·ªõi.
5.  L·∫∑p l·∫°i cho ƒë·∫øn khi h·ªôi t·ª•.

### Code M·∫´u (Python Implementation)
D∆∞·ªõi ƒë√¢y l√† c√°ch hi·ªán th·ª±c h√≥a logic n√†y b·∫±ng Python m·ªôt c√°ch ho√†n ch·ªânh. Trong th·ª±c t·∫ø, ta th∆∞·ªùng d√πng Dictionary ƒë·ªÉ bi·ªÉu di·ªÖn ƒë·ªì th·ªã v√† ƒëi·ªÉm s·ªë.

```python
def calculate_pagerank(graph, damping_factor=0.85, iterations=100, tolerance=1.0e-6):
    """
    T√≠nh PageRank cho c√°c node trong ƒë·ªì th·ªã.
    
    Args:
        graph (dict): ƒê·ªì th·ªã d·∫°ng {node: [list_of_outgoing_nodes]}
        damping_factor (float): H·ªá s·ªë ma s√°t (d)
        iterations (int): S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa
        tolerance (float): Ng∆∞·ª°ng h·ªôi t·ª•
    
    Returns:
        dict: B·∫£ng ƒëi·ªÉm PageRank c·ªßa c√°c node
    """
    # 1. Kh·ªüi t·∫°o (Seed 'current' with initial PR values)
    num_pages = len(graph)
    # B·∫Øt ƒë·∫ßu v·ªõi gi√° tr·ªã 1.0 cho t·∫•t c·∫£ trang
    current_pr = {node: 1.0 / num_pages for node in graph}
    
    # T√≠nh to√°n l·∫∑p l·∫°i
    for i in range(iterations):
        next_pr = {}
        
        # T√≠nh to√°n ph·∫ßn (1-d) / N
        # ƒê√¢y l√† ph·∫ßn ƒëi·ªÉm s·ªë ng·∫´u nhi√™n (random jump) ƒë∆∞·ª£c chia ƒë·ªÅu
        random_jump = (1 - damping_factor) / num_pages
        
        for node in graph:
            # B·∫Øt ƒë·∫ßu v·ªõi random jump
            rank = random_jump
            
            # T√≠nh ph·∫ßn ph√¢n ph·ªëi t·ª´ c√°c trang tr·ªè ƒë·∫øn (Inbound links)
            # Trong slide: current := next, nh∆∞ng logic th·ª±c l√† t√≠nh d·ª±a tr√™n current
            # Duy·ªát qua t·∫•t c·∫£ c√°c node kh√°c ƒë·ªÉ xem ch√∫ng c√≥ tr·ªè ƒë·∫øn node hi·ªán t·∫°i kh√¥ng
            # (ƒê·ªÉ t·ªëi ∆∞u, ta n√™n build reverse graph, nh∆∞ng ·ªü ƒë√¢y minh h·ªça logic tr·ª±c quan)
            
            # Logic slide: "Iterate over all pages... distributing PR from 'current' into 'next' of linkees"
            # C√°ch ti·∫øp c·∫≠n chu·∫©n (Forward approach):
            # Node hi·ªán t·∫°i s·∫Ω nh·∫≠n PR t·ª´ c√°c node tr·ªè ƒë·∫øn n√≥.
            
            # Tuy nhi√™n, ƒë·ªÉ code ch·∫°y ƒë√∫ng v·ªõi graph input (outgoing), ta c·∫ßn reverse logic ho·∫∑c build reverse graph
            # D∆∞·ªõi ƒë√¢y l√† c√°ch t√≠nh theo c√¥ng th·ª©c chu·∫©n:
            # PR(A) = (1-d)/N + d * sum(PR(T)/C(T))
            
            pass # Logic n√†y c·∫ßn reverse graph, xem b√™n d∆∞·ªõi

# --- C√ÅCH HI·ªÜN TH·ª∞C CHU·∫®N H∆†N V·ªöI REVERSE GRAPH ---
def pagerank_optimized(graph, d=0.85, max_iter=100, tol=1e-6):
    # 1. Build Reverse Graph (Ma tr·∫≠n li√™n k·∫øt ng∆∞·ª£c)
    # ƒê·ªÉ bi·∫øt trang A nh·∫≠n PR t·ª´ nh·ªØng trang n√†o
    reverse_graph = {}
    for node, outlinks in graph.items():
        for link in outlinks:
            if link not in reverse_graph:
                reverse_graph[link] = []
            reverse_graph[link].append(node)
            
    # 2. Kh·ªüi t·∫°o
    N = len(graph)
    pr = {node: 1.0 / N for node in graph}
    
    # 3. V√≤ng l·∫∑p (Iteration)
    for _ in range(max_iter):
        new_pr = {}
        diff = 0
        
        for node in graph:
            # T√≠nh gi√° tr·ªã m·ªõi
            # PR(A) = (1-d)/N + d * sum(PR(T)/C(T))
            inbound_sum = 0
            
            # L·∫•y danh s√°ch c√°c trang tr·ªè ƒë·∫øn node hi·ªán t·∫°i
            inlinks = reverse_graph.get(node, [])
            
            for neighbor in inlinks:
                # neighbor l√† T, n√≥ tr·ªè ƒë·∫øn node (A)
                # C(neighbor) l√† s·ªë outbound links c·ªßa neighbor
                outbound_count = len(graph[neighbor])
                if outbound_count > 0:
                    inbound_sum += pr[neighbor] / outbound_count
            
            new_val = (1 - d) / N + d * inbound_sum
            new_pr[node] = new_val
            
            # T√≠nh ƒë·ªô sai l·ªách ƒë·ªÉ ki·ªÉm tra h·ªôi t·ª•
            diff += abs(new_val - pr[node])
            
        pr = new_pr
        
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng (Convergence)
        if diff < tol:
            print(f"H·ªôi t·ª• sau v√≤ng l·∫∑p {_ + 1}")
            break
            
    return pr

# V√≠ d·ª• d·ªØ li·ªáu (Graph)
web_graph = {
    'A': ['B', 'C'],
    'B': ['C'],
    'C': ['A'],
    'D': ['C']
}

# Ch·∫°y th·ª≠
final_pr = pagerank_optimized(web_graph)
print(final_pr)
```

---

## 3. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng & Ph√¢n t√≠ch (Usage Guide)

### Khi n√†o s·ª≠ d·ª•ng? (Use Cases)
*   **Search Engines (C√¥ng c·ª• t√¨m ki·∫øm):** X·∫øp h·∫°ng k·∫øt qu·∫£ t√¨m ki·∫øm ƒë·ªÉ trang quan tr·ªçng nh·∫•t hi·ªÉn th·ªã ƒë·∫ßu ti√™n.
*   **Recommendation Systems:** ƒê·ªÅ xu·∫•t c√°c b√†i vi·∫øt/blog li√™n quan d·ª±a tr√™n c·∫•u tr√∫c li√™n k·∫øt n·ªôi dung.
*   **Social Network Analysis:** T√¨m ng∆∞·ªùi c√≥ ·∫£nh h∆∞·ªüng (Influencers) trong m·∫°ng x√£ h·ªôi (Twitter Retweet graph, Facebook Like graph).
*   **Ph√¢n t√≠ch h·ªá th·ªëng sinh h·ªçc:** X√°c ƒë·ªãnh c√°c protein quan tr·ªçng trong m·∫°ng l∆∞·ªõi t∆∞∆°ng t√°c protein.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o? (How to use)
1.  **X√¢y d·ª±ng ƒë·ªì th·ªã (Graph Construction):** Thu th·∫≠p d·ªØ li·ªáu li√™n k·∫øt (URL tr·ªè ƒë·∫øn URL n√†o).
2.  **Chu·∫©n h√≥a d·ªØ li·ªáu:** Lo·∫°i b·ªè c√°c li√™n k·∫øt v√≤ng (self-loops) ho·∫∑c li√™n k·∫øt n·ªôi b·ªô kh√¥ng c√≥ √Ω nghƒ©a.
3.  **Ch·∫°y thu·∫≠t to√°n l·∫∑p:** √Åp d·ª•ng c√¥ng th·ª©c to√°n h·ªçc ·ªü tr√™n, l·∫∑p l·∫°i cho ƒë·∫øn khi gi√° tr·ªã PR thay ƒë·ªïi kh√¥ng ƒë√°ng k·ªÉ (d∆∞·ªõi ng∆∞·ª°ng tolerance).
4.  **L∆∞u tr·ªØ:** L∆∞u k·∫øt qu·∫£ v√†o c∆° s·ªü d·ªØ li·ªáu (Cassandra, HBase) ƒë·ªÉ ph·ª•c v·ª• truy v·∫•n nhanh.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| Ti√™u ch√≠ | ∆Øu ƒëi·ªÉm (Pros) | Nh∆∞·ª£c ƒëi·ªÉm (Cons) |
| :--- | :--- | :--- |
| **T√≠nh kh√°ch quan** | D·ª±a tr√™n c·∫•u tr√∫c li√™n k·∫øt th·ª±c t·∫ø, kh√≥ thao t√∫ng h∆°n c√°c ch·ªâ s·ªë meta tag. | D·ªÖ b·ªã thao t√∫ng b·ªüi c√°c "li√™n k·∫øt ma" (Link farms) n·∫øu kh√¥ng c√≥ b·ªô l·ªçc. |
| **T√≠nh lan truy·ªÅn** | Ph√¢n ph·ªëi ƒëi·ªÉm s·ªë t·ª± nhi√™n t·ª´ trang quan tr·ªçng sang trang √≠t quan tr·ªçng h∆°n. | **T√≠nh to√°n ch·∫≠m:** V·ªõi ƒë·ªì th·ªã l·ªõn (h√†ng t·ª∑ trang), vi·ªác l·∫∑p l·∫°i r·∫•t t·ªën t√†i nguy√™n. |
| **Kh·∫£ nƒÉng m·ªü r·ªông** | Ph√π h·ª£p v·ªõi m√¥i tr∆∞·ªùng ph√¢n t√°n (MapReduce/Spark) v√¨ m·ªói trang t√≠nh to√°n ƒë·ªôc l·∫≠p. | **Cold Start:** Kh√¥ng th·ªÉ t√≠nh PR cho c√°c trang m·ªõi (kh√¥ng c√≥ li√™n k·∫øt v√†o). |

---

## 4. V√≠ d·ª• Th·ª±c t·∫ø trong Industry

### V√≠ d·ª• 1: Google Search (Th·ªùi k·ª≥ ƒë·∫ßu)
Tr∆∞·ªõc ƒë√¢y, Google s·ª≠ d·ª•ng PageRank thu·∫ßn t√∫y ƒë·ªÉ quy·∫øt ƒë·ªãnh trang web n√†o c√≥ th·ª© h·∫°ng cao khi b·∫°n t√¨m ki·∫øm "Th·ªùi ti·∫øt H√† N·ªôi". Trang web c·ªßa VnExpress c√≥ th·ªÉ c√≥ PR cao h∆°n m·ªôt blog c√° nh√¢n v√¨ c√≥ nhi·ªÅu trang kh√°c li√™n k·∫øt ƒë·∫øn n√≥.

### V√≠ d·ª• 2: Twitter "Who to Follow"
Twitter s·ª≠ d·ª•ng bi·∫øn th·ªÉ c·ªßa PageRank (Personalized PageRank) ƒë·ªÉ x√°c ƒë·ªãnh b·∫°n n√™n theo d√µi ai.
*   **Logic:** N·∫øu b·∫°n theo d√µi A, v√† A theo d√µi B, B c√≥ kh·∫£ nƒÉng ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t cho b·∫°n cao.
*   **C·∫£i ti·∫øn:** Thay v√¨ ch·ªâ t√≠nh to√°n tr√™n to√†n b·ªô Twitter, h·ªç t√≠nh to√°n d·ª±a tr√™n "h√†nh lang" (neighborhood) c·ªßa b·∫°n ƒë·ªÉ gi·∫£m t·∫£i.

### V√≠ d·ª• 3: Ph√¢n t√≠ch gian l·∫≠n t√≠n d·ª•ng
C√°c c√¥ng ty fintech s·ª≠ d·ª•ng ƒë·ªì th·ªã li√™n k·∫øt ƒë·ªÉ ph√°t hi·ªán gian l·∫≠n.
*   N·∫øu m·ªôt nh√≥m ng∆∞·ªùi d√πng (nodes) li√™n k·∫øt v·ªõi nhau (chuy·ªÉn ti·ªÅn qua l·∫°i) nh∆∞ng kh√¥ng c√≥ li√™n k·∫øt ra ngo√†i m·∫°ng l∆∞·ªõi (kh√¥ng giao d·ªãch v·ªõi ng∆∞·ªùi d√πng uy t√≠n), h·ªç c√≥ th·ªÉ b·ªã ƒë√°nh d·∫•u l√† gian l·∫≠n d·ª±a tr√™n ƒë·ªô "t√°ch bi·ªát" trong ƒë·ªì th·ªã PageRank.

---

## 5. T√≥m t·∫Øt (Summary)

PageRank l√† n·ªÅn t·∫£ng c·ªßa c√°c h·ªá th·ªëng ph√¢n t√≠ch ƒë·ªì th·ªã l·ªõn. M·∫∑c d√π Google ƒë√£ chuy·ªÉn sang c√°c thu·∫≠t to√°n ph·ª©c t·∫°p h∆°n (nh∆∞ RankBrain, BERT), nh∆∞ng nguy√™n l√Ω c∆° b·∫£n c·ªßa PageRank v·ªÅ vi·ªác **ph√¢n ph·ªëi uy t√≠n qua c√°c li√™n k·∫øt** v·∫´n l√† ki·∫øn th·ª©c n·ªÅn t·∫£ng b·∫Øt bu·ªôc trong ng√†nh Big Data v√† AI.

---

Ch√†o b·∫°n, t√¥i l√† m·ªôt chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "6_mapreduce.pdf" c·ªßa b·∫°n, ƒë∆∞·ª£c tr√¨nh b√†y l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát theo y√™u c·∫ßu.

---

# Ph√¢n t√≠ch thu·∫≠t to√°n PageRank tr√™n Hadoop MapReduce

T√†i li·ªáu n√†y m√¥ t·∫£ c√°ch th·ª©c ph√¢n ph·ªëi v√† th·ª±c thi thu·∫≠t to√°n PageRank s·ª≠ d·ª•ng m√¥ h√¨nh l·∫≠p tr√¨nh MapReduce. PageRank l√† m·ªôt thu·∫≠t to√°n ƒë·ªì th·ªã ph·ª©c t·∫°p, nh∆∞ng c√°c insight (b·∫£n ch·∫•t) c·ªßa n√≥ cho ph√©p n√≥ ƒë∆∞·ª£c song song h√≥a m·ªôt c√°ch hi·ªáu qu·∫£.

## 1. Ph√¢n t√≠ch & C·∫£i thi·ªán Code

Trong slide kh√¥ng cung c·∫•p code c·ª• th·ªÉ m√† ch·ªâ l√† c√°c b∆∞·ªõc logic (pseudo-code). D∆∞·ªõi ƒë√¢y l√† ph·∫ßn tr√≠ch xu·∫•t v√† vi·∫øt l·∫°i c√°c ƒëo·∫°n code m·∫´u t∆∞∆°ng ·ª©ng v·ªõi c√°c Phase (Giai ƒëo·∫°n) ƒë∆∞·ª£c m√¥ t·∫£.

### Phase 1: Parse HTML (Ph√¢n t√≠ch HTML)

**Logic t·ª´ Slide:**
*   **Input:** `(URL, page content)`
*   **Map:** Tr√≠ch xu·∫•t danh s√°ch c√°c URL li√™n k·∫øt (outgoing links) t·ª´ n·ªôi dung trang. G√°n PageRank ban ƒë·∫ßu (PRinit).
*   **Reduce:** Identity function (Ch·ªâ chuy·ªÉn ti·∫øp d·ªØ li·ªáu).

**Code M·∫´u (Python - Hadoop Streaming):**

```python
# Mapper: parse_html_mapper.py
import sys
import json

def main():
    PR_INIT = 1.0  # PageRank ban ƒë·∫ßu
    
    for line in sys.stdin:
        # Gi·∫£ ƒë·ªãnh input: URL<TAB>HTML_Content
        url, html_content = line.strip().split('\t', 1)
        
        # Logic tr√≠ch xu·∫•t links (ƒë∆°n gi·∫£n h√≥a)
        # Trong th·ª±c t·∫ø d√πng BeautifulSoup ho·∫∑c Regex
        links = extract_links(html_content) 
        
        # Emit: (URL, (PR_init, list_of_urls))
        # Format: URL \t (PR_init, links_json)
        print(f"{url}\t{json.dumps({'pr': PR_INIT, 'links': links})}")

def extract_links(content):
    # Mock function
    return ["http://siteA.com", "http://siteB.com"]

if __name__ == "__main__":
    main()
```

### Phase 2: PageRank Distribution (Ph√¢n ph·ªëi PageRank)

**Logic t·ª´ Slide:**
*   **Input:** `(URL, (cur_rank, url_list))`
*   **Map:** V·ªõi m·ªói link trong `url_list`, emit `(link_target, contribution)`. ƒê·ªìng th·ªùi emit `(URL, url_list)` ƒë·ªÉ gi·ªØ c·∫•u tr√∫c ƒë·ªì th·ªã cho v√≤ng l·∫∑p ti·∫øp theo.
*   **Reduce:** T√≠nh to√°n `new_rank = (1-d) + d * sum(contributions)`. Emit `(URL, (new_rank, url_list))`.

**Code M·∫´u (Python - Hadoop Streaming):**

```python
# Mapper: pagerank_mapper.py
import sys
import json

D = 0.85 # Damping factor

def main():
    for line in sys.stdin:
        url, data_str = line.strip().split('\t', 1)
        data = json.loads(data_str)
        
        cur_rank = data['pr']
        url_list = data['links']
        
        # 1. Emit contributions to children
        if url_list:
            contribution = cur_rank / len(url_list)
            for link in url_list:
                # Emit (Target_URL, contribution_value)
                print(f"{link}\t{json.dumps({'type': 'contrib', 'val': contribution})}")
        
        # 2. Emit the graph structure (to carry over to next iteration)
        # Emit (URL, graph_structure)
        print(f"{url}\t{json.dumps({'type': 'graph', 'links': url_list})}")

# Reducer: pagerank_reducer.py
import sys
import json

D = 0.85

def main():
    current_url = None
    accumulated_rank = 0.0
    url_list = []
    
    for line in sys.stdin:
        url, val_str = line.strip().split('\t', 1)
        val = json.loads(val_str)
        
        if url != current_url:
            if current_url:
                # Calculate new rank
                new_rank = (1 - D) + D * accumulated_rank
                print(f"{current_url}\t{json.dumps({'pr': new_rank, 'links': url_list})}")
            
            current_url = url
            accumulated_rank = 0.0
            url_list = []
        
        if val['type'] == 'contrib':
            accumulated_rank += val['val']
        elif val['type'] == 'graph':
            url_list = val['links']
            
    # Output for the last key
    if current_url:
        new_rank = (1 - D) + D * accumulated_rank
        print(f"{current_url}\t{json.dumps({'pr': new_rank, 'links': url_list})}")

if __name__ == "__main__":
    main()
```

---

## 2. Gi·∫£i th√≠ch Kh√°i ni·ªám (Concept Explanation)

### A. Parallelization Insights (C√°c insight cho ph√©p song song h√≥a)

ƒê√¢y l√† n·ªÅn t·∫£ng l√Ω thuy·∫øt ƒë·ªÉ MapReduce c√≥ th·ªÉ x·ª≠ l√Ω PageRank.

1.  **Dependency Isolation (Ph·ª• thu·ªôc c√¥ l·∫≠p):**
    *   **Gi·∫£i th√≠ch:** Gi√° tr·ªã PageRank m·ªõi (`next` table) c·ªßa m·ªôt trang A ch·ªâ ph·ª• thu·ªôc v√†o c√°c trang li√™n k·∫øt ƒë·∫øn A (in-links), v√† kh√¥ng ph·ª• thu·ªôc tr·ª±c ti·∫øp v√†o gi√° tr·ªã PageRank m·ªõi c·ªßa c√°c trang kh√°c trong c√πng m·ªôt l∆∞·ª£t t√≠nh to√°n.
    *   **T·∫ßm quan tr·ªçng:** ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† ch√∫ng ta kh√¥ng c·∫ßn ƒë·ª£i to√†n b·ªô m·∫°ng l∆∞·ªõi ƒë∆∞·ª£c t√≠nh to√°n xong m·ªõi bi·∫øt ƒë∆∞·ª£c gi√° tr·ªã c·ªßa A. Ch√∫ng ta c√≥ th·ªÉ t√≠nh to√°n ƒë√≥ng g√≥i (batch).

2.  **Row-level Parallelism (Song song h√≥a theo h√†ng):**
    *   **Gi·∫£i th√≠ch:** Ma tr·∫≠n k·ªÅ (Adjacency Matrix) c·ªßa ƒë·ªì th·ªã web c√≥ th·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω song song theo t·ª´ng h√†ng (t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng trang web).
    *   **T·∫ßm quan tr·ªçng:** M·ªói Mapper c√≥ th·ªÉ x·ª≠ l√Ω m·ªôt ph·∫ßn c·ªßa ƒë·ªì th·ªã m√† kh√¥ng c·∫ßn d·ªØ li·ªáu to√†n c·ª•c.

3.  **Sparsity (T√≠nh th∆∞a):**
    *   **Gi·∫£i th√≠ch:** Ma tr·∫≠n k·ªÅ web r·∫•t th∆∞a (sparse), t·ª©c l√† m·ªôt trang ch·ªâ li√™n k·∫øt ƒë·∫øn m·ªôt ph·∫ßn r·∫•t nh·ªè trong h√†ng t·ª∑ trang web kh√°c.
    *   **T·∫ßm quan tr·ªçng:** Thay v√¨ l∆∞u ma tr·∫≠n kh·ªïng l·ªì, ta ch·ªâ c·∫ßn l∆∞u c√°c c·∫∑p `(source, target)`. ƒêi·ªÅu n√†y gi·∫£m ƒë√°ng k·ªÉ l∆∞·ª£ng d·ªØ li·ªáu c·∫ßn truy·ªÅn qua m·∫°ng (I/O).

### B. MapReduce Implementation (Tri·ªÉn khai MapReduce)

*   **Map Step (B·∫£n ƒë·ªì):**
    *   **Nhi·ªám v·ª•:** "Ph√¢n t√°ch" (Fragment) PageRank. M·ªôt trang c√≥ PageRank `R` v√† c√≥ `k` li√™n k·∫øt outbound s·∫Ω "t·∫∑ng" `R/k` ƒëi·ªÉm cho m·ªói trang ƒë√≠ch.
    *   **Output:** C√°c c·∫∑p `(trang_ƒë√≠ch, ƒëi·ªÉm_ƒë√≥ng_g√≥p)`.

*   **Reduce Step (Gi·∫£m):**
    *   **Nhi·ªám v·ª•:** "T·ªïng h·ª£p". T·∫°i m·ªói trang ƒë√≠ch, ta c·ªông t·∫•t c·∫£ c√°c ƒëi·ªÉm ƒë√≥ng g√≥p nh·∫≠n ƒë∆∞·ª£c t·ª´ c√°c trang ngu·ªìn.
    *   **C√¥ng th·ª©c:** `PR(A) = (1 - d) + d * sum(PR(Ti) / C(Ti))`
        *   `d`: Damping factor (th∆∞·ªùng l√† 0.85).
        *   `Ti`: C√°c trang tr·ªè ƒë·∫øn A.
        *   `C(Ti)`: S·ªë l∆∞·ª£ng li√™n k·∫øt outbound c·ªßa Ti.

---

## 3. H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng (Usage Guide)

### Khi n√†o s·ª≠ d·ª•ng? (Use Cases)
*   **X·ª≠ l√Ω ƒë·ªì th·ªã kh·ªïng l·ªì (Massive Graph Processing):** Khi ƒë·ªì th·ªã (graph) qu√° l·ªõn ƒë·ªÉ fit v√†o RAM c·ªßa m·ªôt m√°y ƒë∆°n l·∫ª (v√≠ d·ª•: Web Graph, Social Network).
*   **Thu·∫≠t to√°n c√≥ th·ªÉ ph√¢n t√°ch (Decomposable Algorithms):** C√°c b√†i to√°n m√† t·∫°i ƒë√≥, k·∫øt qu·∫£ cu·ªëi c√πng l√† t·ªïng h·ª£p c·ªßa c√°c ph·∫ßn ƒë·ªôc l·∫≠p (PageRank, Counting Inversions, Matrix Multiplication).
*   **Batch Processing:** C√°c b√†i to√°n kh√¥ng c·∫ßn realtime, c√≥ th·ªÉ ch·∫°y theo l√¥ (batch) h√†ng ng√†y/th√°ng.

### S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o? (How to use)
1.  **Bi·ªÉu di·ªÖn ƒë·ªì th·ªã:** Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu web th√†nh ƒë·ªãnh d·∫°ng `(URL, List_of_Links)`.
2.  **Kh·ªüi t·∫°o:** G√°n PageRank ban ƒë·∫ßu cho m·ªçi URL (v√≠ d·ª•: `1/N` ho·∫∑c `1.0`).
3.  **V√≤ng l·∫∑p (Iterations):**
    *   Ch·∫°y MapReduce Phase 2 (Distribution).
    *   L·∫•y output l√†m input cho v√≤ng l·∫∑p ti·∫øp theo.
    *   L·∫∑p l·∫°i cho ƒë·∫øn khi PageRank h·ªôi t·ª• (gi√° tr·ªã thay ƒë·ªïi r·∫•t nh·ªè) ho·∫∑c ƒë·∫°t s·ªë v√≤ng l·∫∑p gi·ªõi h·∫°n.
4.  **L∆∞u tr·ªØ:** L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng.

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm (Pros & Cons)

| Ti√™u ch√≠ | ∆Øu ƒëi·ªÉm (Pros) | Nh∆∞·ª£c ƒëi·ªÉm (Cons) |
| :--- | :--- | :--- |
| **Kh·∫£ nƒÉng m·ªü r·ªông (Scalability)** | C√≥ th·ªÉ x·ª≠ l√Ω h√†ng t·ª∑ trang web ch·ªâ v·ªõi cluster m√°y t√≠nh gi√° r·∫ª. | Hi·ªáu qu·∫£ kh√¥ng t·ªëi ∆∞u cho ƒë·ªì th·ªã nh·ªè (overhead c·ªßa MapReduce). |
| **T√≠nh ·ªïn ƒë·ªãnh (Fault Tolerance)** | N·∫øu m·ªôt node ch·∫øt, MapReduce t·ª± ƒë·ªông t√°i kh·ªüi ƒë·ªông task tr√™n node kh√°c. | Ph·∫£i ghi d·ªØ li·ªáu xu·ªëng ƒëƒ©a (Disk I/O) ·ªü m·ªói MapReduce job (v√≤ng l·∫∑p), g√¢y ch·∫≠m. |
| **ƒê·ªô ph·ª©c t·∫°p (Complexity)** | Logic ƒë∆°n gi·∫£n, d·ªÖ debug. | Code d√†i d√≤ng, ph·∫£i qu·∫£n l√Ω serialization/deserialization. |
| **T·ªëc ƒë·ªô (Speed)** | T·ªët cho d·ªØ li·ªáu l·ªõn. | **R·∫•t ch·∫≠m** so v·ªõi c√°c framework ƒë·ªì th·ªã chuy√™n d·ª•ng nh∆∞ Apache Spark GraphX hay Pregel v√¨ ph·∫£i t·∫£i d·ªØ li·ªáu qua l·∫°i gi·ªØa c√°c v√≤ng l·∫∑p. |

---

## 4. V√≠ d·ª• Th·ª±c t·∫ø (Real-world Examples)

### V√≠ d·ª• 1: C√¥ng c·ª• T√¨m ki·∫øm (Search Engines)
*   **Ng√†nh:** C√¥ng ngh·ªá th√¥ng tin.
*   **·ª®ng d·ª•ng:** Google (ban ƒë·∫ßu), Bing, Yahoo.
*   **C√°ch d√πng:** D√πng MapReduce ƒë·ªÉ t√≠nh to√°n ch·ªâ s·ªë uy t√≠n (PageRank) c·ªßa h√†ng t·ª∑ trang web. K·∫øt qu·∫£ n√†y ƒë∆∞·ª£c k·∫øt h·ª£p v·ªõi c√°c y·∫øu t·ªë kh√°c (n·ªôi dung, v·ªã tr√≠ ƒë·ªãa l√Ω) ƒë·ªÉ x·∫øp h·∫°ng k·∫øt qu·∫£ t√¨m ki·∫øm.

### V√≠ d·ª• 2: Ph√¢n t√≠ch M·∫°ng x√£ h·ªôi (Social Network Analysis)
*   **Ng√†nh:** Truy·ªÅn th√¥ng x√£ h·ªôi, An ninh m·∫°ng.
*   **·ª®ng d·ª•ng:** T√¨m ng∆∞·ªùi c√≥ ·∫£nh h∆∞·ªüng (Influencer detection), ph√°t hi·ªán c·ªông ƒë·ªìng (Community detection).
*   **C√°ch d√πng:** Thay v√¨ web links, d·ªØ li·ªáu l√† c√°c m·ªëi quan h·ªá "Follow", "Friend". Ta √°p d·ª•ng thu·∫≠t to√°n t∆∞∆°ng t·ª± (Random Walk) tr√™n MapReduce ƒë·ªÉ t√¨m ra c√°c node trung t√¢m (Central nodes) trong m·∫°ng l∆∞·ªõi.

### V√≠ d·ª• 3: H·ªá th·ªëng ƒê·ªÅ xu·∫•t (Recommender Systems)
*   **Ng√†nh:** Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ (E-commerce).
*   **·ª®ng d·ª•ng:** Amazon, Shopee.
*   **C√°ch d√πng:** D√πng Graph-based algorithms (gi·ªëng PageRank) ƒë·ªÉ t√¨m c√°c s·∫£n ph·∫©m c√≥ li√™n quan. V√≠ d·ª•: "Nh·ªØng ng∆∞·ªùi mua s·∫£n ph·∫©m A c≈©ng th∆∞·ªùng mua s·∫£n ph·∫©m B", suy ra B l√† m·ªôt node "quan tr·ªçng" li√™n quan ƒë·∫øn A.

---

Ch√†o b·∫°n, t√¥i l√† chuy√™n gia v·ªÅ Big Data v√† H·ªá th·ªëng Ph√¢n t√°n. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v·ªÅ n·ªôi dung slide "6_mapreduce.pdf" m√† b·∫°n ƒë√£ cung c·∫•p, ƒë∆∞·ª£c tr√¨nh b√†y l·∫°i m·ªôt c√°ch chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát.

---

# Ph√¢n t√≠ch v√† H∆∞·ªõng d·∫´n Chi ti·∫øt v·ªÅ MapReduce trong T√≠nh to√°n L·∫∑p (Iterative Computation)

## 1. T·ªïng quan v·ªÅ V·∫•n ƒë·ªÅ

Slide n√†y ƒë·ªÅ c·∫≠p ƒë·∫øn giai ƒëo·∫°n k·∫øt th√∫c c·ªßa m·ªôt quy tr√¨nh MapReduce ph·ª©c t·∫°p, c·ª• th·ªÉ l√† trong thu·∫≠t to√°n **PageRank**. ƒêi·ªÉm m·∫•u ch·ªët l√† c√°ch x·ª≠ l√Ω c√°c b√†i to√°n y√™u c·∫ßu ch·∫°y l·∫∑p ƒëi l·∫∑p l·∫°i (iterative) tr√™n n·ªÅn t·∫£ng MapReduce, v·ªën ban ƒë·∫ßu ƒë∆∞·ª£c thi·∫øt k·∫ø cho c√°c t√°c v·ª• m·ªôt l·∫ßn (one-pass).

## 2. Ho√†n thi·ªán Quy tr√¨nh (Finishing up)

Ph·∫ßn n√†y m√¥ t·∫£ logic ƒëi·ªÅu khi·ªÉn cho m·ªôt thu·∫≠t to√°n l·∫∑p (nh∆∞ PageRank).

### Gi·∫£i th√≠ch Kh√°i ni·ªám

Trong thu·∫≠t to√°n PageRank, ta c·∫ßn th·ª±c hi·ªán nhi·ªÅu v√≤ng l·∫∑p (iterations) ƒë·ªÉ c√°c gi√° tr·ªã PageRank c√≥ th·ªÉ lan truy·ªÅn qua c√°c li√™n k·∫øt v√† ·ªïn ƒë·ªãnh (convergence). Quy tr√¨nh n√†y kh√¥ng th·ªÉ k·∫øt th√∫c ch·ªâ sau m·ªôt MapReduce job duy nh·∫•t.

*   **Convergence (S·ª± h·ªôi t·ª•):** L√† tr·∫°ng th√°i m√† c√°c gi√° tr·ªã PageRank kh√¥ng thay ƒë·ªïi ƒë√°ng k·ªÉ gi·ªØa c√°c v√≤ng l·∫∑p. Ta c·∫ßn m·ªôt ti√™u ch√≠ ƒë·ªÉ d·ª´ng.
*   **Phase 2:** Trong ng·ªØ c·∫£nh PageRank, Phase 2 th∆∞·ªùng l√† giai ƒëo·∫°n **Reduce** ho·∫∑c m·ªôt job MapReduce ri√™ng bi·ªát th·ª±c hi·ªán t√≠nh to√°n.

### Logic Quy tr√¨nh (Pseudo-code)

D∆∞·ªõi ƒë√¢y l√† m√£ gi·∫£ m√¥ t·∫£ v√≤ng ƒë·ªùi c·ªßa m·ªôt thu·∫≠t to√°n l·∫∑p tr√™n MapReduce:

```python
def page_rank_iterative(graph, max_iterations=10, threshold=0.001):
    # Kh·ªüi t·∫°o PageRank ban ƒë·∫ßu
    ranks = initialize_ranks(graph)
    
    for i in range(max_iterations):
        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho MapReduce
        input_data = prepare_input(ranks, graph)
        
        # Ch·∫°y Phase 2 (MapReduce Job)
        # Map: Ph√¢n ph·ªëi rank ƒë·∫øn c√°c node ƒë√≠ch
        # Reduce: T√≠nh to√°n rank m·ªõi
        new_ranks = run_mapreduce_job(input_data)
        
        # Ki·ªÉm tra h·ªôi t·ª• (Convergence Check)
        if check_convergence(ranks, new_ranks, threshold):
            print("ƒê√£ h·ªôi t·ª•!")
            break
        
        # C·∫≠p nh·∫≠t ranks cho v√≤ng l·∫∑p ti·∫øp theo
        ranks = new_ranks

    # Xu·∫•t k·∫øt qu·∫£ cu·ªëi c√πng
    save_results(ranks)
    return ranks

def check_convergence(old_ranks, new_ranks, threshold):
    total_diff = 0
    for node in new_ranks:
        diff = abs(new_ranks[node] - old_ranks.get(node, 0))
        total_diff += diff
    
    # So s√°nh ch√™nh l·ªách t·ªïng th·ªÉ ho·∫∑c ch√™nh l·ªách t·ª´ng key
    return total_diff < threshold
```

### H∆∞·ªõng d·∫´n S·ª≠ d·ª•ng

*   **Khi n√†o s·ª≠ d·ª•ng?**
    *   Khi x·ª≠ l√Ω c√°c b√†i to√°n ƒë·ªì th·ªã (Graph Processing) nh∆∞ PageRank, Shortest Path.
    *   Khi d·ªØ li·ªáu qu√° l·ªõn ƒë·ªÉ x·ª≠ l√Ω trong b·ªô nh·ªõ (RAM) c·ªßa m·ªôt m√°y v√† c·∫ßn chia nh·ªè d·ªØ li·ªáu.
*   **S·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?**
    *   Vi·∫øt m·ªôt driver program (b·∫±ng Java, Python) ƒë·ªÉ l·∫∑p l·∫°i vi·ªác g·ªçi MapReduce Job.
    *   Job ƒë·∫ßu ra (output) c·ªßa v√≤ng l·∫∑p tr∆∞·ªõc s·∫Ω tr·ªü th√†nh ƒë·∫ßu v√†o (input) c·ªßa v√≤ng l·∫∑p sau.
    *   S·ª≠ d·ª•ng Hadoop API ho·∫∑c frameworks wrapper nh∆∞ Oozie ƒë·ªÉ orchestrate c√°c job n√†y.
*   **∆Øu ƒëi·ªÉm:** Kh·∫£ nƒÉng m·ªü r·ªông (scalability) tuy·ªát v·ªùi, x·ª≠ l√Ω ƒë∆∞·ª£c d·ªØ li·ªáu l·ªõn.
*   **Nh∆∞·ª£c ƒëi·ªÉm:** Hi·ªáu su·∫•t kh√¥ng t·ªët do overhead c·ªßa vi·ªác ƒë·ªçc/ghi d·ªØ li·ªáu t·ª´ disk (HDFS) ·ªü m·ªói v√≤ng l·∫∑p.

---

## 3. Ghi ch√∫ (Remark)

Ph·∫ßn n√†y ph√¢n t√≠ch c√°c th√°ch th·ª©c v√† ƒëi·ªÉm m·∫•u ch·ªët khi th·ª±c hi·ªán song song h√≥a (parallelization) trong MapReduce.

### Gi·∫£i th√≠ch Kh√°i ni·ªám

*   **"Heavy lifting" (C√¥ng vi·ªác n·∫∑ng nh·ªçc):** MapReduce ƒë·∫£m nh·∫≠n ph·∫ßn t√≠nh to√°n ph·ª©c t·∫°p nh·∫•t.
*   **Independent PageRank computations:** Trong m·ªôt b∆∞·ªõc (step) c·ª• th·ªÉ, vi·ªác t√≠nh PageRank c·ªßa m·ªôt node `A` ph·ª• thu·ªôc v√†o c√°c node tr·ªè ƒë·∫øn `A`, nh∆∞ng kh√¥ng ph·ª• thu·ªôc v√†o node `B` (n·∫øu `B` kh√¥ng li√™n quan). T√≠nh ƒë·ªôc l·∫≠p n√†y cho ph√©p chia ƒë·ªÅu c√¥ng vi·ªác cho c√°c mapper/reducer.
*   **Compact representations (Bi·ªÉu di·ªÖn g·ªçn):** ƒê·ªÉ gi·∫£m th·ªùi gian truy·ªÅn d·ªØ li·ªáu gi·ªØa c√°c nodes, ta ch·ªâ n√™n g·ª≠i nh·ªØng d·ªØ li·ªáu c·∫ßn thi·∫øt. V√≠ d·ª•: thay v√¨ g·ª≠i to√†n b·ªô c·∫•u tr√∫c graph, ch·ªâ g·ª≠i `(source_node, destination_node, rank_value)`.

### V√≠ d·ª• D·ªØ li·ªáu (Data Representation)

Thay v√¨ l∆∞u tr·ªØ ma tr·∫≠n k·ªÅ (adjacency matrix) c·ªìng k·ªÅnh, ta s·ª≠ d·ª•ng danh s√°ch li√™n k·∫øt d·∫°ng key-value.

**Input cho MapReduce (Format):**
```text
Node_A  [Node_B, Node_C]  // D·∫°ng n√©n: Node A tr·ªè ƒë·∫øn B v√† C
Node_B  [Node_C]
```

**Output Mapper (D·∫°ng truy·ªÅn t·∫£i):**
```text
Node_B  0.5  // G·ª≠i m·ªôt ph·∫ßn rank c·ªßa A ƒë·∫øn B
Node_C  0.5  // G·ª≠i m·ªôt ph·∫ßn rank c·ªßa A ƒë·∫øn C
```

### ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm

| Ti√™u ch√≠ | Ph√¢n t√≠ch |
| :--- | :--- |
| **∆Øu ƒëi·ªÉm** | - **T√≠nh to√°n song song ho√†n to√†n:** M·ªói mapper/reducer l√†m vi·ªác ƒë·ªôc l·∫≠p.<br>- **Tolerant to failures:** N·∫øu m·ªôt node t√≠nh to√°n ch·∫øt, job c√≥ th·ªÉ ƒë∆∞·ª£c kh·ªüi ch·∫°y l·∫°i. |
| **Nh∆∞·ª£c ƒëi·ªÉm** | - **Disk I/O Bottleneck:** Ph·∫£i ghi k·∫øt qu·∫£ xu·ªëng ƒëƒ©a ·ªü m·ªói v√≤ng l·∫∑p (kh√¥ng nh∆∞ Pregel hay Spark in-memory).<br>- **Latency cao:** Kh√¥ng ph√π h·ª£p v·ªõi c√°c b√†i to√°n c·∫ßn ph·∫£n h·ªìi nhanh (real-time).<br>- **Kh√¥ng scale ƒë∆∞·ª£c cho Internet th·ª±c:**Ê≠£Â¶Ç slide n√≥i, n√≥ ch·ªâ hi·ªáu qu·∫£ cho ƒë·ªì th·ªã c·ª° trung b√¨nh (intermediate-sized graphs). |

---

## 4. V√≠ d·ª• Th·ª±c t·∫ø trong C√¥ng nghi·ªáp

### B√†i to√°n: L·∫≠p ch·ªâ m·ª•c Search Engine (Search Indexing)

Tr∆∞·ªõc khi c√≥ c√°c h·ªá th·ªëng hi·ªán ƒë·∫°i nh∆∞ Spark, Google v√† c√°c c√¥ng ty kh√°c s·ª≠ d·ª•ng MapReduce ƒë·ªÉ x·ª≠ l√Ω web graph.

1.  **B∆∞·ªõc 1 (Crawl):** Thu th·∫≠p d·ªØ li·ªáu web.
2.  **B∆∞·ªõc 2 (MapReduce L·∫∑p):**
    *   **Map:** ƒê·ªçc c√°c trang web, tr√≠ch xu·∫•t li√™n k·∫øt (links).
    *   **Reduce:** T√≠nh to√°n PageRank cho m·ªói URL.
    *   **L·∫∑p l·∫°i** cho ƒë·∫øn khi PageRank ·ªïn ƒë·ªãnh.
3.  **B∆∞·ªõc 3 (Indexing):** K·∫øt h·ª£p PageRank v·ªõi t·ª´ kh√≥a ƒë·ªÉ t·∫°o ch·ªâ m·ª•c t√¨m ki·∫øm.

### V√≠ d·ª• Code M·∫´u (Python v·ªõi Hadoop Streaming)

Gi·∫£ s·ª≠ ta ƒëang vi·∫øt m·ªôt Mapper cho PageRank ƒë·ªÉ ph√¢n ph·ªëi rank:

```python
#!/usr/bin/env python3
import sys

def mapper():
    """
    ƒê·ªçc t·ª´ng d√≤ng t·ª´ stdin.
    D√≤ng 1: D·∫°ng "Node_A \t [Node_B, Node_C]"
    D√≤ng 2: D·∫°ng "Node_A \t 0.15" (Current Rank)
    """
    for line in sys.stdin:
        line = line.strip()
        parts = line.split('\t')
        
        # N·∫øu l√† c·∫•u tr√∫c ƒë·ªì th·ªã (Adjacency list)
        if len(parts) == 2 and parts[1].startswith('['):
            node = parts[0]
            neighbors = eval(parts[1]) # Parse list
            # Ph√°t (Emit) c·∫•u tr√∫c ƒë·ªì th·ªã ƒë·ªÉ gi·ªØ l·∫°i cho Reduce
            print(f"{node}\tGRAPH\t{neighbors}")
            
        # N·∫øu l√† gi√° tr·ªã PageRank
        elif len(parts) == 2:
            node = parts[0]
            rank = float(parts[1])
            # Ph√°t (Emit) rank ƒë·∫øn c√°c node ƒë√≠ch
            # (Logic l·∫•y danh s√°ch neighbors c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω trong Reduce ho·∫∑c Cache)
            # Trong v√≠ d·ª• ƒë∆°n gi·∫£n n√†y, ta gi·∫£ ƒë·ªãnh Mapper ƒë√£ bi·∫øt neighbors
            # ho·∫∑c ta c·∫ßn m·ªôt MapReduce job ri√™ng ƒë·ªÉ t√≠nh to√°n lu·ªìng rank.
            pass

if __name__ == "__main__":
    mapper()
```

**L∆∞u √Ω:** Trong th·ª±c t·∫ø, PageRank tr√™n MapReduce th∆∞·ªùng t√°ch bi·ªát: 1 Job ƒë·ªÉ t√≠nh to√°n rank m·ªõi t·ª´ rank c≈©, v√† 1 Job ƒë·ªÉ t·ªïng h·ª£p/ki·ªÉm tra h·ªôi t·ª•.

---

## 5. K·∫øt lu·∫≠n

Slide n√†y nh·∫•n m·∫°nh r·∫±ng **MapReduce l√† m·ªôt c√¥ng c·ª• m·∫°nh m·∫Ω nh∆∞ng kh√¥ng ph·∫£i l√† "ph√©p m√†u"**. ƒê·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n ph·ª©c t·∫°p nh∆∞ PageRank, ch√∫ng ta ph·∫£i:
1.  Ch·∫•p nh·∫≠n chi ph√≠ overhead c·ªßa vi·ªác ƒë·ªçc/ghi disk.
2.  T·ªëi ∆∞u h√≥a d·ªØ li·ªáu truy·ªÅn t·∫£i (compact representations).
3.  Hi·ªÉu r√µ gi·ªõi h·∫°n: MapReduce ph√π h·ª£p v·ªõi c√°c b√†i to√°n batch processing quy m√¥ l·ªõn nh∆∞ng kh√¥ng ph·∫£i l√† l·ª±a ch·ªçn t·ªët cho c√°c ƒë·ªì th·ªã si√™u l·ªõn (h√†ng t·ª∑ c·∫°nh) n·∫øu kh√¥ng c√≥ c√°c t·ªëi ∆∞u h√≥a n√¢ng cao ho·∫∑c thay th·∫ø b·∫±ng c√°c framework nh∆∞ **Apache Spark** (th·ª±c hi·ªán t√≠nh to√°n trong b·ªô nh·ªõ) ho·∫∑c **Apache Giraph/Pregel** (th·ª±c hi·ªán t√≠nh to√°n theo m√¥ h√¨nh Bulk Synchronous Parallel).

---

